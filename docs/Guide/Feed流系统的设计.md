Feed流是一个目前非常常见的功能，在众多产品中都有展现，通过Feed流可以把动态实时的传播给订阅者，是用户获取信息流的一种有效方式。在大数据时代，如何打造一个千万级规模的Feed流系统仍然是一个挑战。

在互联网领域，尤其现在的移动互联网时代，Feed流产品是非常常见的，比如我们每天都会用到的朋友圈，微博，就是一种非常典型的Feed流产品，还有图片分享网站Pinterest，花瓣网等又是另一种形式的Feed流产品。除此之外，很多App的都会有一个模块，要么叫动态，要么叫消息广场，这些也是Feed流产品，可以说，Feed流产品是遍布天下所有的App中。

## Feed流系统特征

- 多账号内容流
- 非稳定的账号关系
- 读多写少 100:1
- 一致性要求高



## Feed流设计

### 同步（推送）

系统规模和产品类型，以及存储系统确定后，我们可以确定同步方式，常见的方式有三种：

- 推模式（也叫写扩散）：和名字一样，就是一种推的方式，发送者发送了一个消息后，立即将这个消息推送给接收者，但是接收者此时不一定在线，那么就需要有一个地方存储这个数据，这个存储的地方我们称为：同步库。推模式也叫写扩散的原因是，一个消息需要发送个多个粉丝，那么这条消息就会复制多份，写放大，所以也叫写扩散。这种模式下，对同步库的要求就是写入能力极强和稳定。读取的时候因为消息已经发到接收者的收件箱了，只需要读一次自己的收件箱即可，读请求的量极小，所以对读的 QPS 需求不大。归纳下，推模式中对同步库的要求只有一个：写入能力强。
- 拉模式（也叫读扩散）：这种是一种拉的方式，发送者发送了一条消息后，这条消息不会立即推送给粉丝，而是写入自己的发件箱，当粉丝上线后再去自己关注者的发件箱里面去读取，一条消息的写入只有一次，但是读取最多会和粉丝数一样，读会放大，所以也叫读扩散。拉模式的读写比例刚好和写扩散相反，那么对系统的要求是：读取能力强。另外这里还有一个误区，很多人在最开始设计 feed 流系统时，首先想到的是拉模式，因为这种和用户的使用体感是一样的，但是在系统设计上这种方式有不少痛点，最大的是每个粉丝需要记录自己上次读到了关注者的哪条消息，如果有 1000 个关注者，那么这个人需要记录 1000 个位置信息，这个量和关注量成正比的，远比用户数要大的多，这里要特别注意，虽然在产品前期数据量少的时候这种方式可以应付，但是量大了后就会事倍功半，得不偿失，切记切记。
- 推拉结合模式：推模式在单向关系中，因为存在大 V，那么一条消息可能会扩散几百万次，但是这些用户中可能有一半多是僵尸，永远不会上线，那么就存在资源浪费。而拉模式下，在系统架构上会很复杂，同时需要记录的位置信息是天量，不好解决，尤其是用户量多了后会成为第一个故障点。基于此，所以有了推拉结合模式，大部分用户的消息都是写扩散，只有大 V 是读扩散，这样既控制了资源浪费，又减少了系统设计复杂度。但是整体设计复杂度还是要比推模式复杂。

用图表对比：

| 类型         | 推模式                                         | 拉模式                                              | 推拉结合模式 |
| ------------ | ---------------------------------------------- | --------------------------------------------------- | ------------ |
| 写放大       | 高                                             | 无                                                  | 中           |
| 读放大       | 无                                             | 高                                                  | 中           |
| 用户读取延时 | 毫秒                                           | 秒                                                  | 秒           |
| 读写比例     | 1:99                                           | 99:1                                                | ~50:50       |
| 系统要求     | 写能力强                                       | 读能力强                                            | 读写都适中   |
| 常见系统     | Tablestore、Bigtable 等 LSM 架构的分布式 NoSQL | Redis、memcache 等缓存系统或搜索系统 (推荐排序场景) | 两者结合     |
| 架构复杂度   | 简单                                           | 复杂                                                | 更复杂       |

介绍完同步模式中所有场景和模式后，我们归纳下：

- 如果产品中是双向关系，那么就采用推模式。
- 如果产品中是单向关系，且用户数少于 1000 万，那么也采用推模式，足够了。
- 如果产品是单向关系，单用户数大于 1000 万，那么采用推拉结合模式，这时候可以从推模式演进过来，不需要额外重新推翻重做。
- 永远不要只用拉模式。
- 如果是一个初创企业，先用推模式，快速把系统设计出来，然后让产品去验证、迭代，等客户数大幅上涨到 1000 万后，再考虑升级为推拉集合模式。
- 如果是按推荐排序，那么是另外的考虑了，架构会完全不一样，这个后面专门文章介绍。

### 存储

1. 账号关系数据特点
   - 变长有序列表
   - 数据量大
   - 关系简单
2. 账号关系实现
   - 关系型数据库 + 分库分表
   - NoSQL

3. Feed数据特点
   - 数据量大
   - 格式简单
   - 可靠性要求高
4. Feed存储实现
   - 关系型数据库 + 分库分表
   - NoSQL

### 读写

- 特点：读多写少，读写比例 100:1
- Pull vs Push
- tricks
- 大V、普通用户、活跃用户、非活跃用户

### Rank

- 个性化推荐
- 打点
- 召回
- 融合
- 排序

## 存储账号关系

### TableStore

表格存储(TableStore)是阿里云自主研发的专业级分布式NoSQL数据库，是基于共享存储的高性能、低成本、易扩展、全托管的半结构化数据存储平台，

### 使用开源HBase存储账号关系

能满足有序性的分布式NoSQL数据库中，开源HBase就是一个，所以很多企业会选择开源HBase来存储账号关系，或者是关注列表。

### 使用TableStore存储账号关系

所以，就有很多用户选择使用阿里云的表格存储(TableStore)，主要是下面一些地方能带来较大的收益：

- 单表支持10万亿行+，10PB+的数据量。
- 数据按主键列排序，保证有序性。
- 单key读写延迟在毫秒级别，保证关注，取关的响应时间。
- 是全托管的分布式NoSQL数据库服务，无需任何运维。
- 全部采用C++实现，彻底无GC问题，也就不会由于GC而导致较大的毛刺。

所以，使用表格存储(TableStore)来存储账号关系是一个比较好的选择。

## 存储Feed消息

## 推拉结合模式下

### 发布Feed流程

当你发布一条Feed消息的时候，流程是这样的：

1. Feed消息先进入一个队列服务。
2. 先从关注列表中读取到自己的粉丝列表，以及判断自己是否是大V。
3. 将自己的Feed消息写入个人页Timeline（发件箱）。如果是大V，写入流程到此就结束了。
4. 如果是普通用户，还需要将自己的Feed消息写给自己的粉丝，如果有100个粉丝，那么就要写给100个用户，包括Feed内容和Feed ID。
5. 第三步和第四步可以合并在一起，使用BatchWriteRow接口一次性将多行数据写入TableStore。
6. 发布Feed的流程到此结束。

### 读取Feed流流程

当刷新自己的Feed流的时候，流程是这样的：

1. 先去读取自己关注的大V列表
2. 去读取自己的收件箱，只需要一个GetRange读取一个范围即可，范围起始位置是上次读取到的最新Feed的ID，结束位置可以使当前时间，也可以是MAX，建议是MAX值。由于之前使用了主键自增功能，所以这里可以使用GetRange读取。
3.  如果有关注的大V，则再次并发读取每一个大V的发件箱，如果关注了10个大V，那么则需要10次访问。
4. 合并2和3步的结果，然后按时间排序，返回给用户。

至此，使用推拉结合方式的发布，读取Feed流的流程都结束了。



## 实例

- 首页：Rank Feed流
- 关注页：Timeline、Aggregate





---

参考

1. [Feed流系统设计总纲 \- InfoQ](https://www.infoq.cn/article/t0QlHfK7uXxzWO0uO*9s)

2. [如何打造千万级Feed流系统 \- 知乎](https://zhuanlan.zhihu.com/p/30226315)

3. [Feed流系统设计实践（一） \- 个人文章 \- SegmentFault 思否](https://segmentfault.com/a/1190000018634687)

4. [Feed 流设计\(三\)：分发逻辑 \- 吕小荣](http://mednoter.com/design-of-feed-part-three.html)

   

