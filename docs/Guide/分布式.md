## 分布式原则

### CAP定理

我们往往为了可用性和分区容错性，忍痛放弃强一致支持，转而追求最终一致性。大部分业务场景下，我们是可以接受短暂的不一致的。

![](https://image-hosting-lan.oss-cn-beijing.aliyuncs.com/CAP.png)

一致性和可用性，为什么不可能同时成立？答案很简单，因为可能通信失败（即出现分区容错）。

如果保证 G2 的一致性，那么 G1 必须在写操作时，锁定 G2 的读操作和写操作。只有数据同步后，才能重新开放读写。锁定期间，G2 不能读写，没有可用性不。

如果保证 G2 的可用性，那么势必不能锁定 G2，所以一致性不成立。

综上所述，G2 无法同时做到一致性和可用性。系统设计时只能选择一个目标。如果追求一致性，那么无法保证所有节点的可用性；如果追求所有节点的可用性，那就没法做到一致性。



CAP三者不可兼得，该如何取舍：

(1) CA: 优先保证一致性和可用性，放弃分区容错。 这也意味着放弃系统的扩展性，系统不再是分布式的，有违设计的初衷。

(2) CP: 优先保证一致性和分区容错性，放弃可用性。在数据一致性要求比较高的场合(譬如:zookeeper,Hbase) 是比较常见的做法，一旦发生网络故障或者消息丢失，就会牺牲用户体验，等恢复之后用户才逐渐能访问。

(3) AP: 优先保证可用性和分区容错性，放弃一致性。NoSQL中的Cassandra 就是这种架构。跟CP一样，放弃一致性不是说一致性就不保证了，而是逐渐的变得一致。

### 应用CAP分析

| 应用                    | 类型 | 其他                                                |
| ----------------------- | ---- | --------------------------------------------------- |
| Mysql                   | CA   | 主从模式为AP                                        |
| ZooKeeper               | CP   | 在分区后，对于A，只有分区内节点大于quorum才对外服务 |
| Eureka                  | AP   | 最终一致性                                          |
| Redis哨兵/集群模式      | AP   | 最终一致性，单体肯定是CA                            |
| RocketMQ主从            | AP   | 最终一致性                                          |
| 分布式事务-2pc          | CP   | 锁住资源,该资源其他请求阻塞                         |
| 分布式事务-TCC          | AP   | 最终一致性                                          |
| 分布式事务-最大努力尝试 | AP   | 最终一致性                                          |

### BASE理论

在分布式系统中，我们往往追求的是可用性，它的重要程序比一致性要高，那么如何实现高可用性呢？ 前人已经给我们提出来了另外一个理论，就是BASE理论，它是用来对CAP定理进行进一步扩充的。BASE理论指的是：

- Basically Available（基本可用）
- Soft state（软状态）
- Eventually consistent（最终一致性）

BASE理论是对CAP中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：**我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性**（Eventual consistency）。

有了以上理论之后，我们来看一下分布式事务的问题。

## 分布式事务

是指在分布式服务下，不同服务之间保持事务操作的。

对不同数据库的操作必然需要引入分布式事务。

### 常见的分布式事务解决方案

- 基于XA协议的两阶段提交

- 消息事务+最终一致性
- TCC编程模式



## 微服务

几大主要构成框架组件：

- 





## 共识算法

传统分布式系统领域的 Paxos、Raft 以及密码货币中使用的工作量证明（POW）、权益证明（POS）和委托权益证明（DPOS）。

Paxos 和 Raft 是目前分布式系统领域中两种非常著名的解决一致性问题的共识算法，两者都能解决分布式系统中的一致性问题，但是前者的实现与证明非常难以理解，后者的实现比较简洁并且遵循人的直觉，它的出现就是为了解决 Paxos 难以理解并和难以实现的问题。

[ZAB与Paxos算法的联系与区别\_大数据\_有趣的难受\-CSDN博客](https://blog.csdn.net/m0_37383866/article/details/87603114)

[共识算法：Raft \- 简书](https://www.jianshu.com/p/8e4bbe7e276c)

[10分钟弄懂Raft算法 \- 云\+社区 \- 腾讯云](https://cloud.tencent.com/developer/news/398156)

[Raft算法详解 \- 左手编程右手诗 \- 博客园](https://www.cnblogs.com/aibabel/p/10973585.html)

[RAFT算法详解\_网络\_青萍之末的博客\-CSDN博客](https://blog.csdn.net/daaikuaichuan/article/details/98627822)

[可视化Raft算法](https://raft.github.io/raftscope/index.html)

一个分布式网络节点，每个节点有**三种状态：Follower，Candidate，Leader**，状态之间是互相转换的。

在 Raft 运行过程中，最主要进行两个活动：

1. 选主 Leader Election
2. 复制日志 Log Replication

选主：

假设现在有5个节点，5个节点一开始的状态都是 Follower。

在一个节点倒计时结束 (Timeout) 后，这个节点的状态变成 Candidate 开始选举，它给其他几个节点发送选举请求 (RequestVote)

其他四个节点都返回成功，这个节点的状态由 Candidate 变成了 Leader，并在每个一小段时间后，就给所有的 Follower 发送一个 Heartbeat 以保持所有节点的状态，Follower 收到 Leader 的 Heartbeat 后重设 Timeout。

这是最简单的选主情况，**只要有超过一半的节点投支持票了，Candidate 才会被选举为 Leader**，5个节点的情况下，3个节点 (包括 Candidate 本身) 投了支持就行。

