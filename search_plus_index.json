{"./":{"url":"./","title":"Interview-Guide","keywords":"","body":"Interview-Guide 面试总结知识点合集，方便自己阅读和回顾。 Java 相关知识总结 Java相关 Java基础 Java集合 Java并发 Java类加载 JVM Spring相关 Spring的一些概念 Spring复习 SpringBoot Spring+IOC+容器源码分析 Spring框架学习 底层及其他 什么是伪共享 序列化的底层怎么实现的 Redis Redis整理 Redis底层架构 数据库 MySQL整理 中间件 Zookeeper Zookeeper RabbitMQ RabbitMQ Kafka Kafka整理 Kafka进阶 ElasticSearch ElasticSearch整理 分布式系统设计 分布式 架构设计 分布式相关 一致性哈希 限流算法 阿里Sentinel 系统设计 秒杀系统设计 RPC Thrift-RPC gRPC 算法 算法总结 其他 Elastic-Job原理 设计模式 单例模式 工厂模式 模板方法模式 策略模式 观察者模式 责任链模式 面试 面试常见问题回答（主观题） by lancelothe "},"docs/Guide/Java基础.html":{"url":"docs/Guide/Java基础.html","title":"Java基础","keywords":"","body":"Java的基本数据类型 类型 存储需求 bit 数 取值范围 备注 int 4字节 4*8 -2147483648~2147483647 即 (-2)的31次方 ~ (2的31次方) - 1 short 2字节 2*8 -32768~32767 即 (-2)的15次方 ~ (2的15次方) - 1 long 8字节 8*8 即 (-2)的63次方 ~ (2的63次方) - 1 byte 1字节 1*8 -128~127 即 (-2)的7次方 ~ (2的7次方) - 1 float 4字节 4*8 float 类型的数值有一个后缀 F（例如：3.14F） double 8字节 8*8 没有后缀 F 的浮点数值（例如：3.14）默认为 double boolean 1字节 1*8 true、false char 2字节 2*8 Java中，只要是字符，不管是数字还是英文还是汉字，都占两个字节。 至于为什么 Java 中 char 无论中英文数字都占用2字节，是因为 Java 中使用 Unicode 字符，所有字符均以2个字节存储。 重载和重写的区别 重载 发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同。 重写 重写是子类对父类的允许访问的方法的实现过程进行重新编写，发生在子类中，方法名、参数列表必须相同，返回值范围小于等于父类，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类。另外，如果父类方法访问修饰符为 private 则子类就不能重写该方法。也就是说方法提供的行为改变，而方法的外貌并没有改变。 Java 面向对象编程三大特性: 封装 继承 多态 封装 封装把一个对象的属性私有化，同时提供一些可以被外界访问的属性的方法，如果属性不想被外界访问，我们大可不必提供方法给外界访问。但是如果一个类没有提供给外界访问的方法，那么这个类也没有什么意义了。 继承 继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承我们能够非常方便地复用以前的代码。 关于继承如下 3 点请记住： 子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类中的私有属性和方法子类是无法访问，只是拥有。 子类可以拥有自己属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。（以后介绍）。 多态 所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定，而是在程序运行期间才确定，即一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。 在Java中有两种形式可以实现多态：继承（多个子类对同一方法的重写）和接口（实现接口并覆盖接口中同一方法）。 String、StringBuffer 和 StringBuilder 的区别是什么? String 为什么是不可变的 ? 可变性 简单的来说：String 类中使用 final 关键字修饰字符数组来保存字符串，private　final　char　value[]，所以 String 对象是不可变的。而StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串char[]value 但是没有用 final 关键字修饰，所以这两种对象都是可变的。 StringBuilder 与 StringBuffer 的构造方法都是调用父类构造方法也就是 AbstractStringBuilder 实现的，大家可以自行查阅源码。 AbstractStringBuilder.java abstract class AbstractStringBuilder implements Appendable, CharSequence { char[] value; int count; AbstractStringBuilder() { } AbstractStringBuilder(int capacity) { value = new char[capacity]; } 线程安全性 String 中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。　 性能 每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 对于三者使用的总结： 操作少量的数据: 适用String 单线程操作字符串缓冲区下操作大量数据: 适用StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用StringBuffer 接口和抽象类的区别是什么？ 接口的方法默认是 public，所有方法在接口中不能有实现(Java 8 开始接口方法可以有默认实现），而抽象类可以有非抽象的方法。 接口中除了static、final变量，不能有其他变量，而抽象类中则不一定。 一个类可以实现多个接口，但只能实现一个抽象类。接口自己本身可以通过extends关键字扩展多个接口。 接口方法默认修饰符是public，抽象方法可以有public、protected和default这些修饰符（抽象方法就是为了被重写所以不能使用private关键字修饰！）。 从设计层面来说，抽象是对类的抽象，是一种模板设计，而接口是对行为的抽象，是一种行为的规范。 备注：在JDK8中，接口也可以定义静态方法，可以直接用接口名调用。实现类和实现是不可以调用的。如果同时实现两个接口，接口中定义了一样的默认方法，则必须重写，不然会报错。 == 与 equals(重要) == : 它的作用是判断两个对象的地址是不是相等。即，判断两个对象是不是同一个对象(基本数据类型==比较的是值，引用数据类型==比较的是内存地址)。 equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况： 情况1：类没有覆盖 equals() 方法。则通过 equals() 比较该类的两个对象时，等价于通过“==”比较这两个对象。 情况2：类覆盖了 equals() 方法。一般，我们都覆盖 equals() 方法来比较两个对象的内容是否相等；若它们的内容相等，则返回 true (即，认为这两个对象相等)。 举个例子： public class test1 { public static void main(String[] args) { String a = new String(\"ab\"); // a 为一个引用 String b = new String(\"ab\"); // b为另一个引用,对象的内容一样 String aa = \"ab\"; // 放在常量池中 String bb = \"ab\"; // 从常量池中查找 if (aa == bb) // true System.out.println(\"aa==bb\"); if (a == b) // false，非同一对象 System.out.println(\"a==b\"); if (a.equals(b)) // true System.out.println(\"aEQb\"); if (42 == 42.0) { // true System.out.println(\"true\"); } } } 说明： String 中的 equals 方法是被重写过的，因为 object 的 equals 方法是比较的对象的内存地址，而 String 的 equals 方法比较的是对象的值。 当创建 String 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 String 对象。 hashCode 与 equals (重要) 面试官可能会问你：“你重写过 hashcode 和 equals 么，为什么重写equals时必须重写hashCode方法？” hashCode（）介绍 hashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode() 定义在JDK的Object.java中，这就意味着Java中的任何类都包含有hashCode() 函数。 散列表存储的是键值对(key-value)，它的特点是：能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象） 为什么要有 hashCode 我们先以“HashSet 如何检查重复”为例子来说明为什么要有 hashCode： 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashcode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashcode 值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用 equals()方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。（摘自我的Java启蒙书《Head first java》第二版）。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 通过我们可以看出：hashCode() 的作用就是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode()在散列表中才有用，在其它情况下没用。在散列表中hashCode() 的作用是获取对象的散列码，进而确定该对象在散列表中的位置。 hashCode（）与equals（）的相关规定 如果两个对象相等，则hashcode一定也是相同的 两个对象相等，对两个对象分别调用equals方法都返回true 两个对象有相同的hashcode值，它们也不一定是相等的 因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖 hashCode() 的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） 推荐阅读：Java hashCode() 和 equals()的若干问题解答 java中hashmap为什么key的值不一样调用hashcode的hash值为什么相同？ - 知乎 关于hashCode,你一定听说过会重复，那么你见过2个不同的字符串hashCode值却是相同的吗_Java_HD243608836的博客-CSDN博客 关于final关键字的一些总结 final关键字主要用在三个地方：变量、方法、类。 当用final修饰一个变量时： 如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改； 如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。 当用final修饰一个类时：表明这个类不能被继承。final类中的所有成员方法都会被隐式地指定为final方法。 使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的Java版本已经不需要使用final方法进行这些优化了）。类中所有的private方法都隐式地指定为final。 异常处理总结 try 块： 用于捕获异常。其后可接零个或多个catch块，如果没有catch块，则必须跟一个finally块。 catch 块： 用于处理try捕获到的异常。 finally 块： 无论是否捕获或处理异常，finally块里的语句都会被执行。当在try块或catch块中遇到return 语句时，finally语句块将在方法返回之前被执行。 在以下4种特殊情况下，finally块不会被执行： 在finally语句块第一行发生了异常。 因为在其他行，finally块还是会得到执行 在前面的代码中用了System.exit(int)已退出程序。 exit是带参函数 ；若该语句在异常语句之后，finally会执行 程序所在的线程死亡。 关闭CPU。 下面这部分内容来自issue:https://github.com/Snailclimb/JavaGuide/issues/190。 注意： 当try语句和finally语句中都有return语句时，在方法返回之前，finally语句的内容将被执行，并且finally语句的返回值将会覆盖原始的返回值。如下： public static int f(int value) { try { return value * value; } finally { if (value == 2) { return 0; } } } 如果调用 f(2)，返回值将是0，因为finally语句的返回值覆盖了try语句块的返回值。 Java中有些字段不想序列化怎么办？ 对于不想进行序列化的变量，使用transient关键字修饰。 transient关键字的作用是：阻止实例中那些用此关键字修饰的的变量序列化；当对象被反序列化时，被transient修饰的变量值不会被持久化和恢复。transient只能修饰变量，不能修饰类和方法。 BIO、NIO、AIO的区别 BIO (Blocking I/O): 同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。 NIO (New I/O): NIO是一种同步非阻塞的I/O模型，在Java 1.4 中引入了NIO框架，对应 java.nio 包，提供了 Channel , Selector，Buffer等抽象。NIO中的N可以理解为Non-blocking，不单纯是New。它支持面向缓冲的，基于通道的I/O操作方法。 NIO提供了与传统BIO模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞I/O来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发 AIO (Asynchronous I/O): AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的IO模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。AIO 是异步IO的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。对于 NIO 来说，我们的业务线程是在 IO 操作准备好时，得到通知，接着就由这个线程自行进行 IO 操作，IO操作本身是同步的。查阅网上相关资料，我发现就目前来说 AIO 的应用还不是很广泛，Netty 之前也尝试使用过 AIO，不过又放弃了。 值传递？引用传递？ JAVA是值传递！！！ 基本类型作为参数被传递时肯定是值传递；引用类型作为参数被传递时也是值传递，只不过“值”为对应的引用。 static、final、关键字 浅析Java中的final关键字 - Matrix海子 - 博客园 equals()和hashCode() 为什么重写equals()的同时还得重写hashCode() 答案：一般的地方不需要重载hashCode，只有当类需要放在HashTable、HashMap、HashSet等等hash结构的集合时才会重载hashCode，那么为什么要重载hashCode呢？就HashMap来说，好比HashMap就是一个大内存块，里面有很多小内存块，小内存块里面是一系列的对象，可以利用hashCode来查找小内存块hashCode%size(小内存块数量)，所以当equal相等时，hashCode必须相等，而且如果是object对象，必须重载hashCode和equal方法。 两者的关系： equals()相等的两个对象，hashcode()一定相等； equals()不相等的两个对象，hashcode()有可能相等。 hashcode()不等，一定能推出equals()也不等； hashcode()相等，equals()可能相等，也可能不等。 内部类 Java内部类详解 - Matrix海子 - 博客园 静态内部类是内嵌类 静态内部类可以单独初始化: Inner i = new Outer.Inner(); 普通内部类初始化： Outer o = new Outer(); Inner i = o.new Inner(); 接口、抽象类 Java 8 默认方法（Default Methods） - Ebn's Blog Java异常 “崩溃了？不可能，我全 Catch 住了” | Java 异常处理 - 承香墨影 - SegmentFault 思否Java：详解Java中的异常(Error与Exception) - 王晓(Java) - CSDN博客 Java能不能自己写一个类叫java.lang.System/String Java能不能自己写一个类叫java.lang.System/String正确答案 - tang9140的专栏 - CSDN博客classloader 结构，是否可以自己定义一个 java.lang.String 类，为什么？ 双亲代理机制。 - LiuHheng0315 - 博客园 "},"docs/Guide/Java集合.html":{"url":"docs/Guide/Java集合.html","title":"Java集合","keywords":"","body":" java集合框架（深入） - TesterMa - 博客园 Arraylist 与 LinkedList 区别? 1. 是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全； 2. 底层数据结构： Arraylist 底层使用的是 Object 数组；LinkedList 底层使用的是 双向链表 数据结构（JDK1.6之前为循环链表，JDK1.7取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！） 3. 插入和删除是否受元素位置的影响： ① ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 ② LinkedList 采用链表存储，所以对于add(\bE e)方法的插入，删除元素时间复杂度不受元素位置的影响，近似 O（1），如果是要在指定位置i插入和删除元素的话（(add(int index, E element)） 时间复杂度近似为o(n))因为需要先移动到指定位置再插入。 4. 是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。 5. 内存空间占用： ArrayList的空间浪费主要体现在在list列表的结尾会预留一定的容量空间，而LinkedList的空间花费则体现在它的每一个元素都需要消耗比ArrayList更多的空间（因为要存放直接后继和直接前驱以及数据）。 RandomAccess接口 public interface RandomAccess { } 查看源码我们发现实际上 RandomAccess 接口中什么都没有定义。所以，在我看来 RandomAccess 接口不过是一个标识罢了。标识什么？ 标识实现这个接口的类具有随机访问功能。 在 binarySearch（）方法中，它要判断传入的list 是否 RamdomAccess 的实例，如果是，调用indexedBinarySearch（）方法，如果不是，那么调用iteratorBinarySearch（）方法 public static int binarySearch(List> list, T key) { if (list instanceof RandomAccess || list.size() ArrayList 实现了 RandomAccess 接口， 而 LinkedList 没有实现。为什么呢？我觉得还是和底层数据结构有关！ArrayList 底层是数组，而 LinkedList 底层是链表。数组天然支持随机访问，时间复杂度为 O（1），所以称为快速随机访问。链表需要遍历到特定位置才能访问特定位置的元素，时间复杂度为 O（n），所以不支持快速随机访问。，ArrayList 实现了 RandomAccess 接口，就表明了他具有快速随机访问功能。 RandomAccess 接口只是标识，并不是说 ArrayList 实现 RandomAccess 接口才具有快速随机访问功能的！ 下面再总结一下 list 的遍历方式选择： 实现了 RandomAccess 接口的list，优先选择普通 for 循环 ，其次 foreach, 未实现 RandomAccess接口的list，优先选择iterator遍历（foreach遍历底层也是通过iterator实现的,），大size的数据，千万不要使用普通for循环 说一说 ArrayList 的扩容机制吧 以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为10。 确认大小 // 添加元素之前，先调用ensureCapacityInternal方法 // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); // 判断是否超出数据容量， 是则扩容 ensureExplicitCapacity(minCapacity); grow方法 // grow()代码里————我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity >> 1); hugeCapacity() 方法。 从上面 grow() 方法源码我们知道： 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) hugeCapacity() 方法来比较 minCapacity 和 MAX_ARRAY_SIZE，如果minCapacity大于最大容量，则新容量则为Integer.MAX_VALUE，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 Integer.MAX_VALUE - 8。 应用程序可以使用ensureCapacity操作来增加 ArrayList 实例的容量。这可以减少递增式再分配的数量。 这里补充一点比较重要，但是容易被忽视掉的知识点： java 中的 length属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了 length 这个属性. java 中的 length() 方法是针对字符串说的,如果想看这个字符串的长度则用到 length() 这个方法. java 中的 size() 方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看! Map HashMap： JDK1.8之前HashMap由数组+链表组成的，数组是HashMap的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间 LinkedHashMap： LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。详细可以查看：《LinkedHashMap 源码详细分析（JDK1.8）》 Hashtable： 数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的 TreeMap： 红黑树（自平衡的排序二叉树） HashMap 和 Hashtable 的区别 线程是否安全： HashMap 是非线程安全的，HashTable 是线程安全的；HashTable 内部的方法基本都经过synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap 吧！）； 效率： 因为线程安全的问题，HashMap 要比 HashTable 效率高一点。另外，HashTable 基本被淘汰，不要在代码中使用它； 对Null key 和Null value的支持： HashMap 中，null 可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为 null。。但是在 HashTable 中 put 进的键值只要有一个 null，直接抛出 NullPointerException。 初始容量大小和每次扩充容量大小的不同 ： ①创建时如果不指定容量初始值，Hashtable 默认的初始大小为11，之后每次扩充，容量变为原来的2n+1。HashMap 默认的初始化大小为16。之后每次扩充，容量变为原来的2倍。②创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为2的幂次方大小（HashMap 中的tableSizeFor()方法保证，下面给出了源代码）。也就是说 HashMap 总是使用2的幂作为哈希表的大小,后面会介绍到为什么是2的幂次方。 底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。 Collections.synchronizedMap怎么实现线程安全的： 为什么Hashtable 是不允许键或值为 null 的，HashMap 的键值则都可以为 null？ 这是因为Hashtable使用的是安全失败机制（fail-safe），这种机制会使你此次读到的数据不一定是最新的数据。 如果你使用null值，就会使得其无法判断对应的key是不存在还是为空，因为你无法再调用一次contain(key）来对key是否存在进行判断，ConcurrentHashMap同理。 fail-fast是什么 快速失败（fail—fast）是java集合中的一种机制， 在用迭代器遍历一个集合对象时，如果遍历过程中对集合对象的内容进行了修改（增加、删除、修改），则会抛出Concurrent Modification Exception。 迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。 集合在被遍历期间如果内容发生变化，就会改变modCount的值。 每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常，终止遍历。 Tip：这里异常的抛出条件是检测到 modCount != expectedmodCount 这个条件。如果集合发生变化时修改modCount值刚好又设置为了expectedmodCount值，则异常不会抛出。 java.util包下的集合类都是快速失败的，不能在多线程下发生并发修改（迭代过程中被修改）算是一种安全机制吧。 安全失败（fail—safe）大家也可以了解下，java.util.concurrent包下的容器都是安全失败，可以在多线程下并发使用，并发修改。 HashMap的底层实现 掌握 HashMap 看这一篇文章就够了 HashMap面试题解答 - windpoplar - 博客园 面试：HashMap 夺命二十一问！ 扰动函数 HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) & hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。 所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。 JDK 1.8 HashMap 的 hash 方法源码: JDK 1.8 的 hash方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。 static final int hash(Object key) { int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // >>>:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); } 对比一下 JDK1.7的 HashMap 的 hash 方法源码. static int hash(int h) { // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h >>> 20) ^ (h >>> 12); return h ^ (h >>> 7) ^ (h >>> 4); } 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。 深入理解 hashcode() 和 HashMap 中的hash 算法_Java_nO0b-CSDN博客 HashMap原理以及为什么需要同时实现equals和hashcode - 简书 java中hashmap为什么key的值不一样调用hashcode的hash值为什么相同？ - 知乎 数据结构 在Java7叫Entry，在Java8中叫Node。 1.8之前 链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。 1.8之后 相比于之前的版本， JDK1.8之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。 新的Entry节点在插入链表的时候，是怎么插入的么？ Java8之前是头插法，就是说新来的值会取代原有的值，原有的值就顺推到链表中去，就像上面的例子一样，因为写这个代码的作者认为后来的值被查找的可能性更大一点，提升查找的效率。 但是，在Java8之后，都是所用尾部插入了。 为什么改为尾插法 因在多线程插入resize扩容rehash到另一个entry时产生环形链表。 Java8为什么不会出问题 因为Java8之后链表有红黑树的部分，大家可以看到代码已经多了很多if else的逻辑判断了，红黑树的引入巧妙的将原本O(n)的时间复杂度降低到了O(logn)。 使用头插会改变链表的上的顺序，但是如果使用尾插，在扩容时会保持链表元素原本的顺序，就不会出现链表成环的问题了。 总结： Java7在多线程操作HashMap时可能引起死循环，原因是扩容转移后前后链表顺序倒置，在转移过程中修改了原来链表中节点的引用关系。 Java8在同样的前提下并不会引起死循环，原因是扩容转移后前后链表顺序不变，保持之前节点的引用关系。 为什么当桶中键值对数量大于8才转换成红黑树，数量小于6才转换成链表？ HashMap的工作原理以及代码实现，为什么要转换成红黑树？ - littlecarzz - 博客园 HashMap在JDK1.8及以后的版本中引入了红黑树结构，若桶中链表元素个数大于等于8时，链表转换成树结构；若桶中链表元素个数小于等于6时，树结构还原成链表。因为红黑树的平均查找长度是log(n)，长度为8的时候，平均查找长度为3，如果继续使用链表，平均查找长度为8/2=4，这才有转换为树的必要。链表长度如果是小于等于6，6/2=3，虽然速度也很快的，但是转化为树结构和生成树的时间并不会太短。 还有选择6和8，中间有个差值7可以有效防止链表和树频繁转换。假设一下，如果设计成链表个数超过8则链表转换成树结构，链表个数小于8则树结构转换成链表，如果一个HashMap不停的插入、删除元素，链表个数在8左右徘徊，就会频繁的发生树转链表、链表转树，效率会很低。 HashMap 的长度为什么是2的幂次方 为了能让 HashMap 存取高效，尽量较少碰撞，也就是要尽量把数据分配均匀。我们上面也讲到了过了，Hash 值的范围值-2147483648到2147483647，前后加起来大概40亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个40亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。 这个数组下标的计算方法indexFor方法是“ (n - 1) & hash”。（n代表数组长度）。这也就解释了 HashMap 的长度为什么是2的幂次方。 位运算(&)效率要比代替取模运算(%)高很多，主要原因是位运算直接对内存数据进行操作，不需要转成十进制，因此处理速度非常快。 这个算法应该如何设计呢？ 我们首先可能会想到采用%取余的操作来实现。但是，重点来了：“取余(%)操作中如果除数是2的幂次则等价于与其除数减一的与(&)操作（也就是说 hash%length==hash&(length-1)的前提是 length 是2的 n 次方；）。” 并且 采用二进制位操作 &，相对于%能够提高运算效率，这就解释了 HashMap 的长度为什么是2的幂次方。 HashMap是如何保证其容量一定可以是2^n 的呢 指定容量初始化 HashMap根据用户传入的初始化容量，利用无符号右移和按位或运算等方式计算出第一个大于该数的2的幂。 扩容 HashMap的扩容条件就是当HashMap中的元素个数（size）超过临界值（threshold）时就会自动扩容。 threshold = loadFactor * capacity。 loadFactor是装载因子，表示HashMap满的程度，默认值为0.75f，设置成0.75有一个好处，那就是0.75正好是3/4，而capacity又是2的幂。所以，两个数的乘积都是整数。 当HashMap中的元素个数（size）超过临界值（threshold）时就会自动扩容，扩容成原容量的2倍。 解决Hash冲突的几种方法： 拉链法 每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来。 开放定址法 一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入 。 公式为：fi(key) = (f(key)+di) MOD m (di=1,2,3,……,m-1) 再哈希法 又叫双哈希法，有多个不同的Hash函数，当发生冲突时，使用第二个，第三个，….，等哈希函数 计算地址，直到无冲突。虽然不易发生聚集，但是增加了计算时间。 建立公共溢出区 将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。 全网把Map中的hash()分析的最透彻的文章，别无二家。 ConcurrentHashMap JDK1.7的ConcurrentHashMap： JDK1.8的ConcurrentHashMap（TreeBin: 红黑二叉树节点 Node: 链表节点）： ConcurrentHashMap线程安全的具体实现方式/底层具体实现 JDK1.7 首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。 ConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成。 Segment 实现了 ReentrantLock，所以 Segment 是一种可重入锁，扮演锁的角色。HashEntry 用于存储键值对数据。 使用volatile去修饰了他的数据Value还有下一个节点next。 put()方法逻辑： 首先第一步的时候会尝试获取锁，如果获取失败肯定就有其他线程存在竞争，则利用 scanAndLockForPut() 自旋获取锁。 尝试自旋获取锁。 如果重试的次数达到了 MAX_SCAN_RETRIES 则改为阻塞锁获取，保证能获取成功。 get()方法逻辑： get 逻辑比较简单，只需要将 Key 通过 Hash 之后定位到具体的 Segment ，再通过一次 Hash 定位到具体的元素上。 由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值。 ConcurrentHashMap 的 get 方法是非常高效的，因为整个过程都不需要加锁。 JDK1.8 采用了 CAS + synchronized 来保证并发安全性。 synchronized 只锁定当前链表或红黑二叉树的首节点，这样只要 hash 不冲突，就不会产生并发，效率又提升 N 倍。 把之前的HashEntry改成了Node，但是作用不变，把值和next采用了volatile去修饰，保证了可见性，并且也引入了红黑树，在链表大于一定值的时候会转换（默认是8）。 put()操作步骤： 根据 key 计算出 hashcode 。 判断是否需要进行初始化。 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。 如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。 如果都不满足，则利用 synchronized 锁写入数据。 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。 小结：1.8 在 1.7 的数据结构上做了大的改动，采用红黑树之后可以保证查询效率（O(logn)），甚至取消了 ReentrantLock 改为了 synchronized，这样可以看出在新版的 JDK 中对 synchronized 优化是很到位的。 Comparable 和 Comparator 的区别 comparable接口实际上是出自java.lang包 它有一个 compareTo(Object obj)方法用来排序 comparator接口实际上是出自 java.util 包它有一个compare(Object obj1, Object obj2)方法用来排序 一般我们需要对一个集合使用自定义排序时，我们就要重写compareTo()方法或compare()方法，当我们需要对某一个集合实现两种排序方式，比如一个song对象中的歌名和歌手名分别采用一种排序方法的话，我们可以重写compareTo()方法和使用自制的Comparator方法或者以两个Comparator来实现歌名排序和歌星名排序，第二种代表我们只能使用两个参数版的 Collections.sort(). TreeMap按照Value怎么排序 TreeMap底层是根据红黑树的数据结构构建的，默认是根据key的自然排序来组织（比如integer的大小，String的字典排序）。所以，TreeMap只能根据key来排序，是不能根据value来排序的（否则key来排序根本就不能形成TreeMap）。 今天有个需求，就是要根据treeMap中的value排序。所以网上看了一下，大致的思路是把TreeMap的EntrySet转换成list，然后使用Collections.sor排序 List> list = new ArrayList>(map.entrySet()); Collections.sort(list,new Comparator>() { public int compare(Map.Entry o1, Map.Entry o2) { return o1.getValue().compareTo(o2.getValue()); } }); for (Map.Entry entry : list) { System.out.println(entry.getKey()+\"---\"+entry.getValue()); } "},"docs/Guide/Java并发.html":{"url":"docs/Guide/Java并发.html","title":"Java并发","keywords":"","body":"说说并发与并行的区别? 并发： 同一时间段，多个任务都在执行 (单位时间内不一定同时执行)； 并行： 单位时间内，多个任务同时执行。 使用多线程可能带来什么问题? 并发编程的目的就是为了能提高程序的执行效率提高程序运行速度，但是并发编程并不总是能提高程序运行速度的，而且并发编程可能会遇到很多问题，比如：内存泄漏、上下文切换、死锁还有受限于硬件和软件的资源闲置问题。 什么是上下文切换? 多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。 概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。 上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。 Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。 什么是线程死锁?如何避免死锁? 什么是死锁 多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。 如下图所示，线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。 产生死锁必须具备以下四个条件： 互斥条件：该资源任意一个时刻只由一个线程占用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件:线程已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。 如何避免死锁 我们只要破坏产生死锁的四个条件中的其中一个就可以了。 破坏互斥条件 这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。 破坏请求与保持条件 一次性申请所有的资源。 破坏不剥夺条件 占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件 靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。 线程有哪些基本状态？ Java 线程在运行的生命周期中的指定时刻只可能处于下面6种不同状态的其中一个状态（图源《Java 并发编程艺术》4.1.4节）。 线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换。Java 线程状态变迁如下图所示（图源《Java 并发编程艺术》4.1.4节）： 线程创建之后它将处于 NEW（新建） 状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 cpu 时间片（timeslice）后就处于 RUNNING（运行） 状态。 操作系统隐藏 Java虚拟机（JVM）中的 READY 和 RUNNING 状态，它只能看到 RUNNABLE 状态（图源：HowToDoInJava：Java Thread Life Cycle and Thread States），所以 Java 系统一般将这两个状态统称为 RUNNABLE（运行中） 状态 。 当线程执行 wait()方法之后，线程进入 WAITING（等待）状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 TIME_WAITING(超时等待) 状态相当于在等待状态的基础上增加了超时限制，比如通过 sleep（long millis）方法或 wait（long millis）方法可以将 Java 线程置于 TIMED WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 BLOCKED（阻塞） 状态。线程在执行 Runnable 的run()方法之后将会进入到 TERMINATED（终止） 状态。 Java线程状态 六种：NEW、RUNNABLE、BLOCKED、WAITING、TIMED_WAITING、TERMINATED 说说 sleep() 方法和 wait() 方法区别和共同点? 两者最主要的区别在于：sleep 方法没有释放锁，而 wait 方法释放了锁 。 两者都可以暂停线程的执行。 wait 通常被用于线程间交互/通信，sleep 通常被用于暂停执行。 wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify() 或者 notifyAll() 方法。sleep() 方法执行完成后，线程会自动苏醒。或者可以使用 wait(long timeout)超时后线程会自动苏醒。 说说 notify() 和 notifyAll() 方法的区别？ 你真的懂wait、notify和notifyAll吗 - 简书 两个概念：锁池 EntryList、等待池 WaitSet 锁池： 假设线程A已经拥有了某个对象（不是类）的锁，而其它线程B,C想要调用这个对象的某个某个synchronized方法(或者块)之前必须获得该对象锁的拥有权，而恰巧该对象的锁目前正被A所占有，此时B， C线程就会被阻塞，进入一个地方去等待锁的释放，这个地方便是该对象的锁池。 等待池 假设线程A调用了某个对象的wait方法，线程A就会释放该对象的锁，同时线程A就进入到了该对象的等待池中，进入等待池中的线程不会去竞争该对象的锁。 notify和notifAll的区别 notifyAll： 会让所有处于等待池的线程全部进入锁池去竞争获取锁的机会 notify： 只会随机选取一个处于等待池中的线程进入锁池去竞争获取锁的机会。 notifyAll() 将所有WaitSet中的线程从等待池唤醒，全部进入锁池竞争去sync锁，最终也只有一个线程能获取锁去执行，唤醒+竞争锁池本身是线程上下文的重操作，对性能产生不良影响。滥用notifyAll()有可能导致“惊群效应”。 notify() 是对notifyAll()的一个优化，但它有很精确的应用场景，并且要求正确使用。不然可能导致死锁。正确的场景应该是 WaitSet中等待的是相同的条件，唤醒任一个都能正确处理接下来的事项，如果唤醒的线程无法正确处理，务必确保继续notify()下一个线程，并且自身需要重新回到WaitSet中（参见下一条）。导致死锁的原因是notify()随机唤醒了一条线程，但它既无法正确改变条件，也不叫醒另一个兄弟来搞，就会产生一个情况：锁池中的队列空了，等待池中有一堆线程，但不会再被唤醒永远等待。 wait() 应配合while循环使用，不应使用if，务必在wait()调用前后都检查条件，如果不满足，必须调用notify()唤醒另外的线程来处理，自己继续wait()直至条件满足再往下执行。 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？ 这是另一个非常经典的 java 多线程面试问题，而且在面试中会经常被问到。很简单，但是很多人都会答不上来！ new 一个 Thread，线程进入了新建状态，调用 start() 方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start() 会执行线程的相应准备工作，然后自动执行 run() 方法的内容，这是真正的多线程工作。 而直接执行 run() 方法，会把 run 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。 总结： 调用 start 方法方可启动线程并使线程进入就绪状态，而 run 方法只是 thread 的一个普通方法调用，还是在主线程里执行。 synchronized 关键字 面试官和我扯了半个小时的synchronized，最后他输了 在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。 依赖底层操作系统的 mutex 相关指令实现，加锁解锁需要在用户态和内核态之间切换，性能损耗非常明显。 研究人员发现，大多数对象的加锁和解锁都是在特定的线程中完成。也就是出现线程竞争锁的情况概率比较低。他们做了一个实验，找了一些典型的软件，测试同一个线程加锁解锁的重复率，如下图所示，可以看到重复加锁比例非常高。早期JVM 有 19% 的执行时间浪费在锁上。 庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 说说你怎么使用synchronized 关键字 synchronized关键字最主要的三种使用方式： 修饰实例方法： 作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁 修饰静态方法： 也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 修饰代码块： 指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 总结： synchronized 关键字加到 static 静态方法和 synchronized(class) 代码块上都是是给 Class 类上锁。synchronized 关键字加到实例方法上是给对象实例上锁。尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓存功能！ 下面我以一个常见的面试题为例讲解一下 synchronized 关键字的具体使用。 面试中面试官经常会说：“单例模式了解吗？来给我手写一下！给我解释一下双重检验锁方式实现单例模式的原理呗！” 双重校验锁实现对象单例（线程安全） public class Singleton { private volatile static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) { //类对象加锁 synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } 另外，需要注意 uniqueInstance 采用 volatile 关键字修饰也是很有必要。 uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1->3->2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 讲一下synchronized 关键字的底层原理 synchronized 关键字底层原理属于 JVM 层面。 ① synchronized 同步语句块的情况 public class SynchronizedDemo { public void method() { synchronized (this) { System.out.println(\"synchronized 代码块\"); } } } 通过 JDK 自带的 javap 命令查看 SynchronizedDemo 类的相关字节码信息：首先切换到类的对应目录执行 javac SynchronizedDemo.java 命令生成编译后的 .class 文件，然后执行javap -c -s -v -l SynchronizedDemo.class。 从上面我们可以看出： 有两个monitorexit指令的原因是：为了保证抛异常的情况下也能释放锁，所以javac为同步代码块添加了一个隐式的try-finally，在finally中会调用monitorexit命令释放锁。 synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。 当执行 monitorenter 指令时，线程试图获取锁也就是获取 monitor(monitor对象存在于每个Java对象的对象头中，synchronized 锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因) 的持有权。当计数器为0则可以成功获取，获取后将锁计数器设为1也就是加1。相应的在执行 monitorexit 指令后，将锁计数器设为0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。 ② synchronized 修饰方法的的情况 public class SynchronizedDemo2 { public synchronized void method() { System.out.println(\"synchronized 方法\"); } } synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法，JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。 JDK1.6 之后对synchronized锁的底层优化 JDK1.6 对锁的实现引入了大量的优化，如偏向锁、轻量级锁、自旋锁、适应性自旋锁、锁消除、锁粗化等技术来减少锁操作的开销。 锁主要存在四中状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。 ①偏向锁 引入偏向锁的目的和引入轻量级锁的目的很像，他们都是为了没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。但是不同是：轻量级锁在无竞争的情况下使用 CAS 操作去代替使用互斥量。而偏向锁在无竞争的情况下会把整个同步都消除掉。 偏向锁的“偏”就是偏心的偏，它的意思是会偏向于第一个获得它的线程，如果在接下来的执行中，该锁没有被其他线程获取，那么持有偏向锁的线程就不需要进行同步！关于偏向锁的原理可以查看《深入理解Java虚拟机：JVM高级特性与最佳实践》第二版的13章第三节锁优化。 但是对于锁竞争比较激烈的场合，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，否则会得不偿失，需要注意的是，偏向锁失败后，并不会立即膨胀为重量级锁，而是先升级为轻量级锁。 ② 轻量级锁 倘若偏向锁失败，虚拟机并不会立即升级为重量级锁，它还会尝试使用一种称为轻量级锁的优化手段(1.6之后加入的)。轻量级锁不是为了代替重量级锁，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗，因为使用轻量级锁时，不需要申请互斥量。另外，轻量级锁的加锁和解锁都用到了CAS操作。 关于轻量级锁的加锁和解锁的原理可以查看《深入理解Java虚拟机：JVM高级特性与最佳实践》第二版的13章第三节锁优化。 轻量级锁能够提升程序同步性能的依据是“对于绝大部分锁，在整个同步周期内都是不存在竞争的”，这是一个经验数据。如果没有竞争，轻量级锁使用 CAS 操作避免了使用互斥操作的开销。但如果存在锁竞争，除了互斥量开销外，还会额外发生CAS操作，因此在有锁竞争的情况下，轻量级锁比传统的重量级锁更慢！如果锁竞争激烈，那么轻量级将很快膨胀为重量级锁！ ③ 自旋锁和自适应自旋 轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。 互斥同步对性能最大的影响就是阻塞的实现，因为挂起线程/恢复线程的操作都需要转入内核态中完成（用户态转换到内核态会耗费时间）。 一般线程持有锁的时间都不是太长，所以仅仅为了这一点时间去挂起线程/恢复线程是得不偿失的。 所以，虚拟机的开发团队就这样去考虑：“我们能不能让后面来的请求获取锁的线程等待一会而不被挂起呢？看看持有锁的线程是否很快就会释放锁”。为了让一个线程等待，我们只需要让线程执行一个忙循环（自旋），这项技术就叫做自旋。 百度百科对自旋锁的解释： 何谓自旋锁？它是为实现保护共享资源而提出一种锁机制。其实，自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，\"自旋\"一词就是因此而得名。 自旋锁在 JDK1.6 之前其实就已经引入了，不过是默认关闭的，需要通过--XX:+UseSpinning参数来开启。JDK1.6及1.6之后，就改为默认开启的了。需要注意的是：自旋等待不能完全替代阻塞，因为它还是要占用处理器时间。如果锁被占用的时间短，那么效果当然就很好了！反之，相反！自旋等待的时间必须要有限度。如果自旋超过了限定次数任然没有获得锁，就应该挂起线程。自旋次数的默认值是10次，用户可以修改--XX:PreBlockSpin来更改。 另外,在 JDK1.6 中引入了自适应的自旋锁。自适应的自旋锁带来的改进就是：自旋的时间不在固定了，而是和前一次同一个锁上的自旋时间以及锁的拥有者的状态来决定，虚拟机变得越来越“聪明”了。 ④ 锁消除 锁消除理解起来很简单，它指的就是虚拟机即使编译器在运行时，如果检测到那些共享数据不可能存在竞争，那么就执行锁消除。锁消除可以节省毫无意义的请求锁的时间。 ⑤ 锁粗化 原则上，我们在编写代码的时候，总是推荐将同步块的作用范围限制得尽量小，——直在共享数据的实际作用域才进行同步，这样是为了使得需要同步的操作数量尽可能变小，如果存在锁竞争，那等待线程也能尽快拿到锁。 大部分情况下，上面的原则都是没有问题的，但是如果一系列的连续操作都对同一个对象反复加锁和解锁，那么会带来很多不必要的性能消耗。 Synchronized 和 ReentrantLock 的区别 ① 两者都是可重入锁 两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 ② synchronized 依赖于 JVM 而 ReentrantLock 依赖于 API synchronized 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。ReentrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock() 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。 ③ ReentrantLock 比 synchronized 增加了一些高级功能 相比synchronized，ReentrantLock增加了一些高级功能。主要来说主要有三点：①等待可中断；②可实现公平锁；③可实现选择性通知（锁可以绑定多个条件） ReentrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 ReentrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 ReentrantLock默认情况是非公平的，可以通过 ReentrantLock类的ReentrantLock(boolean fair)构造方法来制定是否是公平的。 synchronized关键字与 wait() 和 notify() / notifyAll() 方法相结合可以实现等待/通知机制，ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition() 方法。Condition是JDK1.5之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify()/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，这个功能非常重要，而且是Condition接口默认提供的。而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程。 如果你想使用上述功能，那么选择ReentrantLock是一个不错的选择。 ④ 性能已不是选择标准 volatile关键字 在 JDK1.2 之前，Java的内存模型实现总是从主存（即共享内存）读取变量，是不需要进行特别的注意的。而在当前的 Java 内存模型下，线程可以把变量保存本地内存（比如机器的寄存器）中，而不是直接在主存中进行读写。这就可能造成一个线程在主存中修改了一个变量的值，而另外一个线程还继续使用它在寄存器中的变量值的拷贝，造成数据的不一致。 要解决这个问题，就需要把变量声明为volatile，这就指示 JVM，这个变量是不稳定的，每次使用它都到主存中进行读取。 volatile 关键字的主要作用： 保证变量的内存可见性 防止指令重排序 对于volatile变量，读操作时JMM会把工作内存中对应的值设为无效，要求线程从主内存中读取数据; 写操作时JMM会把工作内存中对应的数据刷新到主内存中，这种情况下，其它线程就可以读取变量的最新值 底层实现： 汇编代码： 0x01a3de1d: movb $0x0,0x1104800(%esi); 0x01a3de24: **lock** addl $0x0,(%esp); 这个lock前缀指令相当于上述的内存屏障，提供了以下保证： 1、将当前CPU缓存行的数据写回到主内存； 2、这个写回内存的操作会导致在其它CPU里缓存了该内存地址的数据无效。 lock前缀指令其实就相当于一个内存屏障。内存屏障是一组CPU处理指令，用来实现对内存操作的顺序限制。volatile的底层就是通过内存屏障来实现的。 编译器和执行器 可以在保证输出结果一样的情况下对指令重排序，使性能得到优化。插入一个内存屏障，相当于告诉CPU和编译器先于这个命令的必须先执行，后于这个命令的必须后执行。 内存屏障另一个作用是强制更新一次不同CPU的缓存。例如，一个写屏障会把这个屏障前写入的数据刷新到缓存，这样任何试图读取该数据的线程将得到最新值，而不用考虑到底是被哪个cpu核心或者哪个CPU执行的。这正是volatile实现内存可见性的基础。 内存屏障细说来有写屏障、读屏障、读写屏障，而且内存屏障的实现依赖于编译器和机器两部分。 synchronized关键字和volatile关键字比较 volatile关键字是线程同步的轻量级实现，所以volatile性能肯定比synchronized关键字要好。但是volatile关键字只能用于变量而synchronized关键字可以修饰方法以及代码块。synchronized关键字在JavaSE1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁以及其它各种优化之后执行效率有了显著提升，实际开发中使用 synchronized 关键字的场景还是更多一些。 多线程访问volatile关键字不会发生阻塞，而synchronized关键字可能会发生阻塞 volatile关键字能保证数据的可见性，但不能保证数据的原子性。synchronized关键字两者都能保证。 volatile关键字主要用于解决变量在多个线程之间的可见性，而 synchronized关键字解决的是多个线程之间访问资源的同步性。 MESI MESI（Modified Exclusive Shared Or Invalid）(也称为伊利诺斯协议，是因为该协议由伊利诺斯州立大学提出）是一种广泛使用的支持写回策略的缓存一致性协议。 线程池 Java线程池实现原理及其在美团业务中的实践 - 美团技术团队 为什么要用线程池 池化技术相比大家已经屡见不鲜了，线程池、数据库连接池、Http 连接池等等都是对这个思想的应用。池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。 线程池提供了一种限制和管理资源（包括执行一个任务）。 每个线程池还维护一些基本统计信息，例如已完成任务的数量。 这里借用《Java 并发编程的艺术》提到的来说一下使用线程池的好处： 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 实现Runnable接口和Callable接口的区别 Runnable自Java 1.0以来一直存在，但Callable仅在Java 1.5中引入,目的就是为了来处理Runnable不支持的用例。Runnable 接口不会返回结果或抛出检查异常，但是Callable 接口可以。所以，如果任务不需要返回结果或抛出异常推荐使用 Runnable 接口，这样代码看起来会更加简洁。 工具类 Executors 可以实现 Runnable 对象和 Callable 对象之间的相互转换。（Executors.callable（Runnable task）或 Executors.callable（Runnable task，Object resule））。 执行execute()方法和submit()方法的区别是什么呢？ execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否； submit()方法用于提交需要返回值的任务。线程池会返回一个 Future 类型的对象，通过这个 Future 对象可以判断任务是否执行成功，并且可以通过 Future 的 get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用 get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 如何创建线程池 《阿里巴巴Java开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险 Executors 返回线程池对象的弊端如下： FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE ，可能堆积大量的请求，从而导致OOM。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致OOM。 方式一：通过构造方法实现 方式二：通过Executor 框架的工具类Executors来实现 我们可以创建三种类型的ThreadPoolExecutor： FixedThreadPool ： 该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 SingleThreadExecutor： 方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 CachedThreadPool： 该方法返回一个可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。 对应Executors工具类中的方法如图所示： ThreadPoolExecutor构造函数重要参数分析 ThreadPoolExecutor 3 个最重要的参数： corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。 ThreadPoolExecutor其他常见参数: keepAliveTime:当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁； unit : keepAliveTime 参数的时间单位。 threadFactory :executor 创建新线程的时候会用到。 handler :饱和策略。关于饱和策略下面单独介绍一下。 ThreadPoolExecutor饱和策略定义 如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任时，ThreadPoolTaskExecutor 定义一些策略: ThreadPoolExecutor.AbortPolicy：抛出 RejectedExecutionException来拒绝新任务的处理。 ThreadPoolExecutor.CallerRunsPolicy：调用执行自己的线程运行任务。您不会任务请求。但是这种策略会降低对于新任务提交速度，影响程序的整体性能。另外，这个策略喜欢增加队列容量。如果您的应用程序可以承受此延迟并且你不能任务丢弃任何一个任务请求的话，你可以选择这个策略。 任务会交个上层线程（主线程）执行，导致主线程既要处理其他任务，又要忙碌处理线程池的源源不断的大量任务，导致hang住。 ThreadPoolExecutor.DiscardPolicy： 不处理新任务，直接丢弃掉，不抛异常。 ThreadPoolExecutor.DiscardOldestPolicy： 此策略将丢弃最早的未处理的任务请求。 我们在代码中模拟了 10 个任务，我们配置的核心线程数为 5 、等待队列容量为 100 ，所以每次只可能存在 5 个任务同时执行，剩下的 5 个任务会被放到等待队列中去。当前的 5 个任务之行完成后，才会之行剩下的 5 个任务。 ThreadPoolExecutor任务队列 使用直接提交策略，也即SynchronousQueue：首先SynchronousQueue是无界的，也就是说他存数任务的能力是没有限制的，但是由于该Queue本身的特性，在某次添加元素后必须等待其他线程取走后才能继续添加。在这里不是核心线程便是新创建的线程。 使用无界队列策略，即LinkedBlockingQueue：对于无界队列来说，总是可以加入的（资源耗尽，当然另当别论）。换句说，永远也不会触发产生新的线程！corePoolSize大小的线程数会一直运行， 有界队列，使用ArrayBlockingQueue：这个是最为复杂的使用，所以JDK不推荐使用也有些道理。与上面的相比，最大的特点便是可以防止资源耗尽的情况发生。 java自带线程池和队列详细讲解 - OSCHINA 线程池怎么回收多余的线程的 说白了整个 Worker 的生命周期大致可以理解为：线程池干活了（execute() / submit()），然后就是正式干活了（runWorker()），使用 getTask() 获取任务（中间会有一系列的判断（corePoolSize 是否达到，任务队列是否满了，线程池是否达到了 maximumPoolSize，超时等），如果没有 task 了，就进行后期的扫尾工作并且从 workers 中移除 worker。 线程池中的空余线程是如何被回收的 怎么回收核心线程： allowCoreThreadTimeOut设置为true 调用shutdown方法 ThreadLocal 通常情况下，我们创建的变量是可以被任何一个线程访问并修改的。如果想实现每一个线程都有自己的专属本地变量该如何解决呢？ JDK中提供的ThreadLocal类正是为了解决这样的问题。 ThreadLocal类主要解决的就是让每个线程绑定自己的值，可以将ThreadLocal类形象的比喻成存放数据的盒子，盒子中可以存储每个线程的私有数据。 如果你创建了一个ThreadLocal变量，那么访问这个变量的每个线程都会有这个变量的本地副本，这也是ThreadLocal变量名的由来。他们可以使用 get（） 和 set（） 方法来获取默认值或将其值更改为当前线程所存的副本的值，从而避免了线程安全问题。 //与此线程有关的ThreadLocal值。由ThreadLocal类维护 ThreadLocal.ThreadLocalMap threadLocals = null; //与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护 ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; 从上面Thread类 源代码可以看出Thread 类中有一个 threadLocals 和 一个 inheritableThreadLocals 变量，它们都是 ThreadLocalMap 类型的变量,我们可以把 ThreadLocalMap 理解为ThreadLocal 类实现的定制化的 HashMap。默认情况下这两个变量都是null，只有当前线程调用 ThreadLocal 类的 set或get方法时才创建它们，实际上调用这两个方法的时候，我们调用的是ThreadLocalMap类对应的 get()、set()方法。 ThreadLocal类的set()方法 public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } 通过上面这些内容，我们足以通过猜测得出结论：最终的变量是放在了当前线程的 ThreadLocalMap 中，并不是存在 ThreadLocal 上，ThreadLocal 可以理解为只是ThreadLocalMap的封装，传递了变量值。 ThrealLocal 类中可以通过Thread.currentThread()获取到当前线程对象后，直接通过getMap(Thread t)可以访问到该线程的ThreadLocalMap对象。 每个Thread中都具备一个ThreadLocalMap，而ThreadLocalMap可以存储以ThreadLocal为key的键值对。 比如我们在同一个线程中声明了两个 ThreadLocal 对象的话，会使用 Thread内部都是使用仅有那个ThreadLocalMap 存放数据的，ThreadLocalMap的 key 就是 ThreadLocal对象，value 就是 ThreadLocal 对象调用set方法设置的值。ThreadLocal 是 map结构是为了让每个线程可以关联多个 ThreadLocal变量。这也就解释了 ThreadLocal 声明的变量为什么在每一个线程都有自己的专属本地变量。 ThreadLocalMap是ThreadLocal的静态内部类。 ThreadLocal内存泄露问题 ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用,而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。这样一来，ThreadLocalMap 中就会出现key为null的Entry。假如我们不做任何措施的话，value 永远无法被GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。使用完 ThreadLocal方法后 最好手动调用remove()方法。 ThreadLocal导致内存泄露的错误行为# 1.使用static的ThreadLocal，延长了ThreadLocal的生命周期，可能导致内存泄漏 2.分配使用了ThreadLocal又不再调用get()，set()，remove()方法 就会导致内存泄漏 3.当使用线程池时，即当前线程不一定会退出（比如固定大小的线程池），这样将一些大对象设置到ThreadLocal中，可能会导致系统出现内存泄露（当对象不再使用时，因为引用存在，无法被回收） ThreadLocal导致内存泄露的根源# 首先需要明确一点：ThreadLocal本身的设计是不会导致内存泄露的，原因更多是使用不当导致的！ ThreadLocalMap对象被Thread对象所持有，当线程退出时，Thread类执行清理操作，比如清理ThreadLocalMap；否则该ThreadLocalMap对象的引用并不会被回收。 //先回顾一下：Thread的exit方法 /** * This method is called by the system to give a Thread * a chance to clean up before it actually exits. */ private void exit() { if (group != null) { group.threadTerminated(this); group = null; } /* Aggressively null out all reference fields: see bug 4006245 */ target = null; /* Speed the release of some of these resources */ threadLocals = null;//清空threadLocalMap的引用 inheritableThreadLocals = null; inheritedAccessControlContext = null; blocker = null; uncaughtExceptionHandler = null; } 根源：由于Entry的key弱引用特性（见注意），当每次GC时JVM会主动将无用的弱引用回收掉，因此当ThreadLocal外部没有强引用依赖时，就会被自动回收，这样就可能造成当ThreadLocal被回收时，相当于将Map中的key设置为null，但问题是该key对应的entry和value并不会主动被GC回收， 当Entry和value未被主动回收时，除非当前线程死亡，否则线程对于Entry的强引用会一直存在，从而导致内存泄露 建议： 当希望回收对象，最好使用ThreadLocal.remove()方法将该变量主动移除，告知JVM执行GC回收 注意： ThreadLocal本身不是弱引用的，Entry继承了WeakReference，同时Entry又将自身的key封装成弱引用，所有真正的弱引用是Entry的key，只不过恰好Entry的key是ThreadLocal！！ static class Entry extends WeakReference> { Object value; Entry(ThreadLocal k, Object v) { //这里才是真正的弱引用！！ super(k);//将key变成了弱引用！而key恰好又是ThreadLocal！ value = v; } } public class WeakReference extends Reference { public WeakReference(T referent) { super(referent); } public WeakReference(T referent, ReferenceQueue q) { super(referent, q); } } 仿ThreadLocalMap结构测试# public class AnalogyThreadLocalDemo { public static void main(String[] args) { HashMap map = new HashMap(); Obj o1 = new Obj(); Obj o2 = new Obj(); map.put(o1, \"o1\"); map.put(o2, \"o2\"); o1 = null; System.gc(); System.out.println(\"##########o1 gc:\" + map); o2 = null; System.gc(); System.out.println(\"##########o2 gc:\" + map); map.clear(); System.gc(); System.out.println(\"##########GC after map clear:\" + map); } } class Obj { private final String DESC = \"obj exists\"; @Override public String toString() { return DESC; } @Override protected void finalize() throws Throwable { System.out.println(\"##########gc over\"); } } 设置VM options: -verbose:gc -XX:+PrintGCDetails -XX:+PrintTenuringDistribution -XX:+PrintGCTimeStamps Output: 0.316: [GC (System.gc()) Desired survivor size 11010048 bytes, new threshold 7 (max 15) [PSYoungGen: 7911K->1290K(76288K)] 7911K->1298K(251392K), 0.0025504 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 0.319: [Full GC (System.gc()) [PSYoungGen: 1290K->0K(76288K)] [ParOldGen: 8K->1194K(175104K)] 1298K->1194K(251392K), [Metaspace: 3310K->3310K(1056768K)], 0.0215288 secs] [Times: user=0.00 sys=0.00, real=0.02 secs] ##########o1 gc:{obj exists=o1, obj exists=o2} 0.342: [GC (System.gc()) Desired survivor size 11010048 bytes, new threshold 7 (max 15) [PSYoungGen: 1310K->64K(76288K)] 2504K->1258K(251392K), 0.0002418 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 0.342: [Full GC (System.gc()) [PSYoungGen: 64K->0K(76288K)] [ParOldGen: 1194K->964K(175104K)] 1258K->964K(251392K), [Metaspace: 3322K->3322K(1056768K)], 0.0058113 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] ##########o2 gc:{obj exists=o1, obj exists=o2} 0.348: [GC (System.gc()) Desired survivor size 11010048 bytes, new threshold 7 (max 15) [PSYoungGen: 1310K->32K(76288K)] 2275K->996K(251392K), 0.0002203 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 0.349: [Full GC (System.gc()) [PSYoungGen: 32K->0K(76288K)] [ParOldGen: 964K->964K(175104K)] 996K->964K(251392K), [Metaspace: 3322K->3322K(1056768K)], 0.0055209 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] ##########gc over ##########gc over ##########GC after map clear:{} Heap PSYoungGen total 76288K, used 3932K [0x000000076af00000, 0x0000000770400000, 0x00000007c0000000) eden space 65536K, 6% used [0x000000076af00000,0x000000076b2d7248,0x000000076ef00000) from space 10752K, 0% used [0x000000076ef00000,0x000000076ef00000,0x000000076f980000) to space 10752K, 0% used [0x000000076f980000,0x000000076f980000,0x0000000770400000) ParOldGen total 175104K, used 964K [0x00000006c0c00000, 0x00000006cb700000, 0x000000076af00000) object space 175104K, 0% used [0x00000006c0c00000,0x00000006c0cf1240,0x00000006cb700000) Metaspace used 3328K, capacity 4500K, committed 4864K, reserved 1056768K class space used 355K, capacity 388K, committed 512K, reserved 1048576K 可以看出，当map.clear()以后，Obj对象才被finalize回收 为什么ThreadLocalMap 设计为ThreadLocal 内部类 主要是说明ThreadLocalMap 是一个线程本地的值，它所有的方法都是private 的，也就意味着除了ThreadLocal 这个类，其他类是不能操作ThreadLocalMap 中的任何方法的，这样就可以对其他类是透明的。同时这个类的权限是包级别的，也就意味着只有同一个包下面的类才能引用ThreadLocalMap 这个类，这也是Thread 为什么可以引用ThreadLocalMap 的原因，因为他们在同一个包下面。 虽然Thread 可以引用ThreadLocalMap，但是不能调用任何ThreadLocalMap 中的方法。这也就是我们平时都是通过ThreadLocal 来获取值和设置值。 但我们调用ThreadLocal 的get 方法的时候，其实我们最后是通过调用ThreadLdocalMap 来获取值的。 到这里，读者应该大概明白了，其实ThreadLdocalMap 对使用者来说是透明的，可以当作空气，我们一值使用的都是ThreadLocal，这样的设计在使用的时候就显得简单，然后封装性又特别好。 ThreadLdocalMap 什么时候开始和Thread 进行绑定的呢 在第一次调用ThreadLocal set() 方法的时候开始绑定的，来我们看下set 方法的源码 public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else //第一次的时候进来这里，因为ThreadLocalMap 还没和Thread 绑定 createMap(t, value); } //这个时候开始创建一个新的ThreadLocalMap 赋值给Thread 进行绑定 void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } 魔数0x61c88647与碰撞解决 机智的读者肯定发现ThreadLocalMap并没有使用链表或红黑树去解决hash冲突的问题，而仅仅只是使用了数组来维护整个哈希表，那么重中之重的散列性要如何保证就是一个很大的考验 ThreadLocalMap通过结合三个巧妙的设计去解决这个问题： 1.Entry的key设计成弱引用，因此key随时可能被GC（也就是失效快），尽量多的面对空槽 2.(单个ThreadLocal时)当遇到碰撞时，通过线性探测的开放地址法解决冲突问题 3.(多个ThreadLocal时)引入了神奇的0x61c88647，增强其的散列性，大大减少碰撞几率 之所以不用累加而用该值，笔者认为可能跟其找最近的空槽有关（跳跃查找比自增1查找用来找空槽可能更有效一些，因为有了更多可选择的空间spreading out），同时也跟其良好的散列性有关 0x61c88647与黄金比例、Fibonacci 数有关，读者可参见What is the meaning of 0x61C88647 constant in ThreadLocal.java ThreadLocal总结# ThreadLocal 并不解决线程间共享数据的问题 ThreadLocal 通过隐式的在不同线程内创建独立实例副本避免了实例线程安全的问题 每个线程持有一个 Map 并维护了 ThreadLocal 对象与具体实例的映射，该 Map 由于只被持有它的线程访问，故不存在线程安全以及锁的问题 ThreadLocalMap 的 Entry 对 ThreadLocal 的引用为弱引用，避免了 ThreadLocal 对象无法被回收的问题 ThreadLocalMap 的 set 方法通过调用 replaceStaleEntry 方法回收键为 null 的 Entry 对象的值（即为具体实例）以及 Entry 对象本身从而防止内存泄漏 ThreadLocal 适用于变量在线程间隔离且在方法间共享的场景 Atomic原子类 所以，所谓原子类说简单点就是具有原子/原子操作特征的类。 并发包 java.util.concurrent 的原子类都存放在java.util.concurrent.atomic下,如下图所示。 AtomicInteger原理分析 AtomicInteger 类的部分源码： // setup to use Unsafe.compareAndSwapInt for updates（更新操作时提供“比较并替换”的作用） private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\"value\")); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; public final boolean compareAndSet(int expect, int update) { return unsafe.compareAndSwapInt(this, valueOffset, expect, update); } AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。 CAS的原理是拿期望的值和原本的一个值作比较，如果相同则更新成新的值。UnSafe 类的 objectFieldOffset() 方法是一个本地方法，这个方法是用来拿到“原来的值”的内存地址，返回值是 valueOffset。另外 value 是一个volatile变量，在内存中可见，因此 JVM 可以保证任何时刻任何线程总能拿到该变量的最新值。 关于 Atomic 原子类这部分更多内容可以查看我的这篇文章：并发编程面试必备：JUC 中的 Atomic 原子类总结 AQS AQS的全称为（AbstractQueuedSynchronizer），这个类在java.util.concurrent.locks包下面。 AQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的ReentrantLock，Semaphore，其他的诸如ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器。 AQS原理 AQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。 看个 AQS(AbstractQueuedSynchronizer)原理图： AQS 使用一个 int 成员变量state来表示同步状态，通过内置的 FIFO 队列来完成获取资源线程的排队工作。AQS 使用 CAS 对该同步状态进行原子操作实现对其值的修改。当state>0时表示已经获取了锁，当state = 0时表示释放了锁。它提供了三个方法（getState()、setState(int newState)、compareAndSetState(int expect,int update)）来对同步状态state进行操作，当然AQS可以确保对state的操作是安全的。 private volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息通过 protected 类型的getState，setState，compareAndSetState进行操作 //返回同步状态的当前值 protected final int getState() { return state; } // 设置同步状态的值 protected final void setState(int newState) { state = newState; } //原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值） protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 主要方法 AQS的设计是基于模板方法模式的，它有一些方法必须要子类去实现的，它们主要有： isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。 tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。 获取资源 这里会涉及到两个变化 新的线程封装成Node节点追加到同步队列中，设置prev节点以及修改当前节点的前置节点的next节点指向自己 通过CAS将tail重新指向新的尾部节点 释放资源 head节点表示获取锁成功的节点，当头结点在释放同步状态时，会唤醒后继节点，如果后继节点获得锁成功，会把自己设置为头结点。 这个过程也是涉及到两个变化 修改head节点指向下一个获得锁的节点 新的获得锁的节点，将prev的指针指向null 这里有一个小的变化，就是设置head节点不需要用CAS，原因是设置head节点是由获得锁的线程来完成的，而同步锁只能由一个线程获得，所以不需要CAS保证，只需要把head节点设置为原首节点的后继节点，并且断开原head节点的next引用即可 资源共享方式 资源有两种共享模式，或者说两种同步方式： 独占模式（Exclusive）：资源是独占的，一次只能一个线程获取。如ReentrantLock。 共享模式（Share）：同时可以被多个线程获取，具体的资源个数可以通过参数指定。如Semaphore/CountDownLatch。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 公平锁、非公平锁 基于AQS的锁(比如ReentrantLock)原理大体是这样: 有一个state变量，初始值为0，假设当前线程为A,每当A获取一次锁，status++. 释放一次，status--.锁会记录当前持有的线程。 当A线程拥有锁的时候，status>0. B线程尝试获取锁的时候会对这个status有一个CAS(0,1)的操作，尝试几次失败后就挂起线程，进入一个等待队列。 如果A线程恰好释放，--status==0, A线程会去唤醒等待队列中第一个线程，即刚刚进入等待队列的B线程，B线程被唤醒之后回去检查这个status的值，尝试CAS(0,1),而如果这时恰好C线程也尝试去争抢这把锁。 非公平锁实现： C直接尝试对这个status CAS(0,1)操作，并成功改变了status的值，B线程获取锁失败，再次挂起，这就是非公平锁，B在C之前尝试获取锁，而最终是C抢到了锁。 公平锁： C发现有线程在等待队列，直接将自己进入等待队列并挂起,B获取锁。 非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。 非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在 tryAcquire 方法中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。 公平锁和非公平锁就这两点区别，如果这两次 CAS 都不成功，那么后面非公平锁和公平锁是一样的，都要进入到阻塞队列等待唤醒。 相对来说，非公平锁会有更好的性能，因为它的吞吐量比较大。当然，非公平锁让获取锁的时间变得更加不确定，可能会导致在阻塞队列中的线程长期处于饥饿状态。 AQS-独占与共享_业精于勤荒于嬉 行成于思毁于随-CSDN博客_aqs 独占 共享 ReentrantLock调用过程 ReentrantLock把所有Lock接口的操作都委派到一个Sync类上，该类继承了AbstractQueuedSynchronizer： static abstract class Sync extends AbstractQueuedSynchronizer Sync又有两个子类： final static class NonfairSync extends Sync final static class FairSync extends Sync 显然是为了支持公平锁和非公平锁而定义，默认情况下为非公平锁。 先理一下Reentrant.lock()方法的调用过程（默认非公平锁）： .png) ReentrantLock就是使用AQS而实现的一把锁，它实现了可重入锁，公平锁和非公平锁。它有一个内部类用作同步器是Sync，Sync是继承了AQS的一个子类，并且公平锁和非公平锁是继承了Sync的两个子类。ReentrantLock的原理是：假设有一个线程A来尝试获取锁，它会先CAS修改state的值，从0修改到1，如果修改成功，那就说明获取锁成功，设置加锁线程为当前线程。如果此时又有一个线程B来尝试获取锁，那么它也会CAS修改state的值，从0修改到1，因为线程A已经修改了state的值，那么线程B就会修改失败，然后他会判断一下加锁线程是否为自己本身线程，如果是自己本身线程的话它就会将state的值直接加1，这是为了实现锁的可重入。如果加锁线程不是当前线程的话，那么就会将它生成一个Node节点，加入到等待队列的队尾，直到什么时候线程A释放了锁它会唤醒等待队列队头的线程。这里还要分为公平锁和非公平锁，默认为非公平锁，公平锁和非公平锁无非就差了一步。如果是公平锁，此时又有外来线程尝试获取锁，它会首先判断一下等待队列是否有第一个节点，如果有第一个节点，就说明等待队列不为空，有等待获取锁的线程，那么它就不会去同步队列中抢占cpu资源。如果是非公平锁的话，它就不会判断等待队列是否有第一个节点，它会直接前往同步对列中去抢占cpu资源。 以下是ReentrantLock的原理图解，简单明了： 深度解析：AQS原理_qq_37685457的博客-CSDN博客_aqs原理 扒一扒 ReentrantLock 以及 AQS 实现原理 大白话聊聊Java并发面试问题之谈谈你对AQS的理解？【石杉的架构笔记】 11 AQS · 深入浅出Java多线程 深入分析AQS实现原理 - 并发编程 - SegmentFault 思否 并发辅助类 CountDownLatch 一般用于某个线程 A 等待若干个其他线程执行完任务之后，它才执行。 CyclicBarrier 一般用于一组线程互相等待至某个状态，然后这一组线程再同时执行。 此外，CountDownLatch 是不能够重用的，而 CyclicBarrier 是可以重用的。 Semaphore：信号量 用来控制同时访问特定资源的线程数量，它通过协调各个线程以保证合理的使用公共资源，可以用做流量控制，譬如数据库连接场景控制等；Semaphore 的构造方法 Semaphore(int permits) 接收一个整型参数，表示可用的许可证数量，即最大并发数量，使用方法就是在线程里面首先调用 acquire 方法获取一个许可，使用完后接着调用 release 归还一个许可，还可以使用 tryAcquire 尝试获取许可，其还提供了一些状态数量获取方法，不再说明。 JDK并发包总结 - 大道方圆 - 博客园 Java并发编程的4个同步辅助类（CountDownLatch、CyclicBarrier、Semaphore、Phaser） Java多线程-ABC三个线程顺序输出的问题 - 会被淹死的鱼 - 博客园 Semaphore 允许多个线程同时访问某个资源。 执行 acquire 方法阻塞，直到有一个许可证可以获得然后拿走一个许可证；每个 release 方法增加一个许可证，这可能会释放一个阻塞的 acquire 方法。然而，其实并没有实际的许可证这个对象，Semaphore 只是维持了一个可获得许可证的数量。 Semaphore 经常用于限制获取某种资源的线程数量。 Semaphore与CountDownLatch一样，也是共享锁的一种实现。它默认构造AQS的state为permits。当执行任务的线程数量超出permits,那么多余的线程将会被放入阻塞队列Park,并自旋判断state是否大于0。只有当state大于0的时候，阻塞的线程才能继续执行,此时先前执行任务的线程继续执行release方法，release方法使得state的变量会加1，那么自旋的线程便会判断成功。 如此，每次只有最多不超过permits数量的线程能自旋成功，便限制了执行任务线程的数量。 CountDownLatch(倒计时器) 以 CountDownLatch 以例，任务分为 N 个子线程去执行，state 也初始化为 N（注意 N 要与线程个数一致）。这 N 个子线程是并行执行的，每个子线程执行完后 countDown()一次，state 会 CAS(Compare and Swap)减 1。等到所有子线程都执行完后(即 state=0)，会 unpark()主调用线程，然后主调用线程就会从 await()函数返回，继续后余动作。 CyclicBarrier(循环栅栏) CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。 CountDownLatch的实现是基于AQS的，而CycliBarrier是基于 ReentrantLock(ReentrantLock也属于AQS同步器)和 Condition 的. CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。CyclicBarrier 默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用await方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。 CyclicBarrier 和 CountDownLatch 的区别 CountDownLatch 是计数器，只能使用一次，而 CyclicBarrier 的计数器提供 reset 功能，可以多次使用。 对于 CountDownLatch 来说，重点是“一个线程（多个线程）等待”，而其他的 N 个线程在完成“某件事情”之后，可以终止，也可以等待。而对于 CyclicBarrier，重点是多个线程，在任意一个线程没有完成，所有的线程都必须等待。 ReentrantReadWriteLock 重入读写锁，它实现了ReadWriteLock接口，在这个类中维护了两个锁，一个是ReadLock，一个是WriteLock，他们都分别实现了Lock接口。读写锁是一种适合读多写少的场景下解决线程安全问题的工具，基本原则是：读和读不互斥、读和写互斥、写和写互斥。也就是说涉及到影响数据变化的操作都会存在互斥。 CopyOnWriteArrayList 在很多应用场景中，读操作可能会远远大于写操作。由于读操作根本不会修改原有的数据，因此对于每次读取都进行加锁其实是一种资源浪费。我们应该允许多个线程同时访问 List 的内部数据，毕竟读取操作是安全的。 这和我们之前在多线程章节讲过 ReentrantReadWriteLock 读写锁的思想非常类似，也就是读读共享、写写互斥、读写互斥、写读互斥。JDK 中提供了 CopyOnWriteArrayList 类比相比于在读写锁的思想又更进一步。为了将读取的性能发挥到极致，CopyOnWriteArrayList 读取是完全不用加锁的，并且更厉害的是：写入也不会阻塞读取操作。只有写入和写入之间需要进行同步等待。这样一来，读操作的性能就会大幅度提升。那它是怎么做的呢？ CopyOnWriteArrayList 类的所有可变操作（add，set 等等）都是通过创建底层数组的新副本来实现的。当 List 需要被修改的时候，我并不修改原有内容，而是对原有数据进行一次复制，将修改的内容写入副本。写完之后，再将修改完的副本替换原来的数据，这样就可以保证写操作不会影响读操作了。 所谓CopyOnWrite 也就是说：在计算机，如果你想要对一块内存进行修改时，我们不在原有内存块中进行写操作，而是将内存拷贝一份，在新的内存中进行写操作，写完之后呢，就将指向原来内存指针指向新的内存，原来的内存就可以被回收掉了。 Java锁有哪些种类 公平锁/非公平锁 可重入锁 独享锁/共享锁 互斥锁/读写锁 乐观锁/悲观锁 分段锁 偏向锁/轻量级锁/重量级锁 自旋锁 乐观锁 与 悲观锁 悲观锁 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 乐观锁常见的两种实现方式 乐观锁一般会使用版本号机制或CAS算法实现。 1. 版本号机制 2. CAS算法 即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 CAS操作底层是基于处理器的CMPXCHG指令实现的，如果是多处理器，为cmpxchg指令添加lock前缀。 深入浅出CAS - 占小狼 乐观锁的缺点 ABA 问题是乐观锁一个常见的问题 1 ABA 问题 如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 \"ABA\"问题。 JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的 compareAndSet 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2 循环时间长开销大 自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3 只能保证一个共享变量的原子操作 CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作。 CAS与synchronized的使用情景 简单的来说CAS适用于写比较少的情况下（多读场景，冲突一般较少），synchronized适用于写比较多的情况下（多写场景，冲突一般较多） 对于资源竞争较少（线程冲突较轻）的情况，使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗cpu资源；而CAS基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。 对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。 补充： Java并发编程这个领域中synchronized关键字一直都是元老级的角色，很久之前很多人都会称它为 “重量级锁” 。但是，在JavaSE 1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 偏向锁 和 轻量级锁 以及其它各种优化之后变得在某些情况下并不是那么重了。synchronized的底层实现主要依靠 Lock-Free 的队列，基本思路是 自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。 Fork/Join框架 工作窃取算法 工作窃取算法指的是在多线程执行不同任务队列的过程中，某个线程执行完自己队列的任务后从其他线程的任务队列里窃取任务来执行。 值得注意的是，当一个线程窃取另一个线程的时候，为了减少两个任务线程之间的竞争，我们通常使用双端队列来存储任务。被窃取的任务线程都从双端队列的头部拿任务执行，而窃取其他任务的线程从双端队列的尾部执行任务。 另外，当一个线程在窃取任务时要是没有其他可用的任务了，这个线程会进入阻塞状态以等待再次“工作”。 ForkJoinTask fork()方法:使用线程池中的空闲线程异步提交任务 join()方法：等待处理任务的线程处理完毕，获得返回值。 ForkJoinPool ForkJoinPool是用于执行ForkJoinTask任务的执行（线程）池。 ForkJoinPool管理着执行池中的线程和任务队列，此外，执行池是否还接受任务，显示线程的运行状态也是在这里处理。 WorkQueue 双端队列，ForkJoinTask存放在这里。 当工作线程在处理自己的工作队列时，会从队列首取任务来执行（FIFO）；如果是窃取其他队列的任务时，窃取的任务位于所属任务队列的队尾（LIFO）。 ForkJoinPool与传统线程池最显著的区别就是它维护了一个工作队列数组（volatile WorkQueue[] workQueues，ForkJoinPool中的每个工作线程都维护着一个工作队列）。 runState ForkJoinPool的运行状态。SHUTDOWN状态用负数表示，其他用2的幂次表示。 Java8 stream里的Fork/Join框架使用 Java8 stream里的并行流计算就是用的Fork/Join框架，在最终的并行执行evaluateParallel方法里创建了new ReduceTask<>(this, helper, spliterator).invoke().get();，而这里的ReduceTask最终继承的就是ForkJoinTask类。 它们的继承关系如下： ReduceTask -> AbstractTask -> CountedCompleter -> ForkJoinTask 这里的ReduceTask的invoke方法，其实是调用的ForkJoinTask的invoke方法，中间三层继承并没有覆盖这个方法的实现。 注意：在多核的情况下，使用Stream的并行计算确实比串行计算能带来很大效率上的提升，并且也能保证结果计算完全准确。 本文一直在强调的“多核”的情况。其实可以看到，我的本地电脑有8核，但并行计算耗时并不是单线程计算耗时除以8，因为线程的创建、销毁以及维护线程上下文的切换等等都有一定的开销。所以如果你的服务器并不是多核服务器，那也没必要用Stream的并行计算。因为在单核的情况下，往往Stream的串行计算比并行计算更快，因为它不需要线程切换的开销。 18 Fork/Join框架 · 深入浅出Java多线程 19 Java 8 Stream并行计算原理 · 深入浅出Java多线程 fail-fastfail-fast是如何抛出ConcurrentModificationException异常的，又是在什么情况下才会抛出? 我们知道，对于集合如list，map类，我们都可以通过迭代器来遍历，而Iterator其实只是一个接口，具体的实现还是要看具体的集合类中的内部类去实现Iterator并实现相关方法。 从源码知道，每次调用next()方法，在实际访问元素前，都会调用checkForComodification方法，该方法源码如下： final void checkForComodification() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); } 可以看出，该方法才是判断是否抛出ConcurrentModificationException异常的关键。在该段代码中，当modCount != expectedModCount 时，就会抛出该异常。但是在一开始的时候，expectedModCount初始值默认等于modCount，为什么会出现modCount != expectedModCount，很明显expectedModCount在整个迭代过程除了一开始赋予初始值modCount外，并没有再发生改变，所以可能发生改变的就只有modCount，在前面关于ArrayList扩容机制的分析中，可以知道在ArrayList进行add，remove，clear等涉及到修改集合中的元素个数的操作时，modCount就会发生改变(modCount ++),所以当另一个线程(并发修改)或者同一个线程遍历过程中，调用相关方法使集合的个数发生改变，就会使modCount发生变化，这样在checkForComodification方法中就会抛出ConcurrentModificationException异常。 类似的，hashMap中发生的原理也是一样的。 safe-fast 并发包下的容器都是“快速安全的” happens before 内存指令重排序 "},"docs/Guide/Java类加载.html":{"url":"docs/Guide/Java类加载.html","title":"Java类加载","keywords":"","body":"类加载机制 类加载的过程 类的个生命周期如下图： 为支持运行时绑定，解析过程在某些情况下可在初始化之后再开始，除解析过程外的其他加载过程必须按照如图顺序开始。 加载 通过全限定类名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个类的 java.lang.Class 对象，作为方法区这个类的各种数据的访问入口。 验证 验证是连接阶段的第一步，这一阶段的目的是为了确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 文件格式验证：如是否以魔数 0xCAFEBABE 开头、主、次版本号是否在当前虚拟机处理范围之内、常量合理性验证等。 此阶段保证输入的字节流能正确地解析并存储于方法区之内，格式上符合描述一个 Java类型信息的要求。 元数据验证：是否存在父类，父类的继承链是否正确，抽象类是否实现了其父类或接口之中要求实现的所有方法，字段、方法是否与父类产生矛盾等。 第二阶段，保证不存在不符合 Java 语言规范的元数据信息。 字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。例如保证跳转指令不会跳转到方法体以外的字节码指令上。 符号引用验证：在解析阶段中发生，保证可以将符号引用转化为直接引用。 可以考虑使用 -Xverify:none 参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。 准备 为类变量分配内存并设置类变量初始值，这些变量所使用的内存都将在方法区中进行分配。 这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在 Java 堆中。 这里所设置的初始值\"通常情况\"下是数据类型默认的零值（如0、0L、null、false等），比如我们定义了public static int value=111 ，那么 value 变量在准备阶段的初始值就是 0 而不是111（初始化阶段才会赋值）。特殊情况：比如给 value 变量加上了 fianl 关键字public static final int value=111 ，那么准备阶段 value 的值就被赋值为 111。 解析 虚拟机将常量池内的符号引用替换为直接引用的过程。 解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符 7 类符号引用进行。 初始化 到初始化阶段，才真正开始执行类中定义的 Java 程序代码，此阶段是执行 () 方法的过程。 () 方法是由编译器按语句在源文件中出现的顺序，依次自动收集类中的所有类变量的赋值动作和静态代码块中的语句合并产生的。（不包括构造器中的语句。构造器是初始化对象的，类加载完成后，创建对象时候将调用的 () 方法来初始化对象） 静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块可以赋值，但是不能访问，如下程序： Copypublic class Test { static { // 给变量赋值可以正常编译通过 i = 0; // 这句编译器会提示\"非法向前引用\" System.out.println(i); } static int i = 1; } () 不需要显式调用父类（接口除外，接口不需要调用父接口的初始化方法，只有使用到父接口中的静态变量时才需要调用）的初始化方法 ()，虚拟机会保证在子类的 () 方法执行之前，父类的 () 方法已经执行完毕，也就意味着父类中定义的静态语句块要优先于子类的变量赋值操作。 () 方法对于类或接口来说并不是必需的，如果一个类中没有静态语句块，也没有对变量的赋值操作，那么编译器可以不为这个类生成 () 方法。 虚拟机会保证一个类的 () 方法在多线程环境中被正确地加锁、同步，如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的 () 方法，其他线程都需要阻塞等待，直到活动线程执行 () 方法完毕。 类加载的时机 对于初始化阶段，虚拟机规范规定了有且只有 5 种情况必须立即对类进行“初始化”（而加载、验证、准备自然需要在此之前开始）： 遇到new、getstatic 和 putstatic 或 invokestatic 这4条字节码指令时，如果类没有进行过初始化，则需要先触发其初始化。对应场景是：使用 new 实例化对象、读取或设置一个类的静态字段（被 final 修饰、已在编译期把结果放入常量池的静态字段除外）、以及调用一个类的静态方法。 对类进行反射调用的时候，如果类没有进行过初始化，则需要先触发其初始化。 当初始化类的父类还没有进行过初始化，则需要先触发其父类的初始化。（而一个接口在初始化时，并不要求其父接口全部都完成了初始化） 虚拟机启动时，用户需要指定一个要执行的主类（包含 main() 方法的那个类）， 虚拟机会先初始化这个主类。 当使用 JDK 1.7 的动态语言支持时，如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果 REF_getStatic、REF_putStatic、REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化。 第5种情况，我暂时看不懂。 以上这 5 种场景中的行为称为对一个类进行主动引用。除此之外，所有引用类的方式都不会触发初始化，称为被动引用，例如： 通过子类引用父类的静态字段，不会导致子类初始化。 通过数组定义来引用类，不会触发此类的初始化。MyClass[] cs = new MyClass[10]; 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。 类加载器 把实现类加载阶段中的“通过一个类的全限定名来获取描述此类的二进制字节流”这个动作的代码模块称为“类加载器”。 将 class 文件二进制数据放入方法区内，然后在堆内（heap）创建一个 java.lang.Class 对象，Class 对象封装了类在方法区内的数据结构，并且向开发者提供了访问方法区内的数据结构的接口。 目前类加载器却在类层次划分、OSGi、热部署、代码加密等领域非常重要，我们运行任何一个 Java 程序都会涉及到类加载器。 类的唯一性和类加载器 对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立其在Java虚拟机中的唯一性。 即使两个类来源于同一个 Class 文件，被同一个虚拟机加载，只要加载它们的类加载器不同，那这两个类也不相等。 这里所指的“相等”，包括代表类的 Class 对象的 equals() 方法、 isAssignableFrom() 方法、isInstance() 方法的返回结果，也包括使用 instanceof 关键字做对象所属关系判定等情况。 类加载器 JVM 中内置了三个重要的 ClassLoader，除了 BootstrapClassLoader 其他类加载器均由 Java 实现且全部继承自java.lang.ClassLoader： BootstrapClassLoader(启动类加载器) ：最顶层的加载类，由C++实现，负责加载 %JAVA_HOME%/lib目录下的jar包和类或者或被 -Xbootclasspath参数指定的路径中的所有类。 ExtensionClassLoader(扩展类加载器) ：主要负责加载目录 %JRE_HOME%/lib/ext 目录下的jar包和类，或被 java.ext.dirs 系统变量所指定的路径下的jar包。 AppClassLoader(应用程序类加载器) :面向我们用户的加载器，负责加载当前应用classpath下的所有jar包和类。 自定义类加载器 除了 BootstrapClassLoader 其他类加载器均由 Java 实现且全部继承自java.lang.ClassLoader。如果我们要自定义自己的类加载器，很明显需要继承 ClassLoader。 双亲委派模型介绍 每一个类都有一个对应它的类加载器。系统中的 ClassLoder 在协同工作的时候会默认使用 双亲委派模型 。即在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。加载的时候，首先会把该请求委派该父类加载器的 loadClass() 处理，因此所有的请求最终都应该传送到顶层的启动类加载器 BootstrapClassLoader 中。当父类加载器无法处理时，才由自己来处理。当父类加载器为null时，会使用启动类加载器 BootstrapClassLoader 作为父类加载器。 每个类加载都有一个父类加载器，我们通过下面的程序来验证。 public class ClassLoaderDemo { public static void main(String[] args) { System.out.println(\"ClassLodarDemo's ClassLoader is \" + ClassLoaderDemo.class.getClassLoader()); System.out.println(\"The Parent of ClassLodarDemo's ClassLoader is \" + ClassLoaderDemo.class.getClassLoader().getParent()); System.out.println(\"The GrandParent of ClassLodarDemo's ClassLoader is \" + ClassLoaderDemo.class.getClassLoader().getParent().getParent()); } } Output ClassLodarDemo's ClassLoader is sun.misc.Launcher$AppClassLoader@18b4aac2 The Parent of ClassLodarDemo's ClassLoader is sun.misc.Launcher$ExtClassLoader@1b6d3586 The GrandParent of ClassLodarDemo's ClassLoader is null AppClassLoader的父类加载器为ExtClassLoader ExtClassLoader的父类加载器为null，null并不代表ExtClassLoader没有父类加载器，而是 BootstrapClassLoader 。 其实这个双亲翻译的容易让别人误解，我们一般理解的双亲都是父母，这里的双亲更多地表达的是“父母这一辈”的人而已，并不是说真的有一个 Mother ClassLoader 和一个 Father ClassLoader 。另外，类加载器之间的“父子”关系也不是通过继承来体现的，是由“优先级”来决定。官方API文档对这部分的描述如下: The Java platform uses a delegation model for loading classes. The basic idea is that every class loader has a \"parent\" class loader. When loading a class, a class loader first \"delegates\" the search for the class to its parent class loader before attempting to find the class itself. 双亲委派模型实现源码分析 双亲委派模型的实现代码非常简单，逻辑非常清晰，都集中在 java.lang.ClassLoader 的 loadClass() 中，相关代码如下所示。 private final ClassLoader parent; protected Class loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // 首先，检查请求的类是否已经被加载过 Class c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) {//父加载器不为空，调用父加载器loadClass()方法处理 c = parent.loadClass(name, false); } else {//父加载器为空，使用启动类加载器 BootstrapClassLoader 加载 c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { //抛出异常说明父类加载器无法完成加载请求 } if (c == null) { long t1 = System.nanoTime(); //自己尝试加载 c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; } } 双亲委派模型的好处 双亲委派模型保证了Java程序的稳定运行，可以避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类），也保证了 Java 的核心 API 不被篡改。如果没有使用双亲委派模型，而是每个类加载器加载自己的话就会出现一些问题，比如我们编写一个称为 java.lang.Object 类的话，那么程序运行的时候，系统就会出现多个不同的 Object 类。 new一个对象过程中发生了什么？ 确认类元信息是否存在。当 JVM 接收到 new 指令时，首先在 metaspace 内检查需要创建的类元信息是否存在。 若不存在，那么在双亲委派模式下，使用当前类加载器以 ClassLoader + 包名＋类名为 Key 进行查找对应的 class 文件。 如果没有找到文件，则抛出 ClassNotFoundException 异常 ， 如果找到，则进行类加载（加载 - 验证 - 准备 - 解析 - 初始化），并生成对应的 Class 类对象。 分配对象内存。 首先计算对象占用空间大小，如果实例成员变量是引用变量，仅分配引用变量空间即可，即 4 个字节大小，接着在堆中划分—块内存给新对象。 在分配内存空间时，需要进行同步操作，比如采用 CAS (Compare And Swap) 失败重试、 区域加锁等方式保证分配操作的原子性。 设定默认值。 成员变量值都需要设定为默认值， 即各种不同形式的零值。 设置对象头。设置新对象的哈希码、 GC 信息、锁信息、对象所属的类元信息等。这个过程的具体设置方式取决于 JVM 实现。 执行 init 方法。 初始化成员变量，执行实例化代码块，调用类的构造方法，并把堆内对象的首地址赋值给引用变量。 深入理解Java类加载 - czwbig - 博客园 类的热加载 有两种用的比较多的方法，1.是自定义类加载器；2.JDK动态代理； 自定义类加载器 创建一个自定义的 ClassLoader 对象，加载类的步骤不遵守双亲委派模型，而是直接加载。 使用刚刚创建的类加载器加载指定的类。 得到刚刚的Class 对象，使用反射创建对象，并调用对象的 operation 方法。 为什么间隔20秒呢？因为我们要在启动之后，修改类，并重新编译。因此需要20秒时间。 JDK动态代理（代表：阿里的Arthas的jad/mc/redefine一条龙） 实现原理是： 1.绑定pid获得虚拟机对象，然后通过虚拟机加载代理jar包，这样就调用到agentmain，获取得到Instrumentation 2.基于Instrumentation接口可以实现JDK的代理机制，从而实现对类进行动态重新定义。 注意：com.sun.tools.attach.VirtualMachine的jar包是 jdk下lib中的tools.jar，所以项目中要引用到这个jar包，而且因为涉及到底层虚拟机，windows和linux机器这个jar不同 因此，整个流程就是： 1.项目中引用 jdk/lib/tools.jar，否则无法使用VirtualMachine类 2.项目中引用 javaagent.jar ，它提供了agentmain接口 3.代码实现动态增加JDK代理 Arthas 热更新功能看起来很神奇，实际上离不开 JDK 一些 API，分别为 instrument API 与 attach API。 Arthas的jad & mc & redefine一条龙 jad 反编译代码 首先运行 jad 命令反编译 class 文件获取源代码,运行命令如下：。 jad --source-only com.andyxh.HelloService > /tmp/HelloService.java 修改反编译之后的代码 拿到源代码之后，使用 VIM 等文本编辑工具编辑源代码，加入需要改动的逻辑。 查找 ClassLoader 然后使用 sc 命令查找加载修改类的 ClassLoader，运行命令如下: $ sc -d com.andyxh.HelloService | grep classLoaderHash classLoaderHash 4f8e5cde ​ 这里运行之后将会得到 ClassLoader 哈希值。 mc 内存编译源代码 使用 mc 命令编译上一步修改保存的源代码，生成最终 class 文件。 $ mc -c 4f8e5cde /tmp/HelloService.java -d /tmp Memory compiler output: /tmp/com/andyxh/HelloService.class Affect(row-cnt:1) cost in 463 ms. redefine 热更新代码 运行 redefine 命令： $ redefine /tmp/com/andyxh/HelloService.class redefine success, size: 1 Arthas里 jad &mc &redefine 一条龙来线上热更新代码，非常强大，但也很危险，需要做好权限管理。 比如，线上应用启动帐号是 admin，当用户可以切换到admin，那么 用户可以修改，获取到应用的任意内存值（不管是否java应用） 用户可以attach jvm attach jvm之后，利用jvm本身的api可以redefine class 所以： 应用的安全主要靠用户权限本身的管理 Arthas主要是让jvm redefine更容易了。用户也可以利用其它工具达到同样的效果 使用热更新功能有一些条件限制，我们只能用它来修改方法内部的一些业务代码，如果我们出现了以下任意一种情况，那么热更新就会执行失败： 增加类属性（类字段）； 增加或删除方法； 替换正在运行的方法。 参考 探秘 Java 热部署 - 简书 Java之——类热加载_Java_冰河的专栏-CSDN博客 JAVA代码热部署，在线不停服动态更新_Java_坦GA的博客-CSDN博客 手把手教你实现热更新功能，带你了解 Arthas 热更新背后的原理 - 程序通事 "},"docs/Guide/JVM.html":{"url":"docs/Guide/JVM.html","title":"JVM","keywords":"","body":"好文推荐： 看完这篇垃圾回收，和面试官扯皮没问题了 看过无数Java GC文章，这5个问题你也未必知道！ JMM（JVM内存模型 Java Memory Model） JMM描述了Java多线程对共享变量的访问规则，以及在JVM中将变量存储到内存和从内存中读取变量这样的底层细节。 java内存模型如上图所示，每个线程都有自己独立的工作内存，当线程要访问内存中的变量时，会先将内存中的变量值复制到自己的工作内存，然后再访问；当线程要改变内存中的变量值时，也是先改变自己工作内存中副本的变量值，然后再刷新到内存中。当线程一改变了某个变量的值，而线程二想要访问该值时，可能会存在以下情况，即线程一的改变还没刷到内存，或者线程二里面缓存了老值，没有去内存中拿最新的值，这时就相当于线程一的改变对线程二不可见了。 Java通过以下几种方式来保证变量在线程之间的可见性： synchronized 当线程调用synchronized修饰的方法（代码块）时，会清空自己工作内存中所有的共享变量，并从内存中重新读取，这样就保证了可以看到别的线程做的改动；当退出synchronzied函数时，会将工作内存中所有更新过的共享变量的值回写到内存中，保证其他线程可以读到该线程更改的值。 volatile volatile变量也能保证可见性，每次对volatile的读操作都会导致工作内存中的变量被内存中的最新值覆盖，对volatile的写操作，也会马上更新到主内存中去，这样就保证了每个线程对变量的改变对其他线程都是可见的。 synchronized 和volatile的不同： 既然synchronized可以实现可见性了为啥还引入volatile呢？两者虽然都能实现可见性，还是有不同之处的，synchronized需要对程序加锁，比较耗费资源；synchronized修饰的代码块，在一个线程退出去之前，其他线程是不能访问的，这样就提供了对某地代码的原子性操作。volatile则比较轻便，不需要加锁，但是不能保证操作的原子性，像i++，这种操作，它是无法保证结果正确的。 final final修饰的变量不会在构造函数返回前被访问到，这样就可以保证final变量的不可变性。因为如果因为指令重排序，其他线程在final对象还没初始化完之前拿到了该变量的引用，有可能读到一个初始化之前的值，然后后面再读又读到一个初始化之后的值，造成的现象就是final修饰的变量值可变了。 cocurrent包 java concurrent jar包提供了大量同步代码块的工具，方便我们正确的同步各个线程的执行。 double-checked locking的问题 什么是Java内存模型 - 简书 全面理解Java内存模型(JMM)及volatile关键字_Java_zejian的博客-CSDN博客 JVM 内存区域 Java 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。JDK. 1.8 和之前的版本略有不同，下面会介绍到。 JDK 1.8 之前： JDK 1.8 ： 线程私有的： 程序计数器 虚拟机栈 本地方法栈 线程共享的： 堆 方法区 直接内存 (非运行时数据区的一部分) 程序计数器 程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。 另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 从上面的介绍中我们知道程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 虚拟机栈 与程序计数器一样，Java 虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型，每次方法调用的数据都是通过栈传递的。 Java 内存可以粗糙的区分为堆内存（Heap）和栈内存 (Stack)，其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 （实际上，Java 虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。） 局部变量表主要存放了编译器可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。 Java 虚拟机栈会出现两种错误：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError： 若 Java 虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 错误。 OutOfMemoryError： 若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出 OutOfMemoryError 错误。 扩展：那么方法/函数如何调用？ Java 栈可用类比数据结构中栈，Java 栈中保存的主要内容是栈帧，每一次函数调用都会有一个对应的栈帧被压入 Java 栈，每一个函数调用结束后，都会有一个栈帧被弹出。 Java 方法有两种返回方式： return 语句。 抛出异常。 不管哪种返回方式都会导致栈帧被弹出。 本地方法栈 和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种错误。 堆 Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 在 JDK 7 版本及JDK 7 版本之前，堆内存被通常被分为下面三部分： 新生代内存(Young Generation) 老生代(Old Generation) 永生代(Permanent Generation) JDK 8 版本之后方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 上图所示的 Eden 区、两个 Survivor 区都属于新生代（为了区分，这两个 Survivor 区域按照顺序被命名为 from 和 to），中间一层属于老年代。 大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区->Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 修正（issue552）：“Hotspot遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了survivor区的一半时，取这个年龄和MaxTenuringThreshold中更小的一个值，作为新的晋升年龄阈值”。 动态年龄计算的代码如下 uint ageTable::compute_tenuring_threshold(size_t survivor_capacity) { //survivor_capacity是survivor空间的大小 size_t desired_survivor_size = (size_t)((((double) survivor_capacity)*TargetSurvivorRatio)/100); size_t total = 0; uint age = 1; while (age desired_survivor_size) break; age++; } uint result = age 堆这里最容易出现的就是 OutOfMemoryError 错误，并且出现这种错误之后的表现形式还会有几种，比如： OutOfMemoryError: GC Overhead Limit Exceeded ： 当JVM花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误。 java.lang.OutOfMemoryError: Java heap space :假如在创建新的对象时, 堆内存中的空间不足以存放新创建的对象, 就会引发java.lang.OutOfMemoryError: Java heap space 错误。(和本机物理内存无关，和你配置的对内存大小有关！) ...... 方法区 方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 方法区也被称为永久代。很多人都会分不清方法区和永久代的关系，为此我也查阅了文献。 方法区和永久代的关系 《Java 虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。那么，在不同的 JVM 上方法区的实现肯定是不同的了。 方法区和永久代的关系很像 Java 中接口和类的关系，类实现了接口，而永久代就是 HotSpot 虚拟机对虚拟机规范中方法区的一种实现方式。 也就是说，永久代是 HotSpot 的概念，方法区是 Java 虚拟机规范中的定义，是一种规范，而永久代是一种实现，一个是标准一个是实现，其他的虚拟机实现并没有永久代这一说法。 常用参数 JDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小 -XX:PermSize=N //方法区 (永久代) 初始大小 -XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。 JDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 下面是一些常用参数： -XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小） -XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小 与永久代很大的不同就是，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。 为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢? 整个永久代有一个 JVM 本身设置固定大小上限，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小。 当你元空间溢出时会得到如下错误： java.lang.OutOfMemoryError: MetaSpace 你可以使用 -XX：MaxMetaspaceSize 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。 元空间里面存放的是类的元数据，这样加载多少类的元数据就不由 MaxPermSize 控制了, 而由系统的实际可用空间来控制，这样能加载的类就更多了。 在 JDK8，合并 HotSpot 和 JRockit 的代码时, JRockit 从来没有一个叫永久代的东西, 合并之后就没有必要额外的设置这么一个永久代的地方了。 运行时常量池 运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用） 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 错误。 JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 ——图片来源：https://blog.csdn.net/wangbiao007/article/details/78545189 直接内存 直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 错误出现。 JDK1.4 中新加入的 NIO(New Input/Output) 类，引入了一种基于通道（Channel） 与缓存区（Buffer） 的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据。 本机直接内存的分配不会受到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。 3.1 对象的创建 下图便是 Java 对象的创建过程，我建议最好是能默写出来，并且要掌握每一步在做什么。 Step1:类加载检查 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 Step2:分配内存 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 内存分配的两种方式：（补充内容，需要掌握） 选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是\"标记-清除\"，还是\"标记-整理\"（也称作\"标记-压缩\"），值得注意的是，复制算法内存也是规整的 内存分配并发问题（补充内容，需要掌握） 在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全： CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配 Step3:初始化零值 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 Step4:设置对象头 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 Step5:执行 init 方法 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 3.2 对象的内存布局 在 Hotspot 虚拟机中，对象在内存中的布局可以分为 3 块区域：对象头、实例数据和对齐填充。 Hotspot 虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的自身运行时数据（哈希码、GC 分代年龄、锁状态标志等等），另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是那个类的实例。 实例数据部分是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。 对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。 因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 3.3 对象的访问定位 建立对象就是为了使用对象，我们的 Java 程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有①使用句柄和②直接指针两种： 句柄： 如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息； 直接指针： 如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象的地址。 这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。 JVM 垃圾回收 常见面试题： 何判断对象是否死亡（两种方法）。 简单的介绍一下强引用、软引用、弱引用、虚引用（虚引用与软引用和弱引用的区别、使用软引用能带来的好处）。 如何判断一个常量是废弃常量 如何判断一个类是无用的类 垃圾收集有哪些算法，各自的特点？ HotSpot 为什么要分为新生代和老年代？ 常见的垃圾回收器有哪些？ 介绍一下 CMS,G1 收集器。 Minor Gc 和 Full GC 有什么不同呢？ JVM内存分配 堆空间的基本结构： 上图所示的 eden 区、s0(\"From\") 区、s1(\"To\") 区都属于新生代，tentired 区属于老年代。大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s1(\"To\")，并且对象的年龄还会加 1(Eden 区->Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。经过这次GC后，Eden区和\"From\"区已经被清空。这个时候，\"From\"和\"To\"会交换他们的角色，也就是新的\"To\"就是上次GC前的“From”，新的\"From\"就是上次GC前的\"To\"。不管怎样，都会保证名为To的Survivor区域是空的。Minor GC会一直重复这样的过程，直到“To”区被填满，\"To\"区被填满之后，会将所有对象移动到老年代中。 1.1 对象优先在 eden 区分配 目前主流的垃圾收集器都会采用分代回收算法，因此需要将堆内存分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 大多数情况下，对象在新生代中 eden 区分配。当 eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.下面我们来进行实际测试以下。 在测试之前我们先来看看 Minor GC 和 Full GC 有什么不同呢？ 新生代 GC（Minor GC）:指发生新生代的的垃圾收集动作，Minor GC 非常频繁，回收速度一般也比较快。 老年代 GC（Major GC/Full GC）:指发生在老年代的 GC，出现了 Major GC 经常会伴随至少一次的 Minor GC（并非绝对），Major GC 的速度一般会比 Minor GC 的慢 10 倍以上。 1.2 大对象直接进入老年代 大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。 为什么要这样呢？ 为了避免为大对象分配内存时由于分配担保机制带来的复制而降低效率。 1.3 长期存活的对象将进入老年代 既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。 如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为 1.对象在 Survivor 中每熬过一次 MinorGC,年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 1.4 动态对象年龄判定 为了更好的适应不同程序的内存情况，虚拟机不是永远要求对象年龄必须达到了某个值才能进入老年代，如果 Survivor 空间中相同年龄所有对象大小的总和大于 Survivor 空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无需达到要求的年龄。 2 对象已经死亡？ 堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断那些对象已经死亡（即不能再被任何途径使用的对象）。 2.1 引用计数法 给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加 1；当引用失效，计数器就减 1；任何时候计数器为 0 的对象就是不可能再被使用的。 这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。 2.2 可达性分析算法 这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。 哪些对象可以当GC Roots呢 虚拟机栈中引用的对象 本地方法栈中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 2.3 再谈引用 1．强引用（StrongReference） 以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。 2．软引用（SoftReference） 如果一个对象只具有软引用，那就类似于可有可无的生活用品。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。 软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA 虚拟机就会把这个软引用加入到与之关联的引用队列中。 3．弱引用（WeakReference） 如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。 4．虚引用（PhantomReference） \"虚引用\"顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。 虚引用主要用来跟踪对象被垃圾回收的活动。 虚引用与软引用和弱引用的一个区别在于： 虚引用必须和引用队列（ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 特别注意，在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为软引用可以加速 JVM 对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生。 2.5 如何判断一个常量是废弃常量 运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？ 假如在常量池中存在字符串 \"abc\"，如果当前没有任何 String 对象引用该字符串常量的话，就说明常量 \"abc\" 就是废弃常量，如果这时发生内存回收的话而且有必要的话，\"abc\" 就会被系统清理出常量池。 注意：我们在 可能是把 Java 内存区域讲的最清楚的一篇文章 也讲了 JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 2.6 如何判断一个类是无用的类 方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？ 判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面 3 个条件才能算是 “无用的类” ： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述 3 个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。 3 垃圾收集算法 3.1 标记-清除算法 该算法分为“标记”和“清除”阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题： 效率问题 空间问题（标记清除后会产生大量不连续的碎片） 3.2 复制算法 为了解决效率问题，“复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 3.3 标记-整理算法 根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 3.4 分代收集算法 当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将 java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 比如在新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 因为新生代中大多数对象的生命周期都很短，因此Minor GC(采用复制算法)非常频繁，虽然它会触发stop-the-world，但是回收速度也比较快。 复制算法与标记-整理算法都有标记的过程，标记的过程都是采用可达性分析算法，所以跳过这个步骤接着分析。复制算法接下来是把标记存活的对象复制到另一块内存区域中，标记-整理算法是将标记的对象整理到一端，这个时候需要注意，并没有进行内存空间清理，针对于新生代需要清理的对象数量十分巨大，所以在将存活的对象插入到待清理对象之前，需要大量移动操作，时间复杂度很高；反观复制算法，不需要移动待回收对象的操作，直接将存活对象复制到另一块空闲内存区域中，大大减小了时间复杂度，所以分析到这里新生代不使用“标记-整理算法”的原因就显而易见了！ 延伸面试问题： HotSpot 为什么要分为新生代和老年代？ 根据上面的对分代收集算法的介绍回答。 4 垃圾收集器 图中有 7 种不同的 垃圾回收器，它们分别用于不同分代的垃圾回收。 新生代回收器：Serial、ParNew、Parallel Scavenge 老年代回收器：Serial Old、Parallel Old、CMS 整堆回收器：G1 两个 垃圾回收器 之间有连线表示它们可以 搭配使用，可选的搭配方案如下： 新生代 老年代 Serial Serial Old Serial CMS ParNew Serial Old ParNew CMS Parallel Scavenge Serial Old Parallel Scavenge Parallel Old G1 G1 作者：零壹技术栈链接：https://juejin.im/post/5b651200f265da0fa00a38d7来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 虽然我们对各个收集器进行比较，但并非要挑选出一个最好的收集器。因为直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的 HotSpot 虚拟机就不会实现那么多不同的垃圾收集器了。 4.1 Serial 收集器（-XX:+UseSerialGC） Serial（串行）收集器收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ \"Stop The World\" ），直到它收集结束。 新生代采用复制算法，老年代采用标记-整理算法。 虚拟机的设计者们当然知道 Stop The World 带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。 但是 Serial 收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial 收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial 收集器对于运行在 Client 模式下的虚拟机来说是个不错的选择。 4.2 ParNew 收集器（-XX:+UseParNewGC） ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。 多线程垃圾回收器（吞吐量优先） 新生代采用复制算法，老年代采用标记-整理算法。 它是许多运行在 Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 并行和并发概念补充： 并行（Parallel） ：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个 CPU 上。 4.3 Parallel Scavenge 收集器 （-XX:+UseParallelGC） Parallel Scavenge 收集器也是使用复制算法的多线程收集器，它看上去几乎和ParNew都一样。 那么它有什么特别之处呢？ -XX:+UseParallelGC # 使用 Parallel 收集器 + 老年代串行 -XX:+UseParallelOldGC # 使用 Parallel 收集器 + 老年代并行 Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。 Parallel Scavenge 收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，手工优化存在困难的话可以选择把内存管理优化交给虚拟机去完成也是一个不错的选择。 新生代采用复制算法，老年代采用标记-整理算法。 4.4.Serial Old 收集器（-XX:+UseSerialGC） Serial 收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在 JDK1.5 以及以前的版本中与 Parallel Scavenge 收集器搭配使用，另一种用途是作为 CMS 收集器的后备方案。 4.5 Parallel Old 收集器（-XX:+UseParallelOldGC） Parallel Scavenge 收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及 CPU 资源的场合，都可以优先考虑 Parallel Scavenge 收集器和 Parallel Old 收集器。 4.6 CMS 收集器（-XX:+UseConcMarkSweepGC） CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。 停顿时间优先 CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS 收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤： 初始标记： 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快 ； 并发标记： 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短 并发清除： 开启用户线程，同时 GC 线程开始对为标记的区域做清扫。 从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面三个明显的缺点： 对 CPU 资源敏感； CMS 回收器过分依赖于 多线程环境，默认情况下，开启的 线程数 为（CPU 的数量 + 3）/ 4，当 CPU 数量少于 4 个时，CMS 对 用户查询 的影响将会很大，因为他们要分出一半的运算能力去 执行回收器线程； 无法处理浮动垃圾； 由于 CMS 回收器 清除已标记的垃圾 （处于最后一个阶段）时，用户线程 还在运行，因此会有新的垃圾产生。但是这部分垃圾 未被标记，在下一次 GC 才能清除，因此被成为 浮动垃圾。 由于 内存回收 和 用户线程 是同时进行的，内存在被 回收 的同时，也在被 分配。当 老生代 中的内存使用超过一定的比例时，系统将会进行 垃圾回收；当 剩余内存 不能满足程序运行要求时，系统将会出现 Concurrent Mode Failure，临时采用 Serial Old 算法进行 清除，此时的 性能 将会降低。 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。 CMS 回收器采用的 标记清除算法，本身存在垃圾收集结束后残余 大量空间碎片 的缺点。CMS 配合适当的 内存整理策略，在一定程度上可以解决这个问题。 4.7 G1 收集器（-XX:+UseG1GC） G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征. 垃圾区域Region优先 G1 是 JDK 1.7 中正式投入使用的用于取代 CMS 的 压缩回收器。它虽然没有在物理上隔断 新生代 与 老生代，但是仍然属于 分代垃圾回收器。G1 仍然会区分 年轻代 与 老年代，年轻代依然分有 Eden 区与 Survivor 区。 G1 首先将 堆 分为 大小相等 的 Region，避免 全区域 的垃圾回收。然后追踪每个 Region 垃圾 堆积的价值大小，在后台维护一个 优先列表，根据允许的回收时间优先回收价值最大的 Region。同时 G1采用 Remembered Set 来存放 Region 之间的 对象引用 ，其他回收器中的 新生代 与 老年代 之间的对象引用，从而避免 全堆扫描。G1 的分区示例如下图所示： 这种使用 Region 划分 内存空间 以及有 优先级 的区域回收方式，保证 G1 回收器在有限的时间内可以获得尽可能 高的回收效率。 被视为 JDK1.7 中 HotSpot 虚拟机的一个重要进化特征。它具备一下特点： 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 空间整合：与 CMS 的“标记--清理”算法不同，G1 从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。 G1 收集器的运作大致分为以下几个步骤： 初始标记 并发标记 最终标记 筛选回收 G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来)。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 GF 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。 最大的区别是出现了Region区块概念，可对回收价值和成本进行排序回收，根据GC期望时间回收，还出现了member set概念，将回收对象放入其中，避免全堆扫描。 G1内存的分配 TLAB（TLAB占用年轻代内存）. 默认使用TLAB加速内存分配,之前文章已经讲过，不赘述。 Eden.如果TLAB不够用，则在Eden中分配内存生成对象。 Humongous.如果对象需要的内存超过一个region的50%以上，会忽略前两个步骤直接在老年代的humongous中分配（连续的Region）。 何时使用G1（-XX:+UseG1GC） 大内存中为了达到低gc延迟. 比如:heap size >=6G,gc pause FullGC时间太长，或者太频繁。 jvm G1 深度分析_shine的博客-CSDN博客 4.8 ZGC Java 11版本包含一个全新的垃圾收集器ZGC，它由Oracle开发，承诺在数TB的堆上具有非常低的暂停时间。随着服务器拥有数百GB到数TB的RAM变得越来越普及，Java有效使用大容量堆内存的能力变得越来越重要。ZGC是一个令人兴奋的新垃圾收集器，旨在为大堆提供非常低的暂停时间。它通过使用有色指针和负载屏障来实现这一点，这些热点是Hotspot新增的GC技术，并开辟了一些其他有趣的未来可能性。它将在Java 11中作为实验性提供，但您现在可以使用Early Access 构建进行试用。那么为什么需要新的GC呢？G1是在2006年推出的，而ZGC的设计针对未来多TB容量大容量普遍存在的可能而设计的，能够有很低的暂停时间（ 详细原理见：https://www.opsian.com/blog/javas-new-zgc-is-very-exciting/ 参考： 深入理解JVM(3)——7种垃圾收集器 - 王泽远的博客 | Crow's Blog JVM系列(六) - JVM垃圾回收器 - 掘金 Java虚拟机垃圾收集器 | hua的博客 YGC和FGC发生时间 YGC的触发时机： edn空间不足 FGC的触发时机： old空间不足 perm空间不足 调用方法System.gc() dump live的内存信息时(jmap –dump:live) 堆中分配很大的对象 所谓大对象，是指需要大量连续内存空间的java对象，例如很长的数组，此种对象会直接进入老年代，而老年代虽然有很大的剩余空间，但是无法找到足够大的连续空间来分配给当前对象，此种情况就会触发JVM进行Full GC。 为了解决这个问题，CMS垃圾收集器提供了一个可配置的参数，即-XX:+UseCMSCompactAtFullCollection开关参数，用于在“享受”完Full GC服务之后额外免费赠送一个碎片整理的过程，内存整理的过程无法并发的，空间碎片问题没有了，但提顿时间不得不变长了，JVM设计者们还提供了另外一个参数 -XX:CMSFullGCsBeforeCompaction,这个参数用于设置在执行多少次不压缩的Full GC后,跟着来一次带压缩的。 ygc时的悲观策略 最复杂的是所谓的悲观策略，它触发的机制是在首先会计算之前晋升的平均大小，也就是从新生代，通过ygc变成新生代的平均大小，然后如果旧生代剩余的空间小于晋升大小，那么就会触发一次FullGC。sdk考虑的策略是， 从平均和长远的情况来看，下次晋升空间不够的可能性非常大， 与其等到那时候在fullGC 不如悲观的认为下次肯定会触发FullGC， 直接先执行一次FullGC。而且从实际使用过程中来看， 也达到了比较稳定的效果。 JVM中什么情况会进入到老年代 大对象: 所谓的大对象是指需要大量连续内存空间的java对象,最典型的大对象就是那种很长的字符串以及数组,大对象对虚拟机的内存分配就是坏消息,尤其是一些朝生夕灭的短命大对象,写程序时应避免。 有一个JVM参数，就是 -XX:PretenureSizeThreshold“,可以把他的值设置为字节数，比如“1048576”，就是1M 如果你创建一个大于这个大小的对象，比如一个超大的数组，或者是别的啥东西，此时就直接把这个大对象放在老年代中，压根不会经过新生代，这样可以避免新生代出现那种大对象，然后在2个Survivor区域里回来复制多次之后才能进入老年代。 长期存活的对象: 虚拟机给每个对象定义了一个对象年龄(Age)计数器,如果对象在Eden出生并经过第一次Minor GC后仍然存活,并且能被Survivor容纳的话,将被移动到Survivor空间中,并且对象年龄设为1,。对象在Survivor区中每熬过一次Minor GC,年龄就增加1,当他的年龄增加到一定程度(默认是15岁), 就将会被晋升到老年代中。对象晋升到老年代的年龄阈值,可以通过参数-XX:MaxTenuringThreshold设置。 动态对象年龄判定: 为了能更好地适应不同程度的内存状况,虚拟机并不是永远地要求对象的年龄必须达到了MaxTenuringThreshold才能晋升到老年代,如果在Survivor空间中相同年龄的所有对象大小的总和大于Survivor空间的一半,年龄大于或等于年龄的对象就可以直接进入老年代,无须等到MaxTenuringThreshold中要求的年龄。 在一次安全Minor GC 中，仍然存活的对象不能在另一个Survivor 完全容纳，则会通过担保机制进入老年代。 老年代空间分配担保规则 在执行任何一次Minor GC之前，JVM会检查一下老年代可用的可用内存空间，是否大于新生代所有对象的总大小 为啥会检查这个呢？因为最极端的情况下，可能新生代的Minor GC过后，所有对象都存活下来了，那岂不是新生代所有对象全部都要进入老年代？ 如果说发现老年代的内存大小是大于新生代所有对象的，此时就可以放心大胆的对新生代发起一次Minor GC了，也可以转移到老年代去。 但是假如执行Minor GC之前，发现老年代的可用内存已经小于了新生代的全部对象大小了，那么这个时候是不是有可能在Minor GC之后新生代的对象全部存活下来，然后全部需要转移到老年代去，但是老年代空间又不够？ 所以假如Minor Gc之前，发现老年代的可用内存已经小于看新生代的全部对象大小了，就会看一个-XX:-HandlePromotionFailure的参数是否设置了，如果有这个参数，那么就会继续进行下一步判断， 下一步判断，就是看老年代的内存大小，是否大于之前每一次Minor GC后进入老年代的对象的平均大小。 举个例子，之前每次Minor GC后，平均都有10MB左右的对象会进入老年代，那么此时老年代可用内存大于10MB 这就说明很可能这次Minor GC过后也是差不多10MB左右的对象会进入老年代，此时老年代空间是够的 如果上面那个步骤判断失败了，或者是 -XX:-HandlePromotionFailure“参数没设置，此时就会直接触发一次Full GC,就是对老年代进行垃圾回收，尽量腾出来一些内存空间，然后再执行Minor GC 如果上面2个步骤都判断成功了，那么就是说可以冒点风险尝试一下Minor GC，此时进行Minor GC有几种可能： （1）Minor GC过后，剩余的存活对象的大小，是小于Survivor区的大小的，那么此时存活对象进入Survicor区域即可 （2）Minor GC过后，剩余的存活对象的大小，是大于Survivor区域的大小，但是是小于老年代可用内存大小的，此时就直接进入老年代即可 （3）Minor GC过后，剩余的存活对象的大小，大于了Survivor区域的大小，也大于了老年代可用内存的大小，此时老年代都放不下这些存活对象了，就会发生Handle Promotion Failure的情况，这个时候就会触发一次Full GC Full GC就是对老年代进行垃圾回收，同时也一般会对新生代进行垃圾回收。 因为这个时候必须把老年代理的没人引用的对象给回收掉，然后才可能让Minor GC过后剩余的存活对象进入老年代里面 如果要Full GC过后，老年代还是没有足够的空间存放Minor GC过后的剩余存活对象，那么此时就会导致所谓的OOM内存溢出了。 为什么要有两个Survivor 主要是为了解决内存碎片化和效率问题。如果只有一个Survivor时，每触发一次minor gc都会有数据从Eden放到Survivor，一直这样循环下去。注意的是，Survivor区也会进行垃圾回收，这样就会出现内存碎片化问题。 碎片化会导致堆中可能没有足够大的连续空间存放一个大对象，影响程序性能。如果有两块Survivor就能将剩余对象集中到其中一块Survivor上，避免碎片问题。 JVM参数 PerfMa-JVM命令查询 # 查看当前JDK所有参数及其默认值 java -XX:+PrintFlagsFinal -version # 触发cms gc的老生代使用率 -XX:CMSInitiatingOccupancyFraction=n # 配合上一个命令使用 -XX:+UseCMSInitiatingOccupancyOnly # 最大GC暂停时间目标(毫秒)，或(仅G1)每个MMU时间片的最大GC时间 -XX:MaxGCPauseMillis=n # 默认是45，也就是heap中45%的容量被使用，则会触发concurrent gc。 -XX:InitiatingHeapOccupancyPercent=45 -Xmx20M # 最大堆的空间 -Xms20M # 最小堆的空间 -Xmn10M # 设置新生代的大小 # （设置新生代和老年代的比值，如果设置为4则表示（eden+from（或者叫s0）+to（或者叫s1））： 老年代 =1：4），即年轻代占堆的五分之一 -XX:NewRatio=4 # 设置两个Survivor（幸存区from和to或者叫s0或者s1区）和eden区的比），8表示两个Survivor：eden=2:8，即Survivor区占年轻代的五分之一 -XX:SurvivorRatio=8 # 需要禁用TLAB -XX:-UseTLAB G1垃圾回收器参数配置 选项/默认值 说明 -XX:+UseG1GC 使用 G1 (Garbage First) 垃圾收集器 -XX:MaxGCPauseMillis=n 设置最大GC停顿时间(GC pause time)指标(target). 这是一个软性指标(soft goal), JVM 会尽量去达成这个目标. -XX:InitiatingHeapOccupancyPercent=n 启动并发GC周期时的堆内存占用百分比. G1之类的垃圾收集器用它来触发并发GC周期,基于整个堆的使用率,而不只是某一代内存的使用比. 值为 0 则表示\"一直执行GC循环\". 默认值为 45. -XX:NewRatio=n 新生代与老生代(new/old generation)的大小比例(Ratio). 默认值为 2. -XX:SurvivorRatio=n eden/survivor 空间大小的比例(Ratio). 默认值为 8. -XX:MaxTenuringThreshold=n 提升年老代的最大临界值(tenuring threshold). 默认值为 15. -XX:ParallelGCThreads=n 设置垃圾收集器在并行阶段使用的线程数,默认值随JVM运行的平台不同而不同. -XX:ConcGCThreads=n 并发垃圾收集器使用的线程数量. 默认值随JVM运行的平台不同而不同. -XX:G1ReservePercent=n 设置堆内存保留为假天花板的总量,以降低提升失败的可能性. 默认值是 10. -XX:G1HeapRegionSize=n G1内堆内存区块大小G1将堆内存默认均分为2048块，1M(Xms + Xmx ) /2 / 2048 , 不大于32M，不小于1M，且为2的指数 -XX:G1MixedGCCountTarget 一次global concurrent marking之后，最多执行Mixed GC的次数.默认值8次 -XX:G1NewSizePercent 初始化年轻代占用整个堆内存的百分比，新生代占堆的最小比例，默认5% -XX:G1MaxNewSizePercent 新生代占堆的最大比例，默认60% -XX:+ParallelRefProcEnabled 并行处理Reference，加快处理速度，缩短耗时 -XX:G1OldCSetRegionThresholdPercent Mixed GC每次回收Region的数量一次Mixed GC中能被选入CSet的最多old generation region数量比列。默认值10% jvm实用调优参数（G1）_点滴之积-CSDN博客_jvm g1 参数 JVM 参数说明（持续更新） - Rayn——做今天最好的自己 - OSCHINA G1 Garbage Collector 及JVM 参数说明（持续更新） - Rayn——做今天最好的自己 - OSCHINA 参数调优案例 Full GC调优Tips： 如果可能是大对象太多造成的，可以gc+heap=info查看humongous regions个数。可以增加通过-XX:G1HeapRegionSize增加Region Size，避免老年代中的大对象占用过多的内存。 增加heap大小，对应的效果G1可以有更多的时间去完成Concurrent Marking。 增加Concurrent Marking的线程，通过-XX:ConcGCThreads设置。 强制mark阶段提早进行。因为在Mark阶段之前，G1会根据应用程序之前的行为，去确定the Initiating Heap Occupancy Percent(IHOP)阈值大小，比如是否需要执行initial Mark，以及后续CleanUp阶段的space-reclamation phase；如果服务流量突然增加或者其他行为改变的话，那么基于之前的预测的阈值就会不准确，可以采取下面的思路： 可以增加G1在IHOP分析过程中的所需要的内存空间，通过-XX:G1ReservePercent来设置，提高预测的效率。 关闭G1的自动IHOP分析机制，-XX:-G1UseAdaptiveIHOP，然后手动的指定这个阈值大小，-XX:InitiatingHeapOccupancyPercent。这样就省去了每次预测的一个时间消耗。 Full gc可能是系统中的humongous object比较多，系统找不到一块连续的regions区域来分配。可以通过-XX:G1HeapRegionSize增加region size，或者将整个heap调大。 Mixed GC或者Young GC调优 Reference Object Processing时间消耗比较久 gc日志中可以看Ref Proc和Ref Enq，Ref ProcG1根据不同引用类型对象的要求去更新对应的referents；Ref EnqG1如果实际引用对象已经不可达了，那么就会将这些引用对象加入对应的引用队列中。如果这一过程比较长，可以考虑将这个过程开启并行，通过-XX:+ParallelRefProcEnabled。 young-only回收较久 主要原因是Collection Set中有太多的存活对象需要拷贝。可以通过gc日志中的Evacuate Collection Set看到对应的时间，可以增加young geenration的最小大小，通过-XX:G1NewSizePercent。 也可能是某一个瞬间，幸存下来的对象一下子有很多，这种情况会造成gc停顿时间猛涨，一般应对这种情况通过-XX:G1MaxNewSizePercent这个参数，增加young generation最大空间。 Mixed回收时间较久 通过开启gc+ergo+cset=trace，如果是predicated young regions花费比较长，可以针对上文中的方法。如果是predicated old regions比较长，则可以通过以下方法： 增加-XX:G1MixedGCCountTarget这个参数，将old generation的regions分散到较多的Collection（上文有解释）中，增加-XX:G1MixedGCCountTarget参数值。避免单次处理较大块的Collection。 https://juejin.im/post/5ed32ec96fb9a0480659e547 CMS的CMSInitiatingOccupancyFraction默认值是多少？又是如何使用的？ - 代码先锋网 先说结论： MinHeapFreeRatio在JDK1.7是0，所以根据公式计算_initiating_occupancy最终得到的值是100%。 而MinHeapFreeRatio在JDK1.8是40，根据公式_initiating_occupancy计算得到的值是92%（68%的可能是其他版本计算得到的值了~） 最后来总结一下吧： 1、CMSInitiatingOccupancyFraction默认值是-1，并不是许多人说的68、92之类的。 2、CMSInitiatingOccupancyFraction是用来计算老年代最大使用率（_initiating_occupancy）的。大于等于0则直接取百分号，小于0则根据公式来计算。这个_initiating_occupancy需要配合-XX:+UseCMSInitiatingOccupancyOnly来使用。 3、不同版本最终得到的_initiating_occupancy不同，归根结底应该是不同版本下的MinHeapFreeRatio值不同。 公式： void ConcurrentMarkSweepGeneration::init_initiating_occupancy(intx io, uintx tr) { assert(io = 0) { _initiating_occupancy = (double)io / 100.0; } else { _initiating_occupancy = ((100 - MinHeapFreeRatio) + (double)(tr * MinHeapFreeRatio) / 100.0) / 100.0; } } java -XX:+PrintFlagsFinal -version |grep CMSInitiatingOccupancyFraction java -XX:+PrintFlagsFinal -version |grep MinHeapFreeRatio java -XX:+PrintFlagsFinal -version |grep -E \"CMSInitiatingOccupancyFraction|MinHeapFreeRatio|CMSTriggerRatio\" 可我在JDK1.8.0_171上执行，MinHeapFreeRatio = 0为什么啊，上文中说应该是40 $ java -XX:+PrintFlagsFinal -version |grep -E \"CMSInitiatingOccupancyFraction|MinHeapFreeRatio|CMSTriggerRatio\" intx CMSInitiatingOccupancyFraction = -1 {product} uintx CMSTriggerRatio = 80 {product} uintx MinHeapFreeRatio = 0 {manageable} java version \"1.8.0_171\" Java(TM) SE Runtime Environment (build 1.8.0_171-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode) 续： 后来通过命令查看运行进程的JVM参数：jmap -heap pid MinHeapFreeRatio居然打印出的是40，为什么不一样啊（这里我用的是G1启动的） Heap Configuration: MinHeapFreeRatio = 40 MaxHeapFreeRatio = 70 MaxHeapSize = 4294967296 (4096.0MB) NewSize = 1073741824 (1024.0MB) MaxNewSize = 1073741824 (1024.0MB) OldSize = 5452592 (5.1999969482421875MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 2097152 (2.0MB) JVM本地测试用例 【深入浅出-JVM】（37）：对象从新生代到老年代过程 | 思考者浩哥 深入理解JVM学习笔记(二十七、JVM 内存分配----大对象直接分配到老年代)_Java_张--小涛涛-CSDN博客 "},"docs/Guide/Spring的一些概念.html":{"url":"docs/Guide/Spring的一些概念.html","title":"Spring的一些概念","keywords":"","body":"Spring的一些概念和思想 Spring的核心：AOP、IOC。 简单点说，就是把对象交给Spring进行管理，通过面向切面编程来实现一些\"模板式\"的操作，使得程序员解放出来，可以更加关注业务实现。 Spring已经是一站式的开源框架解决方案，而且形成了Spring生态。 IOC IOC，从操作上来看，要么通过XML配置实现，要么通过注解的方式实现。在实际开发中，越来越流行注解的方式。 需要根据你使用到的功能，来进行依赖的引入，以及XML的Schema约束引用。 Spring的核心配置文件，名称和位置不固定，不过在实际开发中，一般指定为applicationContext-xxx.xml的方式。这种方式，有2个好处：第一，可以对Spring的配置文件进行分模块管理；第二，由于统一的前缀，方便正则加载这些配置文件。 bean的XML配置创建 直接通过来进行，这样的前提是该类存在无参数的构造方法（背后的原理就是通过反射实例化的）。这种方式是实际中最常使用的，当然，除此之外还存在静态工厂、实例工厂的方式创建。 另外一点还需要注意的是：bean是单例的，还是多例的？ bean标签中存在scope属性用于说明： singleton：单例，default prototype：多例 request/session等。 bean的属性注入:XML方式 bean的XML创建是通过反射进行，那么bean的属性注入，是如何进行的呢？ 可以在构造bean的时候，提供有参数的构造方法进行设置； 可以在提供setter方法，进行设置；（最常用） 什么接口注入，什么P名称空间注入，这些实际都不用…… 或者 要么利用value直接给出属性值，要么通过ref引用另一个bean。 基于注解方式的bean创建以及注入 因为Spring注解的实现是需要AOP的支持，因此在依赖方面需要注意，其次，要在XML中开启注解扫描： 实际上这个配置，会让Spring在指定包下扫描，把带有注解标志的bean实例化，并且会进行属性注入。（你可以参考《写出我的第一个框架：迷你版Spring MVC》） 创建对象的4个注解： @Component/@Controller/@Service/@Repository 在这4个注解上，通过value属性来指定bean的id，通过@scope配合来声明单例OR多例。（目前这4个注解功能是一样的，只是为了让标注类的用途更加清晰，而且Spring留了一手，以后说不定会增强功能呢？） 如何注入属性？ @Autowired/@Resource/@Qualifier 需要注意的是@Resource是javax包下的，说白了就是J2EE提供的；而@Autowired是Spring提供的。（不必提供setter方法） @Resource默认按照名称注入，如果找不到才按照类型注入。 @Autowired默认按照类型注入，可以结合@Qualifier进行名称注入。 那么我们需要注意什么呢？ 如果@Autowired进行类型注入，很可能类型会有多个满足（多态），那么到底注入哪个呢？所以说，如果按照@Autowired类型注入，一定注意这点，结合@Qualifier。实际开发中，显然，注入应该是确定的，那么按照名称注入，应该是首选！ IOC VS DI IOC，控制反转；DI，依赖注入； 只有把对象交给Spring，才能由Spring帮助完成属性设置；因此，依赖注入不能单独存在，需要在IOC基础之上完成操作。 AOP 关于AOP的几个重点概念： JoinPoint：连接点，说白了，就是可以被增强的方法； PointCut：切入点，对哪些JoinPoint进行拦截； Advice：通知，就是拦截后的动作； Aspect：切面，把增强应用到具体方法的过程； Spring的AOP需要借助aspectj来实现，可以通过XML，也可以通过注解来完成。 比如，采用XML方式的话，需要指明用A类的哪个方法对B类的哪些方法上进行增强，这里就涉及到execution表达式了； 比如，采用注解方式的话，就更加简单了，先在XML中开启AOP（），然后在增强方法上直接使用类似@Before(value=\"execution(具体的表达式)\")即可； 其实，在实际开发中，我们对于AOP最常用的就是事务了。 Spring的事务管理 Spring的声明式事务管理，用的最多的就是基于注解的方式。首先我们得配置一个事务管理器，而事务管理器需要我们注入DataSource（DBCP,c3p0等连接池），这一点好理解，因为是DB的事务。要知道，Spring对不同的DAO层框架（Spring JDBC/MyBatis/Hibernate…）提供了不同的事务实现类。 对于多个数据源，当然，我们需要定义多个事务管理器，同时也得开启事务注解。多个事务管理器，可以通过qualifier属性进行区分。 配置完毕后，直接在service层的类或者方法上，使用 @Transactional(value = \"gcs\", rollbackFor = Exception.class) "},"docs/Guide/Spring复习.html":{"url":"docs/Guide/Spring复习.html","title":"Spring复习","keywords":"","body":"Spring Bean的作用域 singleton——唯一 bean 实例 prototype——每次请求都会创建一个新的 bean 实例 request——每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效 session——每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效 globalSession——作用域类似于标准的 HTTP session 作用域，不过仅仅在基于 portlet 的 web 应用中才有意义。 Spring Bean的生命周期 四大阶段： 实例化 属性赋值 初始化 销毁 PostConstruct (P)，afterPropertiesSet (A)，init-method (I) ---> PAI （圆周率π） InstantiationAwareBeanPostProcessor又代表了Spring的另外一段生命周期：实例化。 先区别一下Spring Bean的实例化和初始化两个阶段的主要作用： 1、实例化----实例化的过程是一个创建Bean的过程，即调用Bean的构造函数，单例的Bean放入单例池中 2、初始化----初始化的过程是一个赋值的过程，即调用Bean的setter，设置Bean的属性 InstantiationAwareBeanPostProcessor作用的是Bean实例化前后，即： 1、Bean构造出来之前调用postProcessBeforeInstantiation()方法 2、Bean构造出来之后调用postProcessAfterInstantiation()方法 不过通常来讲，我们不会直接实现InstantiationAwareBeanPostProcessor接口，而是会采用继承InstantiationAwareBeanPostProcessorAdapter这个抽象类的方式来使用。 如果 bean 的 scope 设为prototype时，当容器关闭时，destroy 方法不会被调用。对于 prototype 作用域的 bean，有一点非常重要，那就是 Spring不能对一个 prototype bean 的整个生命周期负责：容器在初始化、配置、装饰或者是装配完一个prototype实例后，将它交给客户端，随后就对该prototype实例不闻不问了。 不管何种作用域，容器都会调用所有对象的初始化生命周期回调方法。但对prototype而言，任何配置好的析构生命周期回调方法都将不会被调用。清除prototype作用域的对象并释放任何prototype bean所持有的昂贵资源，都是客户端代码的职责（让Spring容器释放被prototype作用域bean占用资源的一种可行方式是，通过使用bean的后置处理器，该处理器持有要被清除的bean的引用）。谈及prototype作用域的bean时，在某些方面你可以将Spring容器的角色看作是Java new操作的替代者，任何迟于该时间点的生命周期事宜都得交由客户端来处理。 JavaGuide Spring Bean的生命周期（非常详细） - shoshana~ - 博客园 Spring Bean生命周期-阶段汇总，面试必备(十二) - 简书 请别再问Spring Bean的生命周期了！ - 简书 Spring Bean 生命周期之“我从哪里来？” 懂得这个很重要 - 个人文章 - SegmentFault 思否 Spring解析，加载及实例化Bean的顺序（零配置）_点滴之积-CSDN博客_spring 加载顺序 Spring-bean的循环依赖以及解决方式_Java_惜暮-CSDN博客 Springboot循环依赖如何解决_Java_二十-CSDN博客 spring处理对象相互依赖注入的问题_Java_w1lgy的博客-CSDN博客 Spring 事务实现原理（AOP） 划分处理单元 IOC 由于 Spring 解决的问题是对单个数据库进行局部事务处理的，具体的实现首相用 Spring 中的 IOC 划分了事务处理单元。并且将对事务的各种配置放到了 IOC 容器中（设置事务管理器，设置事务的传播特性及隔离机制）。 AOP 拦截需要进行事务处理的类 Spring 事务处理模块是通过 AOP 功能来实现声明式事务处理的，具体操作（比如事务实行的配置和读取，事务对象的抽象），用 TransactionProxyFactoryBean 接口来使用 AOP 功能，生成 proxy 代理对象，通过 TransactionInterceptor 完成对代理方法的拦截，将事务处理的功能编织到拦截的方法中。读取 IOC 容器事务配置属性，转化为 Spring 事务处理需要的内部数据结构（TransactionAttributeSourceAdvisor），转化为 TransactionAttribute 表示的数据对象。 对事物处理实现（事务的生成、提交、回滚、挂起） Spring 委托给具体的事务处理器实现。实现了一个抽象和适配。适配的具体事务处理器：DataSource 数据源支持、Hibernate 数据源事务处理支持、JDO 数据源事务处理支持，JPA、JTA 数据源事务处理支持。这些支持都是通过设计 PlatformTransactionManager、AbstractPlatforTransaction 一系列事务处理的支持。 为常用数据源支持提供了一系列的 TransactionManager。 Spring事务原理完全解析 | 码农网 Spring IOC、Spring AOP原理 IoC IoC（Inverse of Control:控制反转）是一种设计思想，就是 将原本在程序中手动创建对象的控制权，交由Spring框架来管理。 IoC 在其他语言中也有应用，并非 Spirng 特有。 IoC 容器是 Spring 用来实现 IoC 的载体， IoC 容器实际上就是个Map（key，value）,Map 中存放的是各种对象。 将对象之间的相互依赖关系交给 IoC 容器来管理，并由 IoC 容器完成对象的注入。这样可以很大程度上简化应用的开发，把应用从复杂的依赖关系中解放出来。 IoC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。 在实际项目中一个 Service 类可能有几百甚至上千个类作为它的底层，假如我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IoC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。 Spring 时代我们一般通过 XML 文件来配置 Bean，后来开发人员觉得 XML 文件来配置不太好，于是 SpringBoot 注解配置就慢慢开始流行起来。 推荐阅读：https://www.zhihu.com/question/23277575/answer/169698662 Spring IoC的初始化过程： IoC源码阅读 https://javadoop.com/post/spring-ioc IOC VS DI IOC，控制反转；DI，依赖注入； 只有把对象交给Spring，才能由Spring帮助完成属性设置；因此，依赖注入不能单独存在，需要在IOC基础之上完成操作。 AOP AOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。 Spring AOP就是基于动态代理的，如果要代理的对象，实现了某个接口，那么Spring AOP会使用JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候Spring AOP会使用Cglib ，这时候Spring AOP会使用 Cglib 生成一个被代理对象的子类来作为代理。 Spring AOP分析(1) -- 基本概念 | YGingko's Blog Spring AOP分析(2) -- JdkDynamicAopProxy实现AOP | YGingko's Blog Spring AOP分析(3) -- CglibAopProxy实现AOP | YGingko's Blog Spring Aop和AspecJ Aop有什么区别吗？ 按住Citrl/Command点击跳转 Spring IOC依赖注入的方式 Spring依赖注入的方式主要有四个 基于注解注入方式 set注入方式 构造器注入方式 静态工厂注入方式。 推荐使用基于注解注入方式，配置较少，比较方便。 Spring IOC怎么解决循环依赖 三级缓存，必须在实例化之后，刷新了singletonFactories。 这三级缓存分别指： singletonFactories ： 单例对象工厂的cache earlySingletonObjects ：提前曝光的单例对象的Cache singletonObjects：单例对象的cache 让我们来分析一下“A的某个field或者setter依赖了B的实例对象，同时B的某个field或者setter依赖了A的实例对象”这种循环依赖的情况。A首先完成了初始化的第一步，并且将自己提前曝光到singletonFactories中，此时进行初始化的第二步，发现自己依赖对象B，此时就尝试去get(B)，发现B还没有被create，所以走create流程，B在初始化第一步的时候发现自己依赖了对象A，于是尝试get(A)，尝试一级缓存singletonObjects(肯定没有，因为A还没初始化完全)，尝试二级缓存earlySingletonObjects（也没有），尝试三级缓存singletonFactories，由于A通过ObjectFactory将自己提前曝光了，所以B能够通过ObjectFactory.getObject拿到A对象(虽然A还没有初始化完全，但是总比没有好呀)，B拿到A对象后顺利完成了初始化阶段1、2、3，完全初始化之后将自己放入到一级缓存singletonObjects中。此时返回A中，A此时能拿到B的对象顺利完成自己的初始化阶段2、3，最终A也完成了初始化，进去了一级缓存singletonObjects中，而且更加幸运的是，由于B拿到了A的对象引用，所以B现在hold住的A对象完成了初始化。 知道了这个原理时候，肯定就知道为啥Spring不能解决“A的构造方法中依赖了B的实例对象，同时B的构造方法中依赖了A的实例对象”这类问题了！因为加入singletonFactories三级缓存的前提是执行了构造器，所以构造器的循环依赖没法解决。 Spring-bean的循环依赖以及解决方式_Java_惜暮-CSDN博客 Spring IOC 容器源码分析 - 循环依赖的解决办法 - 个人文章 - SegmentFault 思否 面试中被问Spring循环依赖的三种方式！！！ - JaJian - 博客园 【死磕 Spring】- IOC 之循环依赖处理 - Java 技术驿站-Java 技术驿站 IOC容器的加载过程 简单概括： 刷新预处理 将配置信息解析，注册到BeanFactory 设置bean的类加载器 如果有第三方想在bean加载注册完成后，初始化前做点什么(例如修改属性的值，修改bean的scope为单例或者多例。)，提供了相应的模板方法，后面还调用了这个方法的实现，并且把这些个实现类注册到对应的容器中 初始化当前的事件广播器 初始化所有的bean 广播applicationcontext初始化完成。 //来自于AbstractApplicationContext public void refresh() throws BeansException, IllegalStateException { //进行加锁处理 synchronized (this.startupShutdownMonitor) { // 进行刷新容器的准备工作，比如设定容器开启时间，标记容器已启动状态等等 prepareRefresh(); // 让子类来刷新创建容器 // 这步比较关键，这步完成后，配置文件就会解析成一个个 Bean 定义，注册到 BeanFactory 中， // 当然，这里说的 Bean 还没有初始化，只是配置信息都提取出来了， // 注册也只是将这些信息都保存到了注册中心(说到底核心是一个 beanName-> beanDefinition 的 map) ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean prepareBeanFactory(beanFactory); try { // 这里需要知道 BeanFactoryPostProcessor 这个知识点， //Bean 如果实现了此接口，那么在容器初始化以后，Spring 会负责调用里面的 postProcessBeanFactory 方法。 // 这里是提供给子类的扩展点，到这里的时候，所有的 Bean 都加载、注册完成了，但是都还没有初始化 // 具体的子类可以在这步的时候添加一些特殊的 BeanFactoryPostProcessor 的实现类或做点什么事 postProcessBeanFactory(beanFactory); // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 的实现类，注意看和 BeanFactoryPostProcessor 的区别 // 此接口两个方法: postProcessBeforeInitialization 和 postProcessAfterInitialization // 两个方法分别在 Bean 初始化之前和初始化之后得到执行。注意，到这里 Bean 还没初始化 registerBeanPostProcessors(beanFactory); // 初始化当前 ApplicationContext 的 MessageSource initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器 initApplicationEventMulticaster(); // 从方法名就可以知道，典型的模板方法(钩子方法)， // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前） onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口 registerListeners(); // 初始化所有的 singleton beans（lazy-init 的除外） // 重点方法将会在下一个章节进行说明 finishBeanFactoryInitialization(beanFactory); // 最后，广播事件，ApplicationContext 初始化完成 finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(\"Exception encountered during context initialization - cancelling refresh attempt: \" + ex); } // 销毁已经初始化的 singleton 的 Beans，以免有些 bean 会一直占用资源 destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // 把异常往外抛 throw ex; } finally { // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); } } } IOC容器是怎么工作的 Bean的概念：Bean就是由Spring容器初始化、装配及管理的对象，除此之外，bean就与应用程序中的其他对象没什么区别了。 元数据BeanDefinition：确定如何实例化Bean、管理bean之间的依赖关系以及管理bean，这就需要配置元数据，在spring中由BeanDefinition代表。 工作原理： 准备配置文件：配置文件中声明Bean定义也就是为Bean配置元数据。 由IOC容器进行解析元数据：IOC容器的Bean Reader读取并解析配置文件，根据定义生成BeanDefinition配置元数据对象，IOC容器根据BeanDefinition进行实例化、配置以及组装Bean。 实例化IOC容器：由客户端实例化容器，获取需要的Bean。 下面举个例子： @Test public void testHelloWorld() { //1、读取配置文件实例化一个IoC容器 ApplicationContext context = new ClassPathXmlApplicationContext(\"helloworld.xml\"); //2、从容器中获取Bean，注意此处完全“面向接口编程，而不是面向实现” HelloApi helloApi = context.getBean(\"hello\", HelloApi.class); //3、执行业务逻辑 helloApi.sayHello(); } 拦截器和过滤器的区别 @Autowrite和@Resource的区别 @Autowired与@Resource都可以用来装配Bean，都可以写在字段、setter方法上。 @Autowired是spring自己定义的注解，@Resource是J2EE的，由JSR-250规范定义 @Autowired默认按类型进行自动装配（该注解属于Spring），默认情况下要求依赖对象必须存在，如果要允许为null，需设置required属性为false，例：@Autowired(required=false)。如果要使用名称进行装配，可以与@Qualifier注解一起使用。 @Autowired @Qualifier(\"adminService\") private AdminService adminService; @Resource默认按照名称进行装配（该注解属于J2EE），名称可以通过name属性来指定。如果没有指定name属性，当注解写在字段上时，默认取字段名进行装配；如果注解写在setter方法上，默认取属性名进行装配。当找不到与名称相匹配的Bean时，会按照类型进行装配。但是，name属性一旦指定，就只会按照名称进行装配。 @Resource(name = \"adminService\") private AdminService adminService; @Autowired字段注入会产生警告，并且建议使用构造方法注入。而使用@Resource不会有警告。 不建议使用字段注入和setter注入的方法，前者可能引起NullPointerException，后者不能将属性设置为final。从spring4开始官方一直推荐使用构造方法注入。但是构造方法注入只能使用@Autowired，不能使用@Resource。 综上，如果要字段注入，使用@Resource；要构造方法注入，使用@Autowired。其实实际开发中个人觉得没什么区别。 那么我们需要注意什么呢？ 如果@Autowired进行类型注入，很可能类型会有多个满足（多态），那么到底注入哪个呢？所以说，如果按照@Autowired类型注入，一定注意这点，结合@Qualifier。实际开发中，显然，注入应该是确定的，那么按照名称注入，应该是首选！ @Component和@Bean的区别 作用对象不同: @Component 注解作用于类，而@Bean注解作用于方法。 @Component通常是通过类路径扫描来自动侦测以及自动装配到Spring容器中（我们可以使用 @ComponentScan 注解定义要扫描的路径从中找出标识了需要装配的类自动装配到 Spring 的 bean 容器中）。@Bean 注解通常是我们在标有该注解的方法中定义产生这个 bean,@Bean告诉了Spring这是某个类的示例，当我需要用它的时候还给我。 @Bean 注解比 Component 注解的自定义性更强，而且很多地方我们只能通过 @Bean 注解来注册bean。比如当我们引用第三方库中的类需要装配到 Spring容器时，则只能通过 @Bean来实现。 BeanFactory和ApplicationContext的区别 BeanFactory是spring中最基础的接口。它负责读取bean配置文档，管理bean的加载，实例化，维护bean之间的依赖关系，负责bean的生命周期。 ApplicationContext是BeanFactory的子接口，除了提供上述BeanFactory的所有功能外，还提供了更完整的框架功能：如国际化支持，资源访问，事件传递等。常用的获取ApplicationContext的方法： 2.1 FileSystemXmlApplicationContext：从文件系统或者url指定的xml配置文件创建，参数为配置文件名或者文件名数组。 2.2 ClassPathXmlApplicationContext：从classpath的xml配置文件创建，可以从jar包中读取配置文件 2.3 WebApplicationContextUtils：从web应用的根目录读取配置文件，需要先在web.xml中配置，可以配置监听器或者servlet来实现。 ApplicationContext的初始化和BeanFactory有一个重大区别：BeanFactory在初始化容器时，并未实例化Bean，直到第一次访问某个Bean时才实例化Bean；而ApplicationContext则在初始化应用上下文时就实例化所有的单例Bean，因此ApplicationContext的初始化时间会比BeanFactory稍长一些。 BeanFactory和FactoryBean的区别 共同点： ​ 都是接口 区别： BeanFactory 以Factory结尾，表示它是一个工厂类，用于管理Bean的一个工厂 ​ 在Spring中，所有的Bean都是由BeanFactory(也就是IOC容器)来进行管理的。 但对FactoryBean而言，这个Bean不是简单的Bean，而是一个能生产或者修饰对象生成的工厂Bean, ​ 它的实现与设计模式中的工厂模式和修饰器模式类似。 Spring中共有两种bean，一种为普通bean，另一种则为工厂bean（FactoryBean）。 BeanFactory给具体的IOC容器的实现提供了规范，实现 BeanFactory 接口的类 表明此类是一个工厂，作用就是配置、新建、管理 各种Bean。 另一个接口ApplicationContext接口，他是BeanFactory的派生。BeanFactorty接口提供了配置框架及基本功能，但是无法支持spring的aop功能和web应用。 ApplicationContext支持aop web，因为他继承多种接口如 （1）国际化资源接口 （2）资源加载接口 （3）事件发布接口 46、BeanFactory和FactoryBean的区别_JAVA基础面试题在线听_考试培训课程-喜马拉雅FM spring中BeanFactory和FactoryBean的区别 - bcombetter - 博客园 SpringMVC工作原理 上图的一个笔误的小问题：Spring MVC 的入口函数也就是前端控制器 DispatcherServlet 的作用是接收请求，响应结果。 流程说明（重要）： 客户端（浏览器）发送请求，直接请求到 DispatcherServlet。 DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。 解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。 HandlerAdapter 会根据 Handler来调用真正的处理器开处理请求，并处理相应的业务逻辑。 处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。 ViewResolver 会根据逻辑 View 查找实际的 View。 DispaterServlet 把返回的 Model 传给 View（视图渲染）。 把 View 返回给请求者（浏览器） Spring事务 Spring 管理事务的方式有几种？ 编程式事务，在代码中硬编码。(不推荐使用) 声明式事务，在配置文件中配置（推荐使用） 声明式事务又分为两种： 基于XML的声明式事务 基于注解的声明式事务 Spring 事务中的隔离级别有哪几种? TransactionDefinition 接口中定义了五个表示隔离级别的常量： TransactionDefinition.ISOLATION_DEFAULT: 使用后端数据库默认的隔离级别，Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE: 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 Spring 事务中哪几种事务传播行为? 支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 不支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRES_NEW： 创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常。 其他情况： TransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。 @Transactional(rollbackFor = Exception.class)注解了解吗？ 我们知道：Exception分为运行时异常RuntimeException和非运行时异常。事务管理对于企业应用来说是至关重要的，即使出现异常情况，它也可以保证数据的一致性。 当@Transactional注解作用于类上时，该类的所有 public 方法将都具有该类型的事务属性，同时，我们也可以在方法级别使用该标注来覆盖类级别的定义。如果类或者方法加了这个注解，那么这个类里面的方法抛出异常，就会回滚，数据库里面的数据也会回滚。 在@Transactional注解中如果不配置rollbackFor属性,那么事物只会在遇到RuntimeException的时候才会回滚,加上rollbackFor=Exception.class,可以让事物在遇到非运行时异常时也回滚。 事务不生效的几个结论 private,protect方法事务不生效 final修饰符修饰的方法和类不生效。 在当前的bean中，没有加上事务的方法调用有事务的方法不生效。如下，调用index方法，save方法上的事务不生效。 只要是以代理方式实现的声明式事务，无论是JDK动态代理，还是CGLIB直接写字节码生成代理，都只有public方法上的事务注解才起作用。而且必须在代理类外部调用才行，如果直接在目标类里面调用，事务照样不起作用。 据底层所使用的不同的持久化 API 或框架，使用如下： DataSourceTransactionManager：适用于使用JDBC和iBatis进行数据持久化操作的情况，在定义时需要提供底层的数据源作为其属性，也就是 DataSource。 HibernateTransactionManager：适用于使用Hibernate进行数据持久化操作的情况，与 HibernateTransactionManager 对应的是 SessionFactory。 JpaTransactionManager：适用于使用JPA进行数据持久化操作的情况，与 JpaTransactionManager 对应的是 EntityManagerFactory。 Spring Data Jpa、Jpa、Hibernate、JDBC四者之间的关系 JPA 是一套规范，内部是由接口和抽象类组成。 Hibernate是一套成熟的ORM框架，而且Hibernate实现了JPA规范，所以也可以称Hibernate是JPA的一种实现方式，我们使用JPA的API编程，意味着站在更高的角度上看待问题（面向接口编程）。 Spring Data JPA是Spring提供的一套对JPA操作更加高级的封装，是在JPA规范下的专门用来进行数据持久化的解决方案。 JDBC（Java DataBase Connectivity）是java连接数据库操作的原生接口。JDBC对Java程序员而言是API，对实现与数据库连接的服务提供商而言是接口模型。作为API，JDBC为程序开发提供标准的接口，并为各个数据库厂商及第三方中间件厂商实现与数据库的连接提供了标准方法。一句话概括：JDBC是所有框架操作数据库的必须要用的，由数据库厂商提供，但是为了方便Java程序员调用各个数据库，各个数据库厂商都要实现JDBC接口。 Spring中用到的设计模式 IoC(Inversion of Control,控制翻转) 是Spring 中一个非常非常重要的概念，它不是什么技术，而是一种解耦的设计思想。它的主要目的是借助于“第三方”(Spring 中的 IOC 容器) 实现具有依赖关系的对象之间的解耦(IOC容易管理对象，你只管使用即可)，从而降低代码之间的耦合度。IOC 是一个原则，而不是一个模式，以下模式（但不限于）实现了IoC原则。 Spring IOC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。 IOC 容器负责创建对象，将对象连接在一起，配置这些对象，并从创建中处理这些对象的整个生命周期，直到它们被完全销毁。 控制翻转怎么理解呢? 举个例子：\"对象a 依赖了对象 b，当对象 a 需要使用 对象 b的时候必须自己去创建。但是当系统引入了 IOC 容器后， 对象a 和对象 b 之前就失去了直接的联系。这个时候，当对象 a 需要使用 对象 b的时候， 我们可以指定 IOC 容器去创建一个对象b注入到对象 a 中\"。 对象 a 获得依赖对象 b 的过程,由主动行为变为了被动行为，控制权翻转，这就是控制反转名字的由来。 DI(Dependecy Inject,依赖注入)是实现控制反转的一种设计模式，依赖注入就是将实例变量传入到一个对象中去。 工厂设计模式 Spring使用工厂模式可以通过 BeanFactory 或 ApplicationContext 创建 bean 对象。 两者对比： BeanFactory ：延迟注入(使用到某个 bean 的时候才会注入),相比于ApplicationContext 来说会占用更少的内存，程序启动速度更快。 ApplicationContext ：容器启动的时候，不管你用没用到，一次性创建所有 bean 。BeanFactory 仅提供了最基本的依赖注入支持，ApplicationContext 扩展了 BeanFactory ,除了有BeanFactory的功能还有额外更多功能，所以一般开发人员使用ApplicationContext会更多。 ApplicationContext的三个实现类： ClassPathXmlApplication：把上下文文件当成类路径资源。 FileSystemXmlApplication：从文件系统中的 XML 文件载入上下文定义信息。 XmlWebApplicationContext：从Web系统中的XML文件载入上下文定义信息。 单例设计模式 使用单例模式的好处: 对于频繁使用的对象，可以省略创建对象所花费的时间，这对于那些重量级对象而言，是非常可观的一笔系统开销； 由于 new 操作的次数减少，因而对系统内存的使用频率也会降低，这将减轻 GC 压力，缩短 GC 停顿时间。 Spring 中 bean 的默认作用域就是 singleton(单例)的。 Spring 实现单例的方式： xml : 注解：@Scope(value = \"singleton\") Spring 通过 ConcurrentHashMap 实现单例注册表的特殊方式实现单例模式。 代理设计模式 代理模式在 AOP 中的应用 AOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。 Spring AOP 就是基于动态代理的，如果要代理的对象，实现了某个接口，那么Spring AOP会使用JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候Spring AOP会使用Cglib 生成一个被代理对象的子类来作为代理，如下图所示： 当然你也可以使用 AspectJ ,Spring AOP 已经集成了AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。 使用 AOP 之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样大大简化了代码量。我们需要增加新功能时也方便，这样也提高了系统扩展性。日志功能、事务管理等等场景都用到了 AOP 。 Spring AOP 和 AspectJ AOP 有什么区别? Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。 Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单， 如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比Spring AOP 快很多。 模板方法 模板方法模式是一种行为设计模式，它定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。 模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤的实现方式。 Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。一般情况下，我们都是使用继承的方式来实现模板模式，但是 Spring 并没有使用这种方式，而是使用Callback 模式与模板方法模式配合，既达到了代码复用的效果，同时增加了灵活性。 观察者模式 观察者模式是一种对象行为型模式。它表示的是一种对象与对象之间具有依赖关系，当一个对象发生改变的时候，这个对象所依赖的对象也会做出反应。Spring 事件驱动模型就是观察者模式很经典的一个应用。Spring 事件驱动模型非常有用，在很多场景都可以解耦我们的代码。比如我们每次添加商品的时候都需要重新更新商品索引，这个时候就可以利用观察者模式来解决这个问题。 Spring的事件驱动模型就是这种模式，该模型中的三个重要角色： 事件 事件监听者 事件发布者 Spring 的事件流程总结 定义一个事件: 实现一个继承自 ApplicationEvent，并且写相应的构造函数； 定义一个事件监听者：实现 ApplicationListener 接口，重写 onApplicationEvent() 方法； 使用事件发布者发布消息: 可以通过 ApplicationEventPublisher（ApplicationContext实例的publishEvent方法） 的 publishEvent() 方法发布消息。 适配器模式 适配器模式(Adapter Pattern) 将一个接口转换成客户希望的另一个接口，适配器模式使接口不兼容的那些类可以一起工作，其别名为包装器(Wrapper)。 Spring AOP中的适配器模式 我们知道 Spring AOP 的实现是基于代理模式，但是 Spring AOP 的增强或通知(Advice)使用到了适配器模式，与之相关的接口是AdvisorAdapter 。Advice 常用的类型有：BeforeAdvice（目标方法调用前,前置通知）、AfterAdvice（目标方法调用后,后置通知）、AfterReturningAdvice(目标方法执行结束后，return之前)等等。每个类型Advice（通知）都有对应的拦截器:MethodBeforeAdviceInterceptor、AfterReturningAdviceAdapter、AfterReturningAdviceInterceptor。Spring预定义的通知要通过对应的适配器，适配成 MethodInterceptor接口(方法拦截器)类型的对象（如：MethodBeforeAdviceInterceptor 负责适配 MethodBeforeAdvice）。 Spring MVC中的适配器模式 在Spring MVC中，DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由HandlerAdapter 适配器处理。HandlerAdapter 作为期望接口，具体的适配器实现类用于对目标类进行适配，Controller 作为需要适配的类。 为什么要在 Spring MVC 中使用适配器模式？ Spring MVC 中的 Controller 种类众多，不同类型的 Controller 通过不同的方法来对请求进行处理。如果不利用适配器模式的话，DispatcherServlet 直接获取对应类型的 Controller，需要的自行来判断，像下面这段代码一样： if(mappedHandler.getHandler() instanceof MultiActionController){ ((MultiActionController)mappedHandler.getHandler()).xxx }else if(mappedHandler.getHandler() instanceof XXX){ ... }else if(...){ ... } 假如我们再增加一个 Controller类型就要在上面代码中再加入一行 判断语句，这种形式就使得程序难以维护，也违反了设计模式中的开闭原则 – 对扩展开放，对修改关闭。 装饰器模式 装饰者模式可以动态地给对象添加一些额外的属性或行为。相比于使用继承，装饰者模式更加灵活。简单点儿说就是当我们需要修改原有的功能，但我们又不愿直接去修改原有的代码时，设计一个Decorator套在原有代码外面。其实在 JDK 中就有很多地方用到了装饰者模式，比如 InputStream家族，InputStream 类下有 FileInputStream (读取文件)、BufferedInputStream (增加缓存,使读取文件速度大大提升)等子类都在不修改InputStream 代码的情况下扩展了它的功能。 Spring 中配置 DataSource 的时候，DataSource 可能是不同的数据库和数据源。我们能否根据客户的需求在少修改原有类的代码下动态切换不同的数据源？这个时候就要用到装饰者模式(这一点我自己还没太理解具体原理)。Spring 中用到的包装器模式在类名上含有 Wrapper或者 Decorator。这些类基本上都是动态地给一个对象添加一些额外的职责 "},"docs/Guide/SpringBoot.html":{"url":"docs/Guide/SpringBoot.html","title":"SpringBoot","keywords":"","body":"Spring Boot是怎么保证加载Tomcat Web容器的 自动配置 @SpringBootApplication组合注解里的@EnableAutoConfiguration 注解里引入的自动加载配置的类@Import(AutoConfigurationImportSelector.class) 自动配置文件中读取的ServletWebServerFactoryAutoConfiguration配置类里有引入EmbeddedTomcat.class @Import({ ServletWebServerFactoryAutoConfiguration.BeanPostProcessorsRegistrar.class, ServletWebServerFactoryConfiguration.EmbeddedTomcat.class, ServletWebServerFactoryConfiguration.EmbeddedJetty.class, ServletWebServerFactoryConfiguration.EmbeddedUndertow.class }) public class ServletWebServerFactoryAutoConfiguration EmbeddedTomcat类里是创建ServletWebServer的工厂配置类 @Bean public TomcatServletWebServerFactory tomcatServletWebServerFactory() { return new TomcatServletWebServerFactory(); } 通过工厂模式创建tomcat @Override public WebServer getWebServer(ServletContextInitializer... initializers) { Tomcat tomcat = new Tomcat(); File baseDir = (this.baseDirectory != null) ? this.baseDirectory : createTempDir(\"tomcat\"); tomcat.setBaseDir(baseDir.getAbsolutePath()); Connector connector = new Connector(this.protocol); tomcat.getService().addConnector(connector); customizeConnector(connector); tomcat.setConnector(connector); tomcat.getHost().setAutoDeploy(false); configureEngine(tomcat.getEngine()); for (Connector additionalConnector : this.additionalTomcatConnectors) { tomcat.getService().addConnector(additionalConnector); } prepareContext(tomcat.getHost(), initializers); return getTomcatWebServer(tomcat); } 启动流程，记录几个比较重要的过程 Tomcat tomcat = new Tomcat(); getTomcatWebServer(tomcat) new TomcatWebServer(tomcat, getPort() >= 0) initialize() this.tomcat.start(); 启动 谁调用getWebServer去启动的Tomcat呢，我们从启动流程说一下。 SpringBootApplication.run 服务启动的主方法 调用的内部ConfigurableApplicationContext.run方法 public static ConfigurableApplicationContext run(Class primarySource, String... args){ return run(new Class[] { primarySource }, args); } public static ConfigurableApplicationContext run(Class[] primarySources, String[] args) { return new SpringApplication(primarySources).run(args); } 刷新上下文 refreshContext(context); refresh(context); protected void refresh(ApplicationContext applicationContext) { Assert.isInstanceOf(AbstractApplicationContext.class, applicationContext); ((AbstractApplicationContext) applicationContext).refresh(); } onRefresh();//这里的调用选择 ServletWebServerApplicationContext 实现 创建WebServer createWebServer(); WebServer webServer = this.webServer; ServletContext servletContext = getServletContext(); if (webServer == null && servletContext == null) { ServletWebServerFactory factory = getWebServerFactory(); this.webServer = factory.getWebServer(getSelfInitializer()); } getWebServer() 选择 TomcatServletWebServerFactory 实现的接口 创建Tomcat Web容器。 这里就和上面我们讲的联系起来了，下面就是Tomcat的创建过程。 getWebServer这个方法创建了Tomcat对象，并且做了两件重要的事情：把Connector对象添加到Tomcat中，configureEngine(tomcat.getEngine()); getWebServer方法返回的是TomcatWebServer。 Tomcat中最顶层的容器是Server，代表着整个服务器，从上图中可以看出，一个Server可以包含至少一个Service，用于具体提供服务。 Service主要包含两个部分：Connector和Container。从上图中可以看出 Tomcat 的心脏就是这两个组件，他们的作用如下： 1、Connector用于处理连接相关的事情，并提供Socket与Request和Response相关的转化; 2、Container用于封装和管理Servlet，以及具体处理Request请求； 一个Tomcat中只有一个Server，一个Server可以包含多个Service，一个Service只有一个Container，但是可以有多个Connectors，这是因为一个服务可以有多个连接，如同时提供Http和Https链接，也可以提供向相同协议不同端口的连接,示意图如下（Engine、Host、Context下边会说到）： 多个 Connector 和一个 Container 就形成了一个 Service，有了 Service 就可以对外提供服务了，但是 Service 还要一个生存的环境，必须要有人能够给她生命、掌握其生死大权，那就非 Server 莫属了！所以整个 Tomcat 的生命周期由 Server 控制。 spring boot 是如何启动 tomcat - login123456 - 博客园 Tomcat在SpringBoot中是如何启动的_Java_清风一阵吹我心-CSDN博客 SpringBoot内置tomcat启动原理 - 歪头儿在帝都 - 博客园 四张图带你了解Tomcat系统架构_Java_阿雨的博客-CSDN博客 如何确定Spring Boot默认加载的是Tomcat容器 其实在上面的getWebServer() 选择 TomcatServletWebServerFactory 实现的接口这里有疑问，为什么这里选择用Tomcat的实现，不选择其他的呢？虽然我们都知道Spring Boot的默认Web容器是Tomcat，但是如何确认呢。 这里就要结合自动配置这里的东西里。既然能自动引入Web配置在启动时选择Tomcat作为自己的默认启动容器，必然是和依赖的配置有关，我们找到了引入的Maven里spring-boot-starter-parent里的spring-boot-starter依赖，然后发现我们的Maven依赖里有这样的依赖引入。 这必然就是spring-boot-starter引进来的，在启动时自动配置读到后知道了自己要选择Tomcat容器。 Spring Boot替换默认容器Tomcat_Java_实践求真知-CSDN博客 Spring Boot是如何自配置的，启动类相关的 总结： 加载META-INF/spring-autoconfigure-metadata.properties文件 获取注解的属性及其值（PS：注解指的是@EnableAutoConfiguration注解） 从classpath中搜索所有META-INF/spring.factories配置文件然后，将其中org.springframework.boot.autoconfigure.EnableAutoConfiguration key对应的配置项加载到spring容器。只有spring.boot.enableautoconfiguration为true（默认为true）的时候，才启用自动配置，最终得到EnableAutoConfiguration的配置值，并将其封装到一个List中返回。 对上一步返回的List中的元素去重、排序。 依据第2步中获取的属性值排除一些特定的类，排除方式有2中，一是根据class来排除（exclude），二是根据class name（excludeName）来排除。 对上一步中所得到的List进行过滤，过滤的依据是条件匹配。这里用到的过滤器是org.springframework.boot.autoconfigure.condition.OnClassCondition最终返回的是一个ConditionOutcome[]数组。（PS：很多类都是依赖于其它的类的，当有某个类时才会装配，所以这次过滤的就是根据是否有某个class进而决定是否装配的。这些类所依赖的类都写在META-INF/spring-autoconfigure-metadata.properties文件里）。 关键点： ImportSelector 该接口的方法的返回值都会被纳入到spring容器管理中 SpringFactoriesLoader 该类可以从classpath中搜索所有META-INF/spring.factories配置文件，并读取配置。同理，其实Spring框架本身也提供了几个名字为@Enable开头的Annotation定义。比如@EnableScheduling、@EnableCaching、@EnableMBeanExport等，@EnableAutoConfiguration的理念和这些注解其实是一脉相承的。 观察@EnableAutoConfiguration可以发现，这里Import了@EnableAutoConfigurationImportSelector，这就是Spring Boot自动化配置的“始作俑者”。 第三章：SpringBoot自动装配原理解析 SpringBoot自动装配原理分析_Java_Dongguabai的博客-CSDN博客 深入理解SpringBoot之自动装配 - 聂晨 - 博客园 SpringBoot学习之自动装配 - 聂晨 - 博客园 从SpringBoot源码到自己封装一个Starter - 掘金 "},"docs/Guide/Spring+IOC+容器源码分析.html":{"url":"docs/Guide/Spring+IOC+容器源码分析.html","title":"Spring+IOC+容器源码分析","keywords":"","body":"Spring 最重要的概念是 IOC 和 AOP，本篇文章其实就是要带领大家来分析下 Spring 的 IOC 容器。既然大家平时都要用到 Spring，怎么可以不好好了解 Spring 呢？阅读本文并不能让你成为 Spring 专家，不过一定有助于大家理解 Spring 的很多概念，帮助大家排查应用中和 Spring 相关的一些问题。 本文采用的源码版本是 4.3.11.RELEASE，算是 5.0.x 前比较新的版本了。为了降低难度，本文所说的所有的内容都是基于 xml 的配置的方式，实际使用已经很少人这么做了，至少不是纯 xml 配置，不过从理解源码的角度来看用这种方式来说无疑是最合适的。 阅读建议：读者至少需要知道怎么配置 Spring，了解 Spring 中的各种概念，少部分内容我还假设读者使用过 SpringMVC。本文要说的 IOC 总体来说有两处地方最重要，一个是创建 Bean 容器，一个是初始化 Bean，如果读者觉得一次性看完本文压力有点大，那么可以按这个思路分两次消化。读者不一定对 Spring 容器的源码感兴趣，也许附录部分介绍的知识对读者有些许作用。 希望通过本文可以让读者不惧怕阅读 Spring 源码，也希望大家能反馈表述错误或不合理的地方。 引言 先看下最基本的启动 Spring 容器的例子： public static void main(String[] args) { ApplicationContext context = new ClassPathXmlApplicationContext(\"classpath:applicationfile.xml\"); } 以上代码就可以利用配置文件来启动一个 Spring 容器了，请使用 maven 的小伙伴直接在 dependencies 中加上以下依赖即可，个人比较反对那些不知道要添加什么依赖，然后把 Spring 的所有相关的东西都加进来的方式。 org.springframework spring-context 4.3.11.RELEASE spring-context 会自动将 spring-core、spring-beans、spring-aop、spring-expression 这几个基础 jar 包带进来。 多说一句，很多开发者入门就直接接触的 SpringMVC，对 Spring 其实不是很了解，Spring 是渐进式的工具，并不具有很强的侵入性，它的模块也划分得很合理，即使你的应用不是 web 应用，或者之前完全没有使用到 Spring，而你就想用 Spring 的依赖注入这个功能，其实完全是可以的，它的引入不会对其他的组件产生冲突。 废话说完，我们继续。ApplicationContext context = new ClassPathXmlApplicationContext(...) 其实很好理解，从名字上就可以猜出一二，就是在 ClassPath 中寻找 xml 配置文件，根据 xml 文件内容来构建 ApplicationContext。当然，除了 ClassPathXmlApplicationContext 以外，我们也还有其他构建 ApplicationContext 的方案可供选择，我们先来看看大体的继承结构是怎么样的： 读者可以大致看一下类名，源码分析的时候不至于找不着看哪个类，因为 Spring 为了适应各种使用场景，提供的各个接口都可能有很多的实现类。对于我们来说，就是揪着一个完整的分支看完。 当然，读本文的时候读者也不必太担心，每个代码块分析的时候，我都会告诉读者我们在说哪个类第几行。 我们可以看到，ClassPathXmlApplicationContext 兜兜转转了好久才到 ApplicationContext 接口，同样的，我们也可以使用绿颜色的 FileSystemXmlApplicationContext 和 AnnotationConfigApplicationContext 这两个类。 1、FileSystemXmlApplicationContext 的构造函数需要一个 xml 配置文件在系统中的路径，其他和 ClassPathXmlApplicationContext 基本上一样。 2、AnnotationConfigApplicationContext 是基于注解来使用的，它不需要配置文件，采用 java 配置类和各种注解来配置，是比较简单的方式，也是大势所趋吧。 不过本文旨在帮助大家理解整个构建流程，所以决定使用 ClassPathXmlApplicationContext 进行分析。 我们先来一个简单的例子来看看怎么实例化 ApplicationContext。 首先，定义一个接口： public interface MessageService { String getMessage(); } 定义接口实现类： public class MessageServiceImpl implements MessageService { public String getMessage() { return \"hello world\"; } } 接下来，我们在 resources 目录新建一个配置文件，文件名随意，通常叫 application.xml 或 application-xxx.xml 就可以了： 这样，我们就可以跑起来了： public class App { public static void main(String[] args) { // 用我们的配置文件来启动一个 ApplicationContext ApplicationContext context = new ClassPathXmlApplicationContext(\"classpath:application.xml\"); System.out.println(\"context 启动成功\"); // 从 context 中取出我们的 Bean，而不是用 new MessageServiceImpl() 这种方式 MessageService messageService = context.getBean(MessageService.class); // 这句将输出: hello world System.out.println(messageService.getMessage()); } } 以上例子很简单，不过也够引出本文的主题了，就是怎么样通过配置文件来启动 Spring 的 ApplicationContext ？也就是我们今天要分析的 IOC 的核心了。ApplicationContext 启动过程中，会负责创建实例 Bean，往各个 Bean 中注入依赖等。 BeanFactory 简介 BeanFactory，从名字上也很好理解，生产 bean 的工厂，它负责生产和管理各个 bean 实例。 初学者可别以为我之前说那么多和 BeanFactory 无关，前面说的 ApplicationContext 其实就是一个 BeanFactory。我们来看下和 BeanFactory 接口相关的主要的继承结构： 我想，大家看完这个图以后，可能就不是很开心了。ApplicationContext 往下的继承结构前面一张图说过了，这里就不重复了。这张图呢，背下来肯定是不需要的，有几个重点和大家说明下就好。 ApplicationContext 继承了 ListableBeanFactory，这个 Listable 的意思就是，通过这个接口，我们可以获取多个 Bean，大家看源码会发现，最顶层 BeanFactory 接口的方法都是获取单个 Bean 的。 ApplicationContext 继承了 HierarchicalBeanFactory，Hierarchical 单词本身已经能说明问题了，也就是说我们可以在应用中起多个 BeanFactory，然后可以将各个 BeanFactory 设置为父子关系。 AutowireCapableBeanFactory 这个名字中的 Autowire 大家都非常熟悉，它就是用来自动装配 Bean 用的，但是仔细看上图，ApplicationContext 并没有继承它，不过不用担心，不使用继承，不代表不可以使用组合，如果你看到 ApplicationContext 接口定义中的最后一个方法 getAutowireCapableBeanFactory() 就知道了。 ConfigurableListableBeanFactory 也是一个特殊的接口，看图，特殊之处在于它继承了第二层所有的三个接口，而 ApplicationContext 没有。这点之后会用到。 请先不用花时间在其他的接口和类上，先理解我说的这几点就可以了。 然后，请读者打开编辑器，翻一下 BeanFactory、ListableBeanFactory、HierarchicalBeanFactory、AutowireCapableBeanFactory、ApplicationContext 这几个接口的代码，大概看一下各个接口中的方法，大家心里要有底，限于篇幅，我就不贴代码介绍了。 启动过程分析 下面将会是冗长的代码分析，记住，一定要自己打开源码来看，不然纯看是很累的。 第一步，我们肯定要从 ClassPathXmlApplicationContext 的构造方法说起。 public class ClassPathXmlApplicationContext extends AbstractXmlApplicationContext { private Resource[] configResources; // 如果已经有 ApplicationContext 并需要配置成父子关系，那么调用这个构造方法 public ClassPathXmlApplicationContext(ApplicationContext parent) { super(parent); } ... public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException { super(parent); // 根据提供的路径，处理成配置文件数组(以分号、逗号、空格、tab、换行符分割) setConfigLocations(configLocations); if (refresh) { refresh(); // 核心方法 } } ... } 接下来，就是 refresh()，这里简单说下为什么是 refresh()，而不是 init() 这种名字的方法。因为 ApplicationContext 建立起来以后，其实我们是可以通过调用 refresh() 这个方法重建的，refresh() 会将原来的 ApplicationContext 销毁，然后再重新执行一次初始化操作。 往下看，refresh() 方法里面调用了那么多方法，就知道肯定不简单了，请读者先看个大概，细节之后会详细说。 @Override public void refresh() throws BeansException, IllegalStateException { // 来个锁，不然 refresh() 还没结束，你又来个启动或销毁容器的操作，那不就乱套了嘛 synchronized (this.startupShutdownMonitor) { // 准备工作，记录下容器的启动时间、标记“已启动”状态、处理配置文件中的占位符 prepareRefresh(); // 这步比较关键，这步完成后，配置文件就会解析成一个个 Bean 定义，注册到 BeanFactory 中， // 当然，这里说的 Bean 还没有初始化，只是配置信息都提取出来了， // 注册也只是将这些信息都保存到了注册中心(说到底核心是一个 beanName-> beanDefinition 的 map) ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean // 这块待会会展开说 prepareBeanFactory(beanFactory); try { // 【这里需要知道 BeanFactoryPostProcessor 这个知识点，Bean 如果实现了此接口， // 那么在容器初始化以后，Spring 会负责调用里面的 postProcessBeanFactory 方法。】 // 这里是提供给子类的扩展点，到这里的时候，所有的 Bean 都加载、注册完成了，但是都还没有初始化 // 具体的子类可以在这步的时候添加一些特殊的 BeanFactoryPostProcessor 的实现类或做点什么事 postProcessBeanFactory(beanFactory); // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 的实现类，注意看和 BeanFactoryPostProcessor 的区别 // 此接口两个方法: postProcessBeforeInitialization 和 postProcessAfterInitialization // 两个方法分别在 Bean 初始化之前和初始化之后得到执行。注意，到这里 Bean 还没初始化 registerBeanPostProcessors(beanFactory); // 初始化当前 ApplicationContext 的 MessageSource，国际化这里就不展开说了，不然没完没了了 initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器，这里也不展开了 initApplicationEventMulticaster(); // 从方法名就可以知道，典型的模板方法(钩子方法)， // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前） onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口。这也不是我们的重点，过 registerListeners(); // 重点，重点，重点 // 初始化所有的 singleton beans //（lazy-init 的除外） finishBeanFactoryInitialization(beanFactory); // 最后，广播事件，ApplicationContext 初始化完成 finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); } // Destroy already created singletons to avoid dangling resources. // 销毁已经初始化的 singleton 的 Beans，以免有些 bean 会一直占用资源 destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // 把异常往外抛 throw ex; } finally { // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); } } } 下面，我们开始一步步来肢解这个 refresh() 方法。 创建 Bean 容器前的准备工作 这个比较简单，直接看代码中的几个注释即可。 protected void prepareRefresh() { // 记录启动时间， // 将 active 属性设置为 true，closed 属性设置为 false，它们都是 AtomicBoolean 类型 this.startupDate = System.currentTimeMillis(); this.closed.set(false); this.active.set(true); if (logger.isInfoEnabled()) { logger.info(\"Refreshing \" + this); } // Initialize any placeholder property sources in the context environment initPropertySources(); // 校验 xml 配置文件 getEnvironment().validateRequiredProperties(); this.earlyApplicationEvents = new LinkedHashSet(); } 创建 Bean 容器，加载并注册 Bean 我们回到 refresh() 方法中的下一行 obtainFreshBeanFactory()。 注意，这个方法是全文最重要的部分之一，这里将会初始化 BeanFactory、加载 Bean、注册 Bean 等等。 当然，这步结束后，Bean 并没有完成初始化。这里指的是 Bean 实例并未在这一步生成。 // AbstractApplicationContext.java protected ConfigurableListableBeanFactory obtainFreshBeanFactory() { // 关闭旧的 BeanFactory (如果有)，创建新的 BeanFactory，加载 Bean 定义、注册 Bean 等等 refreshBeanFactory(); // 返回刚刚创建的 BeanFactory ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (logger.isDebugEnabled()) { logger.debug(\"Bean factory for \" + getDisplayName() + \": \" + beanFactory); } return beanFactory; } // AbstractRefreshableApplicationContext.java 120 @Override protected final void refreshBeanFactory() throws BeansException { // 如果 ApplicationContext 中已经加载过 BeanFactory 了，销毁所有 Bean，关闭 BeanFactory // 注意，应用中 BeanFactory 本来就是可以多个的，这里可不是说应用全局是否有 BeanFactory，而是当前 // ApplicationContext 是否有 BeanFactory if (hasBeanFactory()) { destroyBeans(); closeBeanFactory(); } try { // 初始化一个 DefaultListableBeanFactory，为什么用这个，我们马上说。 DefaultListableBeanFactory beanFactory = createBeanFactory(); // 用于 BeanFactory 的序列化，我想不部分人应该都用不到 beanFactory.setSerializationId(getId()); // 下面这两个方法很重要，别跟丢了，具体细节之后说 // 设置 BeanFactory 的两个配置属性：是否允许 Bean 覆盖、是否允许循环引用 customizeBeanFactory(beanFactory); // 加载 Bean 到 BeanFactory 中 loadBeanDefinitions(beanFactory); synchronized (this.beanFactoryMonitor) { this.beanFactory = beanFactory; } } catch (IOException ex) { throw new ApplicationContextException(\"I/O error parsing bean definition source for \" + getDisplayName(), ex); } } 看到这里的时候，我觉得读者就应该站在高处看 ApplicationContext 了，ApplicationContext 继承自 BeanFactory，但是它不应该被理解为 BeanFactory 的实现类，而是说其内部持有一个实例化的 BeanFactory（DefaultListableBeanFactory）。以后所有的 BeanFactory 相关的操作其实是委托给这个实例来处理的。 我们说说为什么选择实例化 DefaultListableBeanFactory ？前面我们说了有个很重要的接口 ConfigurableListableBeanFactory，它实现了 BeanFactory 下面一层的所有三个接口，我把之前的继承图再拿过来大家再仔细看一下： 我们可以看到 ConfigurableListableBeanFactory 只有一个实现类 DefaultListableBeanFactory，而且实现类 DefaultListableBeanFactory 还通过实现右边的 AbstractAutowireCapableBeanFactory 通吃了右路。所以结论就是，最底下这个家伙 DefaultListableBeanFactory 基本上是最牛的 BeanFactory 了，这也是为什么这边会使用这个类来实例化的原因。 如果你想要在程序运行的时候动态往 Spring IOC 容器注册新的 bean，就会使用到这个类。那我们怎么在运行时获得这个实例呢？ 之前我们说过 ApplicationContext 接口能获取到 AutowireCapableBeanFactory，就是最右上角那个，然后它向下转型就能得到 DefaultListableBeanFactory 了。 那怎么拿到 ApplicationContext 实例呢？如果你不会，说明你没用过 Spring。 在继续往下之前，我们需要先了解 BeanDefinition。我们说 BeanFactory 是 Bean 容器，那么 Bean 又是什么呢？ 这里的 BeanDefinition 就是我们所说的 Spring 的 Bean，我们自己定义的各个 Bean 其实会转换成一个个 BeanDefinition 存在于 Spring 的 BeanFactory 中。 所以，如果有人问你 Bean 是什么的时候，你要知道 Bean 在代码层面上可以简单认为是 BeanDefinition 的实例。 BeanDefinition 中保存了我们的 Bean 信息，比如这个 Bean 指向的是哪个类、是否是单例的、是否懒加载、这个 Bean 依赖了哪些 Bean 等等。 BeanDefinition 接口定义 我们来看下 BeanDefinition 的接口定义： public interface BeanDefinition extends AttributeAccessor, BeanMetadataElement { // 我们可以看到，默认只提供 sington 和 prototype 两种， // 很多读者可能知道还有 request, session, globalSession, application, websocket 这几种， // 不过，它们属于基于 web 的扩展。 String SCOPE_SINGLETON = ConfigurableBeanFactory.SCOPE_SINGLETON; String SCOPE_PROTOTYPE = ConfigurableBeanFactory.SCOPE_PROTOTYPE; // 比较不重要，直接跳过吧 int ROLE_APPLICATION = 0; int ROLE_SUPPORT = 1; int ROLE_INFRASTRUCTURE = 2; // 设置父 Bean，这里涉及到 bean 继承，不是 java 继承。请参见附录的详细介绍 // 一句话就是：继承父 Bean 的配置信息而已 void setParentName(String parentName); // 获取父 Bean String getParentName(); // 设置 Bean 的类名称，将来是要通过反射来生成实例的 void setBeanClassName(String beanClassName); // 获取 Bean 的类名称 String getBeanClassName(); // 设置 bean 的 scope void setScope(String scope); String getScope(); // 设置是否懒加载 void setLazyInit(boolean lazyInit); boolean isLazyInit(); // 设置该 Bean 依赖的所有的 Bean，注意，这里的依赖不是指属性依赖(如 @Autowire 标记的)， // 是 depends-on=\"\" 属性设置的值。 void setDependsOn(String... dependsOn); // 返回该 Bean 的所有依赖 String[] getDependsOn(); // 设置该 Bean 是否可以注入到其他 Bean 中，只对根据类型注入有效， // 如果根据名称注入，即使这边设置了 false，也是可以的 void setAutowireCandidate(boolean autowireCandidate); // 该 Bean 是否可以注入到其他 Bean 中 boolean isAutowireCandidate(); // 主要的。同一接口的多个实现，如果不指定名字的话，Spring 会优先选择设置 primary 为 true 的 bean void setPrimary(boolean primary); // 是否是 primary 的 boolean isPrimary(); // 如果该 Bean 采用工厂方法生成，指定工厂名称。对工厂不熟悉的读者，请参加附录 // 一句话就是：有些实例不是用反射生成的，而是用工厂模式生成的 void setFactoryBeanName(String factoryBeanName); // 获取工厂名称 String getFactoryBeanName(); // 指定工厂类中的 工厂方法名称 void setFactoryMethodName(String factoryMethodName); // 获取工厂类中的 工厂方法名称 String getFactoryMethodName(); // 构造器参数 ConstructorArgumentValues getConstructorArgumentValues(); // Bean 中的属性值，后面给 bean 注入属性值的时候会说到 MutablePropertyValues getPropertyValues(); // 是否 singleton boolean isSingleton(); // 是否 prototype boolean isPrototype(); // 如果这个 Bean 是被设置为 abstract，那么不能实例化， // 常用于作为 父bean 用于继承，其实也很少用...... boolean isAbstract(); int getRole(); String getDescription(); String getResourceDescription(); BeanDefinition getOriginatingBeanDefinition(); } 这个 BeanDefinition 其实已经包含很多的信息了，暂时不清楚所有的方法对应什么东西没关系，希望看完本文后读者可以彻底搞清楚里面的所有东西。 这里接口虽然那么多，但是没有类似 getInstance() 这种方法来获取我们定义的类的实例，真正的我们定义的类生成的实例到哪里去了呢？别着急，这个要很后面才能讲到。 有了 BeanDefinition 的概念以后，我们再往下看 refreshBeanFactory() 方法中的剩余部分： customizeBeanFactory(beanFactory); loadBeanDefinitions(beanFactory); 虽然只有两个方法，但路还很长啊。。。 customizeBeanFactory customizeBeanFactory(beanFactory) 比较简单，就是配置是否允许 BeanDefinition 覆盖、是否允许循环引用。 protected void customizeBeanFactory(DefaultListableBeanFactory beanFactory) { if (this.allowBeanDefinitionOverriding != null) { // 是否允许 Bean 定义覆盖 beanFactory.setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding); } if (this.allowCircularReferences != null) { // 是否允许 Bean 间的循环依赖 beanFactory.setAllowCircularReferences(this.allowCircularReferences); } } BeanDefinition 的覆盖问题可能会有开发者碰到这个坑，就是在配置文件中定义 bean 时使用了相同的 id 或 name，默认情况下，allowBeanDefinitionOverriding 属性为 null，如果在同一配置文件中重复了，会抛错，但是如果不是同一配置文件中，会发生覆盖。 循环引用也很好理解：A 依赖 B，而 B 依赖 A。或 A 依赖 B，B 依赖 C，而 C 依赖 A。 默认情况下，Spring 允许循环依赖，当然如果你在 A 的构造方法中依赖 B，在 B 的构造方法中依赖 A 是不行的。 至于这两个属性怎么配置？我在附录中进行了介绍，尤其对于覆盖问题，很多人都希望禁止出现 Bean 覆盖，可是 Spring 默认是不同文件的时候可以覆盖的。 之后的源码中还会出现这两个属性，读者有个印象就可以了，它们不是非常重要。 加载 Bean: loadBeanDefinitions 接下来是最重要的 loadBeanDefinitions(beanFactory) 方法了，这个方法将根据配置，加载各个 Bean，然后放到 BeanFactory 中。 读取配置的操作在 XmlBeanDefinitionReader 中，其负责加载配置、解析。 // AbstractXmlApplicationContext.java 80 /** 我们可以看到，此方法将通过一个 XmlBeanDefinitionReader 实例来加载各个 Bean。*/ @Override protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException { // 给这个 BeanFactory 实例化一个 XmlBeanDefinitionReader XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); // Configure the bean definition reader with this context's // resource loading environment. beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // 初始化 BeanDefinitionReader，其实这个是提供给子类覆写的， // 我看了一下，没有类覆写这个方法，我们姑且当做不重要吧 initBeanDefinitionReader(beanDefinitionReader); // 重点来了，继续往下 loadBeanDefinitions(beanDefinitionReader); } 现在还在这个类中，接下来用刚刚初始化的 Reader 开始来加载 xml 配置，这块代码读者可以选择性跳过，不是很重要。也就是说，下面这个代码块，读者可以很轻松地略过。 // AbstractXmlApplicationContext.java 120 protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException { Resource[] configResources = getConfigResources(); if (configResources != null) { // 往下看 reader.loadBeanDefinitions(configResources); } String[] configLocations = getConfigLocations(); if (configLocations != null) { // 2 reader.loadBeanDefinitions(configLocations); } } // 上面虽然有两个分支，不过第二个分支很快通过解析路径转换为 Resource 以后也会进到这里 @Override public int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException { Assert.notNull(resources, \"Resource array must not be null\"); int counter = 0; // 注意这里是个 for 循环，也就是每个文件是一个 resource for (Resource resource : resources) { // 继续往下看 counter += loadBeanDefinitions(resource); } // 最后返回 counter，表示总共加载了多少的 BeanDefinition return counter; } // XmlBeanDefinitionReader 303 @Override public int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException { return loadBeanDefinitions(new EncodedResource(resource)); } // XmlBeanDefinitionReader 314 public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException { Assert.notNull(encodedResource, \"EncodedResource must not be null\"); if (logger.isInfoEnabled()) { logger.info(\"Loading XML bean definitions from \" + encodedResource.getResource()); } // 用一个 ThreadLocal 来存放配置文件资源 Set currentResources = this.resourcesCurrentlyBeingLoaded.get(); if (currentResources == null) { currentResources = new HashSet(4); this.resourcesCurrentlyBeingLoaded.set(currentResources); } if (!currentResources.add(encodedResource)) { throw new BeanDefinitionStoreException( \"Detected cyclic loading of \" + encodedResource + \" - check your import definitions!\"); } try { InputStream inputStream = encodedResource.getResource().getInputStream(); try { InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) { inputSource.setEncoding(encodedResource.getEncoding()); } // 核心部分是这里，往下面看 return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); } finally { inputStream.close(); } } catch (IOException ex) { throw new BeanDefinitionStoreException( \"IOException parsing XML document from \" + encodedResource.getResource(), ex); } finally { currentResources.remove(encodedResource); if (currentResources.isEmpty()) { this.resourcesCurrentlyBeingLoaded.remove(); } } } // 还在这个文件中，第 388 行 protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException { try { // 这里就不看了，将 xml 文件转换为 Document 对象 Document doc = doLoadDocument(inputSource, resource); // 继续 return registerBeanDefinitions(doc, resource); } catch (... } // 还在这个文件中，第 505 行 // 返回值：返回从当前配置文件加载了多少数量的 Bean public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException { BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); int countBefore = getRegistry().getBeanDefinitionCount(); // 这里 documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); return getRegistry().getBeanDefinitionCount() - countBefore; } // DefaultBeanDefinitionDocumentReader 90 @Override public void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) { this.readerContext = readerContext; logger.debug(\"Loading bean definitions\"); Element root = doc.getDocumentElement(); // 从 xml 根节点开始解析文件 doRegisterBeanDefinitions(root); } 经过漫长的链路，一个配置文件终于转换为一颗 DOM 树了，注意，这里指的是其中一个配置文件，不是所有的，读者可以看到上面有个 for 循环的。下面开始从根节点开始解析： doRegisterBeanDefinitions： // DefaultBeanDefinitionDocumentReader 116 protected void doRegisterBeanDefinitions(Element root) { // 我们看名字就知道，BeanDefinitionParserDelegate 必定是一个重要的类，它负责解析 Bean 定义， // 这里为什么要定义一个 parent? 看到后面就知道了，是递归问题， // 因为 内部是可以定义 的，所以这个方法的 root 其实不一定就是 xml 的根节点，也可以是嵌套在里面的 节点，从源码分析的角度，我们当做根节点就好了 BeanDefinitionParserDelegate parent = this.delegate; this.delegate = createDelegate(getReaderContext(), root, parent); if (this.delegate.isDefaultNamespace(root)) { // 这块说的是根节点 中的 profile 是否是当前环境需要的， // 如果当前环境配置的 profile 不包含此 profile，那就直接 return 了，不对此 解析 // 不熟悉 profile 为何物，不熟悉怎么配置 profile 读者的请移步附录区 String profileSpec = root.getAttribute(PROFILE_ATTRIBUTE); if (StringUtils.hasText(profileSpec)) { String[] specifiedProfiles = StringUtils.tokenizeToStringArray( profileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS); if (!getReaderContext().getEnvironment().acceptsProfiles(specifiedProfiles)) { if (logger.isInfoEnabled()) { logger.info(\"Skipped XML bean definition file due to specified profiles [\" + profileSpec + \"] not matching: \" + getReaderContext().getResource()); } return; } } } preProcessXml(root); // 钩子 // 往下看 parseBeanDefinitions(root, this.delegate); postProcessXml(root); // 钩子 this.delegate = parent; } preProcessXml(root) 和 postProcessXml(root) 是给子类用的钩子方法，鉴于没有被使用到，也不是我们的重点，我们直接跳过。 这里涉及到了 profile 的问题，对于不了解的读者，我在附录中对 profile 做了简单的解释，读者可以参考一下。 接下来，看核心解析方法 parseBeanDefinitions(root, this.delegate) : // default namespace 涉及到的就四个标签 、、 和 ， // 其他的属于 custom 的 protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) { if (delegate.isDefaultNamespace(root)) { NodeList nl = root.getChildNodes(); for (int i = 0; i 从上面的代码，我们可以看到，对于每个配置来说，分别进入到 parseDefaultElement(ele, delegate); 和 delegate.parseCustomElement(ele); 这两个分支了。 parseDefaultElement(ele, delegate) 代表解析的节点是 、、、 这几个。 这里的四个标签之所以是 default 的，是因为它们是处于这个 namespace 下定义的： http://www.springframework.org/schema/beans 又到初学者科普时间，不熟悉 namespace 的读者请看下面贴出来的 xml，这里的第二行 xmlns 就是咯。 而对于其他的标签，将进入到 delegate.parseCustomElement(element) 这个分支。如我们经常会使用到的 、、、等。 这些属于扩展，如果需要使用上面这些 ”非 default“ 标签，那么上面的 xml 头部的地方也要引入相应的 namespace 和 .xsd 文件的路径，如下所示。同时代码中需要提供相应的 parser 来解析，如 MvcNamespaceHandler、TaskNamespaceHandler、ContextNamespaceHandler、AopNamespaceHandler 等。 假如读者想分析 的实现原理，就应该到 ContextNamespaceHandler 中找答案。 同理，以后你要是碰到 这种标签，那么就应该搜一搜是不是有 DubboNamespaceHandler 这个处理类。 回过神来，看看处理 default 标签的方法： private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) { if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) { // 处理 标签 importBeanDefinitionResource(ele); } else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) { // 处理 标签定义 // processAliasRegistration(ele); } else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) { // 处理 标签定义，这也算是我们的重点吧 processBeanDefinition(ele, delegate); } else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) { // 如果碰到的是嵌套的 标签，需要递归 doRegisterBeanDefinitions(ele); } } 如果每个标签都说，那我不吐血，你们都要吐血了。我们挑我们的重点 标签出来说。 processBeanDefinition 解析 bean 标签 下面是 processBeanDefinition 解析 标签： // DefaultBeanDefinitionDocumentReader 298 protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) { // 将 节点中的信息提取出来，然后封装到一个 BeanDefinitionHolder 中，细节往下看 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); // 下面的几行先不要看，跳过先，跳过先，跳过先，后面会继续说的 if (bdHolder != null) { bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try { // Register the final decorated instance. BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); } catch (BeanDefinitionStoreException ex) { getReaderContext().error(\"Failed to register bean definition with name '\" + bdHolder.getBeanName() + \"'\", ele, ex); } // Send registration event. getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); } } 继续往下看怎么解析之前，我们先看下 标签中可以定义哪些属性： Property class 类的全限定名 name 可指定 id、name(用逗号、分号、空格分隔) scope 作用域 constructor arguments 指定构造参数 properties 设置属性的值 autowiring mode no(默认值)、byName、byType、 constructor lazy-initialization mode 是否懒加载(如果被非懒加载的bean依赖了那么其实也就不能懒加载了) initialization method bean 属性设置完成后，会调用这个方法 destruction method bean 销毁后的回调方法 上面表格中的内容我想大家都非常熟悉吧，如果不熟悉，那就是你不够了解 Spring 的配置了。 简单地说就是像下面这样子： 当然，除了上面举例出来的这些，还有 factory-bean、factory-method、、、、 这几个，大家是不是熟悉呢？自己检验一下自己对 Spring 中 bean 的了解程度。 有了以上这些知识以后，我们再继续往里看怎么解析 bean 元素，是怎么转换到 BeanDefinitionHolder 的。 // BeanDefinitionParserDelegate 428 public BeanDefinitionHolder parseBeanDefinitionElement(Element ele) { return parseBeanDefinitionElement(ele, null); } public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, BeanDefinition containingBean) { String id = ele.getAttribute(ID_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); List aliases = new ArrayList(); // 将 name 属性的定义按照 “逗号、分号、空格” 切分，形成一个 别名列表数组， // 当然，如果你不定义 name 属性的话，就是空的了 // 我在附录中简单介绍了一下 id 和 name 的配置，大家可以看一眼，有个20秒就可以了 if (StringUtils.hasLength(nameAttr)) { String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS); aliases.addAll(Arrays.asList(nameArr)); } String beanName = id; // 如果没有指定id, 那么用别名列表的第一个名字作为beanName if (!StringUtils.hasText(beanName) && !aliases.isEmpty()) { beanName = aliases.remove(0); if (logger.isDebugEnabled()) { logger.debug(\"No XML 'id' specified - using '\" + beanName + \"' as bean name and \" + aliases + \" as aliases\"); } } if (containingBean == null) { checkNameUniqueness(beanName, aliases, ele); } // 根据 ... 中的配置创建 BeanDefinition，然后把配置中的信息都设置到实例中, // 细节后面细说，先知道下面这行结束后，一个 BeanDefinition 实例就出来了。 AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); // 到这里，整个 标签就算解析结束了，一个 BeanDefinition 就形成了。 if (beanDefinition != null) { // 如果都没有设置 id 和 name，那么此时的 beanName 就会为 null，进入下面这块代码产生 // 如果读者不感兴趣的话，我觉得不需要关心这块代码，对本文源码分析来说，这些东西不重要 if (!StringUtils.hasText(beanName)) { try { if (containingBean != null) {// 按照我们的思路，这里 containingBean 是 null 的 beanName = BeanDefinitionReaderUtils.generateBeanName( beanDefinition, this.readerContext.getRegistry(), true); } else { // 如果我们不定义 id 和 name，那么我们引言里的那个例子： // 1. beanName 为：com.javadoop.example.MessageServiceImpl#0 // 2. beanClassName 为：com.javadoop.example.MessageServiceImpl beanName = this.readerContext.generateBeanName(beanDefinition); String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null && beanName.startsWith(beanClassName) && beanName.length() > beanClassName.length() && !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) { // 把 beanClassName 设置为 Bean 的别名 aliases.add(beanClassName); } } if (logger.isDebugEnabled()) { logger.debug(\"Neither XML 'id' nor 'name' specified - \" + \"using generated bean name [\" + beanName + \"]\"); } } catch (Exception ex) { error(ex.getMessage(), ele); return null; } } String[] aliasesArray = StringUtils.toStringArray(aliases); // 返回 BeanDefinitionHolder return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); } return null; } 然后，我们再看看怎么根据配置创建 BeanDefinition 实例的： public AbstractBeanDefinition parseBeanDefinitionElement( Element ele, String beanName, BeanDefinition containingBean) { this.parseState.push(new BeanEntry(beanName)); String className = null; if (ele.hasAttribute(CLASS_ATTRIBUTE)) { className = ele.getAttribute(CLASS_ATTRIBUTE).trim(); } try { String parent = null; if (ele.hasAttribute(PARENT_ATTRIBUTE)) { parent = ele.getAttribute(PARENT_ATTRIBUTE); } // 创建 BeanDefinition，然后设置类信息而已，很简单，就不贴代码了 AbstractBeanDefinition bd = createBeanDefinition(className, parent); // 设置 BeanDefinition 的一堆属性，这些属性定义在 AbstractBeanDefinition 中 parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT)); /** * 下面的一堆是解析 ...... 内部的子元素， * 解析出来以后的信息都放到 bd 的属性中 */ // 解析 parseMetaElements(ele, bd); // 解析 parseLookupOverrideSubElements(ele, bd.getMethodOverrides()); // 解析 parseReplacedMethodSubElements(ele, bd.getMethodOverrides()); // 解析 parseConstructorArgElements(ele, bd); // 解析 parsePropertyElements(ele, bd); // 解析 parseQualifierElements(ele, bd); bd.setResource(this.readerContext.getResource()); bd.setSource(extractSource(ele)); return bd; } catch (ClassNotFoundException ex) { error(\"Bean class [\" + className + \"] not found\", ele, ex); } catch (NoClassDefFoundError err) { error(\"Class that bean class [\" + className + \"] depends on not found\", ele, err); } catch (Throwable ex) { error(\"Unexpected failure during bean definition parsing\", ele, ex); } finally { this.parseState.pop(); } return null; } 到这里，我们已经完成了根据 配置创建了一个 BeanDefinitionHolder 实例。注意，是一个。 我们回到解析 的入口方法: protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) { // 将 节点转换为 BeanDefinitionHolder，就是上面说的一堆 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) { // 如果有自定义属性的话，进行相应的解析，先忽略 bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try { // 我们把这步叫做 注册Bean 吧 BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); } catch (BeanDefinitionStoreException ex) { getReaderContext().error(\"Failed to register bean definition with name '\" + bdHolder.getBeanName() + \"'\", ele, ex); } // 注册完成后，发送事件，本文不展开说这个 getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); } } 大家再仔细看一下这块吧，我们后面就不回来说这个了。这里已经根据一个 标签产生了一个 BeanDefinitionHolder 的实例，这个实例里面也就是一个 BeanDefinition 的实例和它的 beanName、aliases 这三个信息，注意，我们的关注点始终在 BeanDefinition 上： public class BeanDefinitionHolder implements BeanMetadataElement { private final BeanDefinition beanDefinition; private final String beanName; private final String[] aliases; ... 然后我们准备注册这个 BeanDefinition，最后，把这个注册事件发送出去。 下面，我们开始说说注册 Bean 吧。 注册 Bean // BeanDefinitionReaderUtils 143 public static void registerBeanDefinition( BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) throws BeanDefinitionStoreException { String beanName = definitionHolder.getBeanName(); // 注册这个 Bean registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition()); // 如果还有别名的话，也要根据别名全部注册一遍，不然根据别名就会找不到 Bean 了 String[] aliases = definitionHolder.getAliases(); if (aliases != null) { for (String alias : aliases) { // alias -> beanName 保存它们的别名信息，这个很简单，用一个 map 保存一下就可以了， // 获取的时候，会先将 alias 转换为 beanName，然后再查找 registry.registerAlias(beanName, alias); } } } 别名注册的放一边，毕竟它很简单，我们看看怎么注册 Bean。 // DefaultListableBeanFactory 793 @Override public void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException { Assert.hasText(beanName, \"Bean name must not be empty\"); Assert.notNull(beanDefinition, \"BeanDefinition must not be null\"); if (beanDefinition instanceof AbstractBeanDefinition) { try { ((AbstractBeanDefinition) beanDefinition).validate(); } catch (BeanDefinitionValidationException ex) { throw new BeanDefinitionStoreException(...); } } // old? 还记得 “允许 bean 覆盖” 这个配置吗？allowBeanDefinitionOverriding BeanDefinition oldBeanDefinition; // 之后会看到，所有的 Bean 注册后会放入这个 beanDefinitionMap 中 oldBeanDefinition = this.beanDefinitionMap.get(beanName); // 处理重复名称的 Bean 定义的情况 if (oldBeanDefinition != null) { if (!isAllowBeanDefinitionOverriding()) { // 如果不允许覆盖的话，抛异常 throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription()... } else if (oldBeanDefinition.getRole() updatedDefinitions = new ArrayList(this.beanDefinitionNames.size() + 1); updatedDefinitions.addAll(this.beanDefinitionNames); updatedDefinitions.add(beanName); this.beanDefinitionNames = updatedDefinitions; if (this.manualSingletonNames.contains(beanName)) { Set updatedSingletons = new LinkedHashSet(this.manualSingletonNames); updatedSingletons.remove(beanName); this.manualSingletonNames = updatedSingletons; } } } else { // 最正常的应该是进到这个分支。 // 将 BeanDefinition 放到这个 map 中，这个 map 保存了所有的 BeanDefinition this.beanDefinitionMap.put(beanName, beanDefinition); // 这是个 ArrayList，所以会按照 bean 配置的顺序保存每一个注册的 Bean 的名字 this.beanDefinitionNames.add(beanName); // 这是个 LinkedHashSet，代表的是手动注册的 singleton bean， // 注意这里是 remove 方法，到这里的 Bean 当然不是手动注册的 // 手动指的是通过调用以下方法注册的 bean ： // registerSingleton(String beanName, Object singletonObject) // 这不是重点，解释只是为了不让大家疑惑。Spring 会在后面\"手动\"注册一些 Bean， // 如 \"environment\"、\"systemProperties\" 等 bean，我们自己也可以在运行时注册 Bean 到容器中的 this.manualSingletonNames.remove(beanName); } // 这个不重要，在预初始化的时候会用到，不必管它。 this.frozenBeanDefinitionNames = null; } if (oldBeanDefinition != null || containsSingleton(beanName)) { resetBeanDefinition(beanName); } } 总结一下，到这里已经初始化了 Bean 容器， 配置也相应的转换为了一个个 BeanDefinition，然后注册了各个 BeanDefinition 到注册中心，并且发送了注册事件。 --------- 分割线 --------- 到这里是一个分水岭，前面的内容都还算比较简单，不过应该也比较繁琐，大家要清楚地知道前面都做了哪些事情。 Bean 容器实例化完成后 说到这里，我们回到 refresh() 方法，我重新贴了一遍代码，看看我们说到哪了。是的，我们才说完 obtainFreshBeanFactory() 方法。 考虑到篇幅，这里开始大幅缩减掉没必要详细介绍的部分，大家直接看下面的代码中的注释就好了。 @Override public void refresh() throws BeansException, IllegalStateException { // 来个锁，不然 refresh() 还没结束，你又来个启动或销毁容器的操作，那不就乱套了嘛 synchronized (this.startupShutdownMonitor) { // 准备工作，记录下容器的启动时间、标记“已启动”状态、处理配置文件中的占位符 prepareRefresh(); // 这步比较关键，这步完成后，配置文件就会解析成一个个 Bean 定义，注册到 BeanFactory 中， // 当然，这里说的 Bean 还没有初始化，只是配置信息都提取出来了， // 注册也只是将这些信息都保存到了注册中心(说到底核心是一个 beanName-> beanDefinition 的 map) ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean // 这块待会会展开说 prepareBeanFactory(beanFactory); try { // 【这里需要知道 BeanFactoryPostProcessor 这个知识点，Bean 如果实现了此接口， // 那么在容器初始化以后，Spring 会负责调用里面的 postProcessBeanFactory 方法。】 // 这里是提供给子类的扩展点，到这里的时候，所有的 Bean 都加载、注册完成了，但是都还没有初始化 // 具体的子类可以在这步的时候添加一些特殊的 BeanFactoryPostProcessor 的实现类或做点什么事 postProcessBeanFactory(beanFactory); // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 回调方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 的实现类，注意看和 BeanFactoryPostProcessor 的区别 // 此接口两个方法: postProcessBeforeInitialization 和 postProcessAfterInitialization // 两个方法分别在 Bean 初始化之前和初始化之后得到执行。这里仅仅是注册，之后会看到回调这两方法的时机 registerBeanPostProcessors(beanFactory); // 初始化当前 ApplicationContext 的 MessageSource，国际化这里就不展开说了，不然没完没了了 initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器，这里也不展开了 initApplicationEventMulticaster(); // 从方法名就可以知道，典型的模板方法(钩子方法)，不展开说 // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前） onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口。这也不是我们的重点，过 registerListeners(); // 重点，重点，重点 // 初始化所有的 singleton beans //（lazy-init 的除外） finishBeanFactoryInitialization(beanFactory); // 最后，广播事件，ApplicationContext 初始化完成，不展开 finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); } // Destroy already created singletons to avoid dangling resources. // 销毁已经初始化的 singleton 的 Beans，以免有些 bean 会一直占用资源 destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // 把异常往外抛 throw ex; } finally { // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); } } } 准备 Bean 容器: prepareBeanFactory 之前我们说过，Spring 把我们在 xml 配置的 bean 都注册以后，会\"手动\"注册一些特殊的 bean。 这里简单介绍下 prepareBeanFactory(factory) 方法： /** * Configure the factory's standard context characteristics, * such as the context's ClassLoader and post-processors. * @param beanFactory the BeanFactory to configure */ protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) { // 设置 BeanFactory 的类加载器，我们知道 BeanFactory 需要加载类，也就需要类加载器， // 这里设置为加载当前 ApplicationContext 类的类加载器 beanFactory.setBeanClassLoader(getClassLoader()); // 设置 BeanExpressionResolver beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader())); // beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, getEnvironment())); // 添加一个 BeanPostProcessor，这个 processor 比较简单： // 实现了 Aware 接口的 beans 在初始化的时候，这个 processor 负责回调， // 这个我们很常用，如我们会为了获取 ApplicationContext 而 implement ApplicationContextAware // 注意：它不仅仅回调 ApplicationContextAware， // 还会负责回调 EnvironmentAware、ResourceLoaderAware 等，看下源码就清楚了 beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); // 下面几行的意思就是，如果某个 bean 依赖于以下几个接口的实现类，在自动装配的时候忽略它们， // Spring 会通过其他方式来处理这些依赖。 beanFactory.ignoreDependencyInterface(EnvironmentAware.class); beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); /** * 下面几行就是为特殊的几个 bean 赋值，如果有 bean 依赖了以下几个，会注入这边相应的值， * 之前我们说过，\"当前 ApplicationContext 持有一个 BeanFactory\"，这里解释了第一行。 * ApplicationContext 还继承了 ResourceLoader、ApplicationEventPublisher、MessageSource * 所以对于这几个依赖，可以赋值为 this，注意 this 是一个 ApplicationContext * 那这里怎么没看到为 MessageSource 赋值呢？那是因为 MessageSource 被注册成为了一个普通的 bean */ beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory); beanFactory.registerResolvableDependency(ResourceLoader.class, this); beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this); beanFactory.registerResolvableDependency(ApplicationContext.class, this); // 这个 BeanPostProcessor 也很简单，在 bean 实例化后，如果是 ApplicationListener 的子类， // 那么将其添加到 listener 列表中，可以理解成：注册 事件监听器 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); // 这里涉及到特殊的 bean，名为：loadTimeWeaver，这不是我们的重点，忽略它 // tips: ltw 是 AspectJ 的概念，指的是在运行期进行织入，这个和 Spring AOP 不一样， // 感兴趣的读者请参考我写的关于 AspectJ 的另一篇文章 https://www.javadoop.com/post/aspectj if (beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) { beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); // Set a temporary ClassLoader for type matching. beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); } /** * 从下面几行代码我们可以知道，Spring 往往很 \"智能\" 就是因为它会帮我们默认注册一些有用的 bean， * 我们也可以选择覆盖 */ // 如果没有定义 \"environment\" 这个 bean，那么 Spring 会 \"手动\" 注册一个 if (!beanFactory.containsLocalBean(ENVIRONMENT_BEAN_NAME)) { beanFactory.registerSingleton(ENVIRONMENT_BEAN_NAME, getEnvironment()); } // 如果没有定义 \"systemProperties\" 这个 bean，那么 Spring 会 \"手动\" 注册一个 if (!beanFactory.containsLocalBean(SYSTEM_PROPERTIES_BEAN_NAME)) { beanFactory.registerSingleton(SYSTEM_PROPERTIES_BEAN_NAME, getEnvironment().getSystemProperties()); } // 如果没有定义 \"systemEnvironment\" 这个 bean，那么 Spring 会 \"手动\" 注册一个 if (!beanFactory.containsLocalBean(SYSTEM_ENVIRONMENT_BEAN_NAME)) { beanFactory.registerSingleton(SYSTEM_ENVIRONMENT_BEAN_NAME, getEnvironment().getSystemEnvironment()); } } 在上面这块代码中，Spring 对一些特殊的 bean 进行了处理，读者如果暂时还不能消化它们也没有关系，慢慢往下看。 初始化所有的 singleton beans 我们的重点当然是 finishBeanFactoryInitialization(beanFactory); 这个巨头了，这里会负责初始化所有的 singleton beans。 注意，后面的描述中，我都会使用初始化或预初始化来代表这个阶段，Spring 会在这个阶段完成所有的 singleton beans 的实例化。 我们来总结一下，到目前为止，应该说 BeanFactory 已经创建完成，并且所有的实现了 BeanFactoryPostProcessor 接口的 Bean 都已经初始化并且其中的 postProcessBeanFactory(factory) 方法已经得到回调执行了。而且 Spring 已经“手动”注册了一些特殊的 Bean，如 environment、systemProperties 等。 剩下的就是初始化 singleton beans 了，我们知道它们是单例的，如果没有设置懒加载，那么 Spring 会在接下来初始化所有的 singleton beans。 // AbstractApplicationContext.java 834 // 初始化剩余的 singleton beans protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) { // 首先，初始化名字为 conversionService 的 Bean。本着送佛送到西的精神，我在附录中简单介绍了一下 ConversionService，因为这实在太实用了 // 什么，看代码这里没有初始化 Bean 啊！ // 注意了，初始化的动作包装在 beanFactory.getBean(...) 中，这里先不说细节，先往下看吧 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) && beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) { beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); } // Register a default embedded value resolver if no bean post-processor // (such as a PropertyPlaceholderConfigurer bean) registered any before: // at this point, primarily for resolution in annotation attribute values. if (!beanFactory.hasEmbeddedValueResolver()) { beanFactory.addEmbeddedValueResolver(new StringValueResolver() { @Override public String resolveStringValue(String strVal) { return getEnvironment().resolvePlaceholders(strVal); } }); } // 先初始化 LoadTimeWeaverAware 类型的 Bean // 之前也说过，这是 AspectJ 相关的内容，放心跳过吧 String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) { getBean(weaverAwareName); } // Stop using the temporary ClassLoader for type matching. beanFactory.setTempClassLoader(null); // 没什么别的目的，因为到这一步的时候，Spring 已经开始预初始化 singleton beans 了， // 肯定不希望这个时候还出现 bean 定义解析、加载、注册。 beanFactory.freezeConfiguration(); // 开始初始化 beanFactory.preInstantiateSingletons(); } 从上面最后一行往里看，我们就又回到 DefaultListableBeanFactory 这个类了，这个类大家应该都不陌生了吧。 preInstantiateSingletons // DefaultListableBeanFactory 728 @Override public void preInstantiateSingletons() throws BeansException { if (this.logger.isDebugEnabled()) { this.logger.debug(\"Pre-instantiating singletons in \" + this); } // this.beanDefinitionNames 保存了所有的 beanNames List beanNames = new ArrayList(this.beanDefinitionNames); // 下面这个循环，触发所有的非懒加载的 singleton beans 的初始化操作 for (String beanName : beanNames) { // 合并父 Bean 中的配置，注意 中的 parent，用的不多吧， // 考虑到这可能会影响大家的理解，我在附录中解释了一下 \"Bean 继承\"，不了解的请到附录中看一下 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); // 非抽象、非懒加载的 singletons。如果配置了 'abstract = true'，那是不需要初始化的 if (!bd.isAbstract() && bd.isSingleton() && !bd.isLazyInit()) { // 处理 FactoryBean(读者如果不熟悉 FactoryBean，请移步附录区了解) if (isFactoryBean(beanName)) { // FactoryBean 的话，在 beanName 前面加上 ‘&’ 符号。再调用 getBean，getBean 方法别急 final FactoryBean factory = (FactoryBean) getBean(FACTORY_BEAN_PREFIX + beanName); // 判断当前 FactoryBean 是否是 SmartFactoryBean 的实现，此处忽略，直接跳过 boolean isEagerInit; if (System.getSecurityManager() != null && factory instanceof SmartFactoryBean) { isEagerInit = AccessController.doPrivileged(new PrivilegedAction() { @Override public Boolean run() { return ((SmartFactoryBean) factory).isEagerInit(); } }, getAccessControlContext()); } else { isEagerInit = (factory instanceof SmartFactoryBean && ((SmartFactoryBean) factory).isEagerInit()); } if (isEagerInit) { getBean(beanName); } } else { // 对于普通的 Bean，只要调用 getBean(beanName) 这个方法就可以进行初始化了 getBean(beanName); } } } // 到这里说明所有的非懒加载的 singleton beans 已经完成了初始化 // 如果我们定义的 bean 是实现了 SmartInitializingSingleton 接口的，那么在这里得到回调，忽略 for (String beanName : beanNames) { Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) { final SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) { AccessController.doPrivileged(new PrivilegedAction() { @Override public Object run() { smartSingleton.afterSingletonsInstantiated(); return null; } }, getAccessControlContext()); } else { smartSingleton.afterSingletonsInstantiated(); } } } } 接下来，我们就进入到 getBean(beanName) 方法了，这个方法我们经常用来从 BeanFactory 中获取一个 Bean，而初始化的过程也封装到了这个方法里。 getBean 在继续前进之前，读者应该具备 FactoryBean 的知识，如果读者还不熟悉，请移步附录部分了解 FactoryBean。 // AbstractBeanFactory 196 @Override public Object getBean(String name) throws BeansException { return doGetBean(name, null, null, false); } // 我们在剖析初始化 Bean 的过程，但是 getBean 方法我们经常是用来从容器中获取 Bean 用的，注意切换思路， // 已经初始化过了就从容器中直接返回，否则就先初始化再返回 @SuppressWarnings(\"unchecked\") protected T doGetBean( final String name, final Class requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException { // 获取一个 “正统的” beanName，处理两种情况，一个是前面说的 FactoryBean(前面带 ‘&’)， // 一个是别名问题，因为这个方法是 getBean，获取 Bean 用的，你要是传一个别名进来，是完全可以的 final String beanName = transformedBeanName(name); // 注意跟着这个，这个是返回值 Object bean; // 检查下是不是已经创建过了 Object sharedInstance = getSingleton(beanName); // 这里说下 args 呗，虽然看上去一点不重要。前面我们一路进来的时候都是 getBean(beanName)， // 所以 args 传参其实是 null 的，但是如果 args 不为空的时候，那么意味着调用方不是希望获取 Bean，而是创建 Bean if (sharedInstance != null && args == null) { if (logger.isDebugEnabled()) { if (isSingletonCurrentlyInCreation(beanName)) { logger.debug(\"...\"); } else { logger.debug(\"Returning cached instance of singleton bean '\" + beanName + \"'\"); } } // 下面这个方法：如果是普通 Bean 的话，直接返回 sharedInstance， // 如果是 FactoryBean 的话，返回它创建的那个实例对象 // (FactoryBean 知识，读者若不清楚请移步附录) bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); } else { if (isPrototypeCurrentlyInCreation(beanName)) { // 创建过了此 beanName 的 prototype 类型的 bean，那么抛异常， // 往往是因为陷入了循环引用 throw new BeanCurrentlyInCreationException(beanName); } // 检查一下这个 BeanDefinition 在容器中是否存在 BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null && !containsBeanDefinition(beanName)) { // 如果当前容器不存在这个 BeanDefinition，试试父容器中有没有 String nameToLookup = originalBeanName(name); if (args != null) { // 返回父容器的查询结果 return (T) parentBeanFactory.getBean(nameToLookup, args); } else { // No args -> delegate to standard getBean method. return parentBeanFactory.getBean(nameToLookup, requiredType); } } if (!typeCheckOnly) { // typeCheckOnly 为 false，将当前 beanName 放入一个 alreadyCreated 的 Set 集合中。 markBeanAsCreated(beanName); } /* * 稍稍总结一下： * 到这里的话，要准备创建 Bean 了，对于 singleton 的 Bean 来说，容器中还没创建过此 Bean； * 对于 prototype 的 Bean 来说，本来就是要创建一个新的 Bean。 */ try { final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); checkMergedBeanDefinition(mbd, beanName, args); // 先初始化依赖的所有 Bean，这个很好理解。 // 注意，这里的依赖指的是 depends-on 中定义的依赖 String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) { for (String dep : dependsOn) { // 检查是不是有循环依赖，这里的循环依赖和我们前面说的循环依赖又不一样，这里肯定是不允许出现的，不然要乱套了，读者想一下就知道了 if (isDependent(beanName, dep)) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Circular depends-on relationship between '\" + beanName + \"' and '\" + dep + \"'\"); } // 注册一下依赖关系 registerDependentBean(dep, beanName); // 先初始化被依赖项 getBean(dep); } } // 如果是 singleton scope 的，创建 singleton 的实例 if (mbd.isSingleton()) { sharedInstance = getSingleton(beanName, new ObjectFactory() { @Override public Object getObject() throws BeansException { try { // 执行创建 Bean，详情后面再说 return createBean(beanName, mbd, args); } catch (BeansException ex) { destroySingleton(beanName); throw ex; } } }); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); } // 如果是 prototype scope 的，创建 prototype 的实例 else if (mbd.isPrototype()) { // It's a prototype -> create a new instance. Object prototypeInstance = null; try { beforePrototypeCreation(beanName); // 执行创建 Bean prototypeInstance = createBean(beanName, mbd, args); } finally { afterPrototypeCreation(beanName); } bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); } // 如果不是 singleton 和 prototype 的话，需要委托给相应的实现类来处理 else { String scopeName = mbd.getScope(); final Scope scope = this.scopes.get(scopeName); if (scope == null) { throw new IllegalStateException(\"No Scope registered for scope name '\" + scopeName + \"'\"); } try { Object scopedInstance = scope.get(beanName, new ObjectFactory() { @Override public Object getObject() throws BeansException { beforePrototypeCreation(beanName); try { // 执行创建 Bean return createBean(beanName, mbd, args); } finally { afterPrototypeCreation(beanName); } } }); bean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); } catch (IllegalStateException ex) { throw new BeanCreationException(beanName, \"Scope '\" + scopeName + \"' is not active for the current thread; consider \" + \"defining a scoped proxy for this bean if you intend to refer to it from a singleton\", ex); } } } catch (BeansException ex) { cleanupAfterBeanCreationFailure(beanName); throw ex; } } // 最后，检查一下类型对不对，不对的话就抛异常，对的话就返回了 if (requiredType != null && bean != null && !requiredType.isInstance(bean)) { try { return getTypeConverter().convertIfNecessary(bean, requiredType); } catch (TypeMismatchException ex) { if (logger.isDebugEnabled()) { logger.debug(\"Failed to convert bean '\" + name + \"' to required type '\" + ClassUtils.getQualifiedName(requiredType) + \"'\", ex); } throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); } } return (T) bean; } 大家应该也猜到了，接下来当然是分析 createBean 方法： protected abstract Object createBean(String beanName, RootBeanDefinition mbd, Object[] args) throws BeanCreationException; 第三个参数 args 数组代表创建实例需要的参数，不就是给构造方法用的参数，或者是工厂 Bean 的参数嘛，不过要注意，在我们的初始化阶段，args 是 null。 这回我们要到一个新的类了 AbstractAutowireCapableBeanFactory，看类名，AutowireCapable？类名是不是也说明了点问题了。 主要是为了以下场景，采用 @Autowired 注解注入属性值： public class MessageServiceImpl implements MessageService { @Autowired private UserService userService; public String getMessage() { return userService.getMessage(); } } 以上这种属于混用了 xml 和 注解 两种方式的配置方式，Spring 会处理这种情况。 好了，读者要知道这么回事就可以了，继续向前。 // AbstractAutowireCapableBeanFactory 447 /** * Central method of this class: creates a bean instance, * populates the bean instance, applies post-processors, etc. * @see #doCreateBean */ @Override protected Object createBean(String beanName, RootBeanDefinition mbd, Object[] args) throws BeanCreationException { if (logger.isDebugEnabled()) { logger.debug(\"Creating instance of bean '\" + beanName + \"'\"); } RootBeanDefinition mbdToUse = mbd; // 确保 BeanDefinition 中的 Class 被加载 Class resolvedClass = resolveBeanClass(mbd, beanName); if (resolvedClass != null && !mbd.hasBeanClass() && mbd.getBeanClassName() != null) { mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); } // 准备方法覆写，这里又涉及到一个概念：MethodOverrides，它来自于 bean 定义中的 // 和 ，如果读者感兴趣，回到 bean 解析的地方看看对这两个标签的解析。 // 我在附录中也对这两个标签的相关知识点进行了介绍，读者可以移步去看看 try { mbdToUse.prepareMethodOverrides(); } catch (BeanDefinitionValidationException ex) { throw new BeanDefinitionStoreException(mbdToUse.getResourceDescription(), beanName, \"Validation of method overrides failed\", ex); } try { // 让 InstantiationAwareBeanPostProcessor 在这一步有机会返回代理， // 在 《Spring AOP 源码分析》那篇文章中有解释，这里先跳过 Object bean = resolveBeforeInstantiation(beanName, mbdToUse); if (bean != null) { return bean; } } catch (Throwable ex) { throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName, \"BeanPostProcessor before instantiation of bean failed\", ex); } // 重头戏，创建 bean Object beanInstance = doCreateBean(beanName, mbdToUse, args); if (logger.isDebugEnabled()) { logger.debug(\"Finished creating instance of bean '\" + beanName + \"'\"); } return beanInstance; } 创建 Bean 我们继续往里看 doCreateBean 这个方法： /** * Actually create the specified bean. Pre-creation processing has already happened * at this point, e.g. checking {@code postProcessBeforeInstantiation} callbacks. * Differentiates between default bean instantiation, use of a * factory method, and autowiring a constructor. * @param beanName the name of the bean * @param mbd the merged bean definition for the bean * @param args explicit arguments to use for constructor or factory method invocation * @return a new instance of the bean * @throws BeanCreationException if the bean could not be created * @see #instantiateBean * @see #instantiateUsingFactoryMethod * @see #autowireConstructor */ protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) throws BeanCreationException { // Instantiate the bean. BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) { instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); } if (instanceWrapper == null) { // 说明不是 FactoryBean，这里实例化 Bean，这里非常关键，细节之后再说 instanceWrapper = createBeanInstance(beanName, mbd, args); } // 这个就是 Bean 里面的 我们定义的类 的实例，很多地方我直接描述成 \"bean 实例\" final Object bean = (instanceWrapper != null ? instanceWrapper.getWrappedInstance() : null); // 类型 Class beanType = (instanceWrapper != null ? instanceWrapper.getWrappedClass() : null); mbd.resolvedTargetType = beanType; // 建议跳过吧，涉及接口：MergedBeanDefinitionPostProcessor synchronized (mbd.postProcessingLock) { if (!mbd.postProcessed) { try { // MergedBeanDefinitionPostProcessor，这个我真不展开说了，直接跳过吧，很少用的 applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); } catch (Throwable ex) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Post-processing of merged bean definition failed\", ex); } mbd.postProcessed = true; } } // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. // 下面这块代码是为了解决循环依赖的问题，以后有时间，我再对循环依赖这个问题进行解析吧 boolean earlySingletonExposure = (mbd.isSingleton() && this.allowCircularReferences && isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) { if (logger.isDebugEnabled()) { logger.debug(\"Eagerly caching bean '\" + beanName + \"' to allow for resolving potential circular references\"); } addSingletonFactory(beanName, new ObjectFactory() { @Override public Object getObject() throws BeansException { return getEarlyBeanReference(beanName, mbd, bean); } }); } // Initialize the bean instance. Object exposedObject = bean; try { // 这一步也是非常关键的，这一步负责属性装配，因为前面的实例只是实例化了，并没有设值，这里就是设值 populateBean(beanName, mbd, instanceWrapper); if (exposedObject != null) { // 还记得 init-method 吗？还有 InitializingBean 接口？还有 BeanPostProcessor 接口？ // 这里就是处理 bean 初始化完成后的各种回调 exposedObject = initializeBean(beanName, exposedObject, mbd); } } catch (Throwable ex) { if (ex instanceof BeanCreationException && beanName.equals(((BeanCreationException) ex).getBeanName())) { throw (BeanCreationException) ex; } else { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Initialization of bean failed\", ex); } } if (earlySingletonExposure) { // Object earlySingletonReference = getSingleton(beanName, false); if (earlySingletonReference != null) { if (exposedObject == bean) { exposedObject = earlySingletonReference; } else if (!this.allowRawInjectionDespiteWrapping && hasDependentBean(beanName)) { String[] dependentBeans = getDependentBeans(beanName); Set actualDependentBeans = new LinkedHashSet(dependentBeans.length); for (String dependentBean : dependentBeans) { if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) { actualDependentBeans.add(dependentBean); } } if (!actualDependentBeans.isEmpty()) { throw new BeanCurrentlyInCreationException(beanName, \"Bean with name '\" + beanName + \"' has been injected into other beans [\" + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + \"] in its raw version as part of a circular reference, but has eventually been \" + \"wrapped. This means that said other beans do not use the final version of the \" + \"bean. This is often the result of over-eager type matching - consider using \" + \"'getBeanNamesOfType' with the 'allowEagerInit' flag turned off, for example.\"); } } } } // Register bean as disposable. try { registerDisposableBeanIfNecessary(beanName, bean, mbd); } catch (BeanDefinitionValidationException ex) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Invalid destruction signature\", ex); } return exposedObject; } 到这里，我们已经分析完了 doCreateBean 方法，总的来说，我们已经说完了整个初始化流程。 接下来我们挑 doCreateBean 中的三个细节出来说说。一个是创建 Bean 实例的 createBeanInstance 方法，一个是依赖注入的 populateBean 方法，还有就是回调方法 initializeBean。 注意了，接下来的这三个方法要认真说那也是极其复杂的，很多地方我就点到为止了，感兴趣的读者可以自己往里看，最好就是碰到不懂的，自己写代码去调试它。 创建 Bean 实例 我们先看看 createBeanInstance 方法。需要说明的是，这个方法如果每个分支都分析下去，必然也是极其复杂冗长的，我们挑重点说。此方法的目的就是实例化我们指定的类。 protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, Object[] args) { // 确保已经加载了此 class Class beanClass = resolveBeanClass(mbd, beanName); // 校验一下这个类的访问权限 if (beanClass != null && !Modifier.isPublic(beanClass.getModifiers()) && !mbd.isNonPublicAccessAllowed()) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Bean class isn't public, and non-public access not allowed: \" + beanClass.getName()); } if (mbd.getFactoryMethodName() != null) { // 采用工厂方法实例化，不熟悉这个概念的读者请看附录，注意，不是 FactoryBean return instantiateUsingFactoryMethod(beanName, mbd, args); } // 如果不是第一次创建，比如第二次创建 prototype bean。 // 这种情况下，我们可以从第一次创建知道，采用无参构造函数，还是构造函数依赖注入 来完成实例化 boolean resolved = false; boolean autowireNecessary = false; if (args == null) { synchronized (mbd.constructorArgumentLock) { if (mbd.resolvedConstructorOrFactoryMethod != null) { resolved = true; autowireNecessary = mbd.constructorArgumentsResolved; } } } if (resolved) { if (autowireNecessary) { // 构造函数依赖注入 return autowireConstructor(beanName, mbd, null, null); } else { // 无参构造函数 return instantiateBean(beanName, mbd); } } // 判断是否采用有参构造函数 Constructor[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName); if (ctors != null || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_CONSTRUCTOR || mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) { // 构造函数依赖注入 return autowireConstructor(beanName, mbd, ctors, args); } // 调用无参构造函数 return instantiateBean(beanName, mbd); } 挑个简单的无参构造函数构造实例来看看： protected BeanWrapper instantiateBean(final String beanName, final RootBeanDefinition mbd) { try { Object beanInstance; final BeanFactory parent = this; if (System.getSecurityManager() != null) { beanInstance = AccessController.doPrivileged(new PrivilegedAction() { @Override public Object run() { return getInstantiationStrategy().instantiate(mbd, beanName, parent); } }, getAccessControlContext()); } else { // 实例化 beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); } // 包装一下，返回 BeanWrapper bw = new BeanWrapperImpl(beanInstance); initBeanWrapper(bw); return bw; } catch (Throwable ex) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Instantiation of bean failed\", ex); } } 我们可以看到，关键的地方在于： beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); 这里会进行实际的实例化过程，我们进去看看: // SimpleInstantiationStrategy 59 @Override public Object instantiate(RootBeanDefinition bd, String beanName, BeanFactory owner) { // 如果不存在方法覆写，那就使用 java 反射进行实例化，否则使用 CGLIB, // 方法覆写 请参见附录\"方法注入\"中对 lookup-method 和 replaced-method 的介绍 if (bd.getMethodOverrides().isEmpty()) { Constructor constructorToUse; synchronized (bd.constructorArgumentLock) { constructorToUse = (Constructor) bd.resolvedConstructorOrFactoryMethod; if (constructorToUse == null) { final Class clazz = bd.getBeanClass(); if (clazz.isInterface()) { throw new BeanInstantiationException(clazz, \"Specified class is an interface\"); } try { if (System.getSecurityManager() != null) { constructorToUse = AccessController.doPrivileged(new PrivilegedExceptionAction>() { @Override public Constructor run() throws Exception { return clazz.getDeclaredConstructor((Class[]) null); } }); } else { constructorToUse = clazz.getDeclaredConstructor((Class[]) null); } bd.resolvedConstructorOrFactoryMethod = constructorToUse; } catch (Throwable ex) { throw new BeanInstantiationException(clazz, \"No default constructor found\", ex); } } } // 利用构造方法进行实例化 return BeanUtils.instantiateClass(constructorToUse); } else { // 存在方法覆写，利用 CGLIB 来完成实例化，需要依赖于 CGLIB 生成子类，这里就不展开了。 // tips: 因为如果不使用 CGLIB 的话，存在 override 的情况 JDK 并没有提供相应的实例化支持 return instantiateWithMethodInjection(bd, beanName, owner); } } 到这里，我们就算实例化完成了。我们开始说怎么进行属性注入。 bean 属性注入 看完了 createBeanInstance(...) 方法，我们来看看 populateBean(...) 方法，该方法负责进行属性设值，处理依赖。 // AbstractAutowireCapableBeanFactory 1203 protected void populateBean(String beanName, RootBeanDefinition mbd, BeanWrapper bw) { // bean 实例的所有属性都在这里了 PropertyValues pvs = mbd.getPropertyValues(); if (bw == null) { if (!pvs.isEmpty()) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Cannot apply property values to null instance\"); } else { // Skip property population phase for null instance. return; } } // 到这步的时候，bean 实例化完成（通过工厂方法或构造方法），但是还没开始属性设值， // InstantiationAwareBeanPostProcessor 的实现类可以在这里对 bean 进行状态修改， // 我也没找到有实际的使用，所以我们暂且忽略这块吧 boolean continueWithPropertyPopulation = true; if (!mbd.isSynthetic() && hasInstantiationAwareBeanPostProcessors()) { for (BeanPostProcessor bp : getBeanPostProcessors()) { if (bp instanceof InstantiationAwareBeanPostProcessor) { InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; // 如果返回 false，代表不需要进行后续的属性设值，也不需要再经过其他的 BeanPostProcessor 的处理 if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) { continueWithPropertyPopulation = false; break; } } } } if (!continueWithPropertyPopulation) { return; } if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) { MutablePropertyValues newPvs = new MutablePropertyValues(pvs); // 通过名字找到所有属性值，如果是 bean 依赖，先初始化依赖的 bean。记录依赖关系 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME) { autowireByName(beanName, mbd, bw, newPvs); } // 通过类型装配。复杂一些 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) { autowireByType(beanName, mbd, bw, newPvs); } pvs = newPvs; } boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors(); boolean needsDepCheck = (mbd.getDependencyCheck() != RootBeanDefinition.DEPENDENCY_CHECK_NONE); if (hasInstAwareBpps || needsDepCheck) { PropertyDescriptor[] filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); if (hasInstAwareBpps) { for (BeanPostProcessor bp : getBeanPostProcessors()) { if (bp instanceof InstantiationAwareBeanPostProcessor) { InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; // 这里有个非常有用的 BeanPostProcessor 进到这里: AutowiredAnnotationBeanPostProcessor // 对采用 @Autowired、@Value 注解的依赖进行设值，这里的内容也是非常丰富的，不过本文不会展开说了，感兴趣的读者请自行研究 pvs = ibp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvs == null) { return; } } } } if (needsDepCheck) { checkDependencies(beanName, mbd, filteredPds, pvs); } } // 设置 bean 实例的属性值 applyPropertyValues(beanName, mbd, bw, pvs); } initializeBean 属性注入完成后，这一步其实就是处理各种回调了，这块代码比较简单。 protected Object initializeBean(final String beanName, final Object bean, RootBeanDefinition mbd) { if (System.getSecurityManager() != null) { AccessController.doPrivileged(new PrivilegedAction() { @Override public Object run() { invokeAwareMethods(beanName, bean); return null; } }, getAccessControlContext()); } else { // 如果 bean 实现了 BeanNameAware、BeanClassLoaderAware 或 BeanFactoryAware 接口，回调 invokeAwareMethods(beanName, bean); } Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) { // BeanPostProcessor 的 postProcessBeforeInitialization 回调 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); } try { // 处理 bean 中定义的 init-method， // 或者如果 bean 实现了 InitializingBean 接口，调用 afterPropertiesSet() 方法 invokeInitMethods(beanName, wrappedBean, mbd); } catch (Throwable ex) { throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, \"Invocation of init method failed\", ex); } if (mbd == null || !mbd.isSynthetic()) { // BeanPostProcessor 的 postProcessAfterInitialization 回调 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); } return wrappedBean; } 大家发现没有，BeanPostProcessor 的两个回调都发生在这边，只不过中间处理了 init-method，是不是和读者原来的认知有点不一样了？ 附录 id 和 name 每个 Bean 在 Spring 容器中都有一个唯一的名字（beanName）和 0 个或多个别名（aliases）。 我们从 Spring 容器中获取 Bean 的时候，可以根据 beanName，也可以通过别名。 beanFactory.getBean(\"beanName or alias\"); 在配置 的过程中，我们可以配置 id 和 name，看几个例子就知道是怎么回事了。 以上配置的结果就是：beanName 为 messageService，别名有 3 个，分别为 m1、m2、m3。 以上配置的结果就是：beanName 为 m1，别名有 2 个，分别为 m2、m3。 beanName 为：com.javadoop.example.MessageServiceImpl#0， 别名 1 个，为： com.javadoop.example.MessageServiceImpl 以上配置的结果就是：beanName 为 messageService，没有别名。 配置是否允许 Bean 覆盖、是否允许循环依赖 我们说过，默认情况下，allowBeanDefinitionOverriding 属性为 null。如果在同一配置文件中 Bean id 或 name 重复了，会抛错，但是如果不是同一配置文件中，会发生覆盖。 可是有些时候我们希望在系统启动的过程中就严格杜绝发生 Bean 覆盖，因为万一出现这种情况，会增加我们排查问题的成本。 循环依赖说的是 A 依赖 B，而 B 又依赖 A。或者是 A 依赖 B，B 依赖 C，而 C 却依赖 A。默认 allowCircularReferences 也是 null。 它们两个属性是一起出现的，必然可以在同一个地方一起进行配置。 添加这两个属性的作者 Juergen Hoeller 在这个 jira 的讨论中说明了怎么配置这两个属性。 public class NoBeanOverridingContextLoader extends ContextLoader { @Override protected void customizeContext(ServletContext servletContext, ConfigurableWebApplicationContext applicationContext) { super.customizeContext(servletContext, applicationContext); AbstractRefreshableApplicationContext arac = (AbstractRefreshableApplicationContext) applicationContext; arac.setAllowBeanDefinitionOverriding(false); } } public class MyContextLoaderListener extends org.springframework.web.context.ContextLoaderListener { @Override protected ContextLoader createContextLoader() { return new NoBeanOverridingContextLoader(); } } com.javadoop.MyContextLoaderListener 如果以上方式不能满足你的需求，请参考这个链接：解决spring中不同配置文件中存在name或者id相同的bean可能引起的问题 profile 我们可以把不同环境的配置分别配置到单独的文件中，举个例子： 应该不必做过多解释了吧，看每个文件第一行的 profile=\"\"。 当然，我们也可以在一个配置文件中使用： 理解起来也很简单吧。 接下来的问题是，怎么使用特定的 profile 呢？Spring 在启动的过程中，会去寻找 “spring.profiles.active” 的属性值，根据这个属性值来的。那怎么配置这个值呢？ Spring 会在这几个地方寻找 spring.profiles.active 的属性值：操作系统环境变量、JVM 系统变量、web.xml 中定义的参数、JNDI。 最简单的方式莫过于在程序启动的时候指定： -Dspring.profiles.active=\"profile1,profile2\" profile 可以激活多个 当然，我们也可以通过代码的形式从 Environment 中设置 profile： AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(); ctx.getEnvironment().setActiveProfiles(\"development\"); ctx.register(SomeConfig.class, StandaloneDataConfig.class, JndiDataConfig.class); ctx.refresh(); // 重启 如果是 Spring Boot 的话更简单，我们一般会创建 application.properties、application-dev.properties、application-prod.properties 等文件，其中 application.properties 配置各个环境通用的配置，application-{profile}.properties 中配置特定环境的配置，然后在启动的时候指定 profile： java -Dspring.profiles.active=prod -jar JavaDoop.jar 如果是单元测试中使用的话，在测试类中使用 @ActiveProfiles 指定，这里就不展开了。 工厂模式生成 Bean 请读者注意 factory-bean 和 FactoryBean 的区别。这节说的是前者，是说静态工厂或实例工厂，而后者是 Spring 中的特殊接口，代表一类特殊的 Bean，附录的下面一节会介绍 FactoryBean。 设计模式里，工厂方法模式分静态工厂和实例工厂，我们分别看看 Spring 中怎么配置这两个，来个代码示例就什么都清楚了。 静态工厂： public class ClientService { private static ClientService clientService = new ClientService(); private ClientService() {} // 静态方法 public static ClientService createInstance() { return clientService; } } 实例工厂： public class DefaultServiceLocator { private static ClientService clientService = new ClientServiceImpl(); private static AccountService accountService = new AccountServiceImpl(); public ClientService createClientServiceInstance() { return clientService; } public AccountService createAccountServiceInstance() { return accountService; } } FactoryBean FactoryBean 适用于 Bean 的创建过程比较复杂的场景，比如数据库连接池的创建。 public interface FactoryBean { T getObject() throws Exception; Class getObjectType(); boolean isSingleton(); } public class Person { private Car car ; private void setCar(Car car){ this.car = car; } } 我们假设现在需要创建一个 Person 的 Bean，首先我们需要一个 Car 的实例，我们这里假设 Car 的实例创建很麻烦，那么我们可以把创建 Car 的复杂过程包装起来： public class MyCarFactoryBean implements FactoryBean{ private String make; private int year ; public void setMake(String m){ this.make =m ; } public void setYear(int y){ this.year = y; } public Car getObject(){ // 这里我们假设 Car 的实例化过程非常复杂，反正就不是几行代码可以写完的那种 CarBuilder cb = CarBuilder.car(); if(year!=0) cb.setYear(this.year); if(StringUtils.hasText(this.make)) cb.setMake( this.make ); return cb.factory(); } public Class getObjectType() { return Car.class ; } public boolean isSingleton() { return false; } } 我们看看装配的时候是怎么配置的： 看到不一样了吗？id 为 “car” 的 bean 其实指定的是一个 FactoryBean，不过配置的时候，我们直接让配置 Person 的 Bean 直接依赖于这个 FactoryBean 就可以了。中间的过程 Spring 已经封装好了。 说到这里，我们再来点干货。我们知道，现在还用 xml 配置 Bean 依赖的越来越少了，更多时候，我们可能会采用 java config 的方式来配置，这里有什么不一样呢？ @Configuration public class CarConfiguration { @Bean public MyCarFactoryBean carFactoryBean(){ MyCarFactoryBean cfb = new MyCarFactoryBean(); cfb.setMake(\"Honda\"); cfb.setYear(1984); return cfb; } @Bean public Person aPerson(){ Person person = new Person(); // 注意这里的不同 person.setCar(carFactoryBean().getObject()); return person; } } 这个时候，其实我们的思路也很简单，把 MyCarFactoryBean 看成是一个简单的 Bean 就可以了，不必理会什么 FactoryBean，它是不是 FactoryBean 和我们没关系。 初始化 Bean 的回调 有以下四种方案： public class AnotherExampleBean implements InitializingBean { public void afterPropertiesSet() { // do some initialization work } } @Bean(initMethod = \"init\") public Foo foo() { return new Foo(); } @PostConstruct public void init() { } 销毁 Bean 的回调 public class AnotherExampleBean implements DisposableBean { public void destroy() { // do some destruction work (like releasing pooled connections) } } @Bean(destroyMethod = \"cleanup\") public Bar bar() { return new Bar(); } @PreDestroy public void cleanup() { } ConversionService 既然文中说到了这个，顺便提一下好了。 最有用的场景就是，它用来将前端传过来的参数和后端的 controller 方法上的参数进行绑定的时候用。 像前端传过来的字符串、整数要转换为后端的 String、Integer 很容易，但是如果 controller 方法需要的是一个枚举值，或者是 Date 这些非基础类型（含基础类型包装类）值的时候，我们就可以考虑采用 ConversionService 来进行转换。 ConversionService 接口很简单，所以要自定义一个 convert 的话也很简单。 下面再说一个实现这种转换很简单的方式，那就是实现 Converter 接口。 来看一个很简单的例子，这样比什么都管用。 public class StringToDateConverter implements Converter { @Override public Date convert(String source) { try { return DateUtils.parseDate(source, \"yyyy-MM-dd\", \"yyyy-MM-dd HH:mm:ss\", \"yyyy-MM-dd HH:mm\", \"HH:mm:ss\", \"HH:mm\"); } catch (ParseException e) { return null; } } } 只要注册这个 Bean 就可以了。这样，前端往后端传的时间描述字符串就很容易绑定成 Date 类型了，不需要其他任何操作。 Bean 继承 在初始化 Bean 的地方，我们说过了这个： RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); 这里涉及到的就是 中的 parent 属性，我们来看看 Spring 中是用这个来干什么的。 首先，我们要明白，这里的继承和 java 语法中的继承没有任何关系，不过思路是相通的。child bean 会继承 parent bean 的所有配置，也可以覆盖一些配置，当然也可以新增额外的配置。 Spring 中提供了继承自 AbstractBeanDefinition 的 ChildBeanDefinition 来表示 child bean。 看如下一个例子: parent bean 设置了 abstract=\"true\" 所以它不会被实例化，child bean 继承了 parent bean 的两个属性，但是对 name 属性进行了覆写。 child bean 会继承 scope、构造器参数值、属性值、init-method、destroy-method 等等。 当然，我不是说 parent bean 中的 abstract = true 在这里是必须的，只是说如果加上了以后 Spring 在实例化 singleton beans 的时候会忽略这个 bean。 比如下面这个极端 parent bean，它没有指定 class，所以毫无疑问，这个 bean 的作用就是用来充当模板用的 parent bean，此处就必须加上 abstract = true。 方法注入 一般来说，我们的应用中大多数的 Bean 都是 singleton 的。singleton 依赖 singleton，或者 prototype 依赖 prototype 都很好解决，直接设置属性依赖就可以了。 但是，如果是 singleton 依赖 prototype 呢？这个时候不能用属性依赖，因为如果用属性依赖的话，我们每次其实拿到的还是第一次初始化时候的 bean。 一种解决方案就是不要用属性依赖，每次获取依赖的 bean 的时候从 BeanFactory 中取。这个也是大家最常用的方式了吧。怎么取，我就不介绍了，大部分 Spring 项目大家都会定义那么个工具类的。 另一种解决方案就是这里要介绍的通过使用 Lookup method。 lookup-method 我们来看一下 Spring Reference 中提供的一个例子： package fiona.apple; // no more Spring imports! public abstract class CommandManager { public Object process(Object commandState) { // grab a new instance of the appropriate Command interface Command command = createCommand(); // set the state on the (hopefully brand new) Command instance command.setState(commandState); return command.execute(); } // okay... but where is the implementation of this method? protected abstract Command createCommand(); } xml 配置 ： Spring 采用 CGLIB 生成字节码的方式来生成一个子类。我们定义的类不能定义为 final class，抽象方法上也不能加 final。 lookup-method 上的配置也可以采用注解来完成，这样就可以不用配置 了，其他不变： public abstract class CommandManager { public Object process(Object commandState) { MyCommand command = createCommand(); command.setState(commandState); return command.execute(); } @Lookup(\"myCommand\") protected abstract Command createCommand(); } 注意，既然用了注解，要配置注解扫描： 甚至，我们可以像下面这样： public abstract class CommandManager { public Object process(Object commandState) { MyCommand command = createCommand(); command.setState(commandState); return command.execute(); } @Lookup protected abstract MyCommand createCommand(); } 上面的返回值用了 MyCommand，当然，如果 Command 只有一个实现类，那返回值也可以写 Command。 replaced-method 记住它的功能，就是替换掉 bean 中的一些方法。 public class MyValueCalculator { public String computeValue(String input) { // some real code... } // some other methods... } 方法覆写，注意要实现 MethodReplacer 接口： public class ReplacementComputeValue implements org.springframework.beans.factory.support.MethodReplacer { public Object reimplement(Object o, Method m, Object[] args) throws Throwable { // get the input value, work with it, and return a computed result String input = (String) args[0]; ... return ...; } } 配置也很简单： String arg-type 明显不是必须的，除非存在方法重载，这样必须通过参数类型列表来判断这里要覆盖哪个方法。 BeanPostProcessor 应该说 BeanPostProcessor 概念在 Spring 中也是比较重要的。我们看下接口定义： public interface BeanPostProcessor { Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException; } 看这个接口中的两个方法名字我们大体上可以猜测 bean 在初始化之前会执行 postProcessBeforeInitialization 这个方法，初始化完成之后会执行 postProcessAfterInitialization 这个方法。但是，这么理解是非常片面的。 首先，我们要明白，除了我们自己定义的 BeanPostProcessor 实现外，Spring 容器在启动时自动给我们也加了几个。如在获取 BeanFactory 的 obtainFactory() 方法结束后的 prepareBeanFactory(factory)，大家仔细看会发现，Spring 往容器中添加了这两个 BeanPostProcessor：ApplicationContextAwareProcessor、ApplicationListenerDetector。 我们回到这个接口本身，读者请看第一个方法，这个方法接受的第一个参数是 bean 实例，第二个参数是 bean 的名字，重点在返回值将会作为新的 bean 实例，所以，没事的话这里不能随便返回个 null。 那意味着什么呢？我们很容易想到的就是，我们这里可以对一些我们想要修饰的 bean 实例做一些事情。但是对于 Spring 框架来说，它会决定是不是要在这个方法中返回 bean 实例的代理，这样就有更大的想象空间了。 最后，我们说说如果我们自己定义一个 bean 实现 BeanPostProcessor 的话，它的执行时机是什么时候？ 如果仔细看了代码分析的话，其实很容易知道了，在 bean 实例化完成、属性注入完成之后，会执行回调方法，具体请参见类 AbstractAutowireCapableBeanFactory#initBean 方法。 首先会回调几个实现了 Aware 接口的 bean，然后就开始回调 BeanPostProcessor 的 postProcessBeforeInitialization 方法，之后是回调 init-method，然后再回调 BeanPostProcessor 的 postProcessAfterInitialization 方法。 总结 按理说，总结应该写在附录前面，我就不讲究了。 在花了那么多时间后，这篇文章终于算是基本写完了，大家在惊叹 Spring 给我们做了那么多的事的时候，应该透过现象看本质，去理解 Spring 写得好的地方，去理解它的设计思想。 本文的缺陷在于对 Spring 预初始化 singleton beans 的过程分析不够，主要是代码量真的比较大，分支旁路众多。同时，虽然附录条目不少，但是庞大的 Spring 真的引出了很多的概念，希望日后有精力可以慢慢补充一些。 （全文完） "},"docs/Guide/Spring框架学习.html":{"url":"docs/Guide/Spring框架学习.html","title":"Spring框架学习","keywords":"","body":"过滤器（Filter） ​ Filter可以认为是Servlet的一种“加强版”，它主要用于对用户请求进行预处理，也可以对HttpServletResponse进行后处理，是个典型的处理链。Filter也可以对用户请求生成响应，这一点与Servlet相同，但实际上很少会使用Filter向用户请求生成响应。使用Filter完整的流程是：Filter对用户请求进行预处理，接着将请求交给Servlet进行预处理并生成响应，最后Filter再对服务器响应进行后处理。 创建一个Filter只需两个步骤 创建Filter处理类 web.xml文件中配置Filter 创建Filter必须实现javax.servlet.Filter接口，在该接口中定义了如下三个方法。 void init(FilterConfig config):用于完成Filter的初始化。 void destory()：用于Filter销毁前，完成某些资源的回收。 void doFilter(ServletRequest request,ServletResponse response,FilterChain chain)： 实现过滤功能，该方法就是对每个请求及响应增加的额外处理。该方法可以实现对用户请求进行预处理(ServletRequest request)，也可实现对服务器响应进行后处理(ServletResponse response)—它们的分界线为是否调用了chain.doFilter(),执行该方法之前，即对用户请求进行预处理；执行该方法之后，即对服务器响应进行后处理。 拦截器（Interceptor） ​ 拦截器，在AOP(Aspect-Oriented Programming)中用于在某个方法或字段被访问之前，进行拦截，然后在之前或之后加入某些操作。拦截是AOP的一种实现策略。 创建Interceptor必须实现com.opensymphony.xwork2.interceptor.Interceptor接口，该接口定义了如下三个方法。 void init():在该拦截器被实例化之后，在该拦截器执行拦截之前，系统将回调该方法。对于每个拦截器而言，其init()方法只执行一次。因此，该方法的方法体主要用于初始化资源。 void destory():该方法与init()方法对应。在拦截器实例被销毁之前，系统将回调该拦截器的destory方法，该方法用于销毁在init方法里打开的资源。 String intercept(ActionInvocation invocation):该方法是用户需要实现的拦截动作。就像Action的execute方法一样。intercept方法会返回一个字符串作为逻辑视图。如果该方法直接返回了一个字符串，系统会将跳转到该逻辑视图对应的实际视图资源，不会调用被拦截的Action。该方法的ActionInvocation参数包含了被拦截的Action的引用，可以通过调用该参数的invoke方法，将控制权转给下一个拦截器，或者转给Action的execute方法(如果该拦截器后没有其他拦截器，则直接执行Action的execute方法)。 Filter和Interceptor的区别 Filter是基于函数回调的，而Interceptor则是基于Java反射的。 Filter依赖于Servlet容器，而Interceptor不依赖于Servlet容器。 Filter对几乎所有的请求起作用，而Interceptor只能对action请求起作用。 Interceptor可以访问Action的上下文，值栈里的对象，而Filter不能。 在action的生命周期里，Interceptor可以被多次调用，而Filter只能在容器初始化时调用一次。 Filter和Interceptor的执行顺序 过滤前-拦截前-action执行-拦截后-过滤后 "},"docs/Guide/什么是伪共享.html":{"url":"docs/Guide/什么是伪共享.html","title":"什么是伪共享","keywords":"","body":"什么是伪共享 CPU缓存 缓存行 MESI协议 避免伪共享 伪共享简介 - 简书 杂谈 什么是伪共享（false sharing）？ - 彤哥读源码 - 博客园 Java中的伪共享(false sharing)以及应对方案_java_u012233832的专栏-CSDN博客 伪共享（false sharing），并发编程无声的性能杀手 - cyfonly - 博客园 "},"docs/Guide/序列化的底层怎么实现的.html":{"url":"docs/Guide/序列化的底层怎么实现的.html","title":"序列化的底层怎么实现的","keywords":"","body":"序列化和反序列化 Java序列化是指把Java对象保存为二进制字节码的过程，Java反序列化是指把二进制码重新转换成Java对象的过程。 Java序列化和反序列化数据，是通过ObjectOutputStream和ObjectInputStream这两个类来实现的， 为什么需要序列化 第一种情况是：一般情况下Java对象的声明周期都比Java虚拟机的要短，实际应用中我们希望在JVM停止运行之后能够持久化指定的对象，这时候就需要把对象进行序列化之后保存。 第二种情况是：需要把Java对象通过网络进行传输的时候。因为数据只能够以二进制的形式在网络中进行传输，因此当把对象通过网络发送出去之前需要先序列化成二进制数据，在接收端读到二进制数据之后反序列化成Java对象。 如何序列化 1、JDK类库中序列化和反序列化API （1）java.io.ObjectOutputStream：表示对象输出流； 它的writeObject(Object obj)方法可以对参数指定的obj对象进行序列化，把得到的字节序列写到一个目标输出流中； （2）java.io.ObjectInputStream：表示对象输入流； 它的readObject()方法源输入流中读取字节序列，再把它们反序列化成为一个对象，并将其返回； 2、实现序列化的要求 只有实现了Serializable或Externalizable接口的类的对象才能被序列化，否则抛出异常！ 3、实现Java对象序列化与反序列化的方法 假定一个User类，它的对象需要序列化，可以有如下三种方法： （1）若User类仅仅实现了Serializable接口，则可以按照以下方式进行序列化和反序列化 ObjectOutputStream采用默认的序列化方式，对User对象的非transient的实例变量进行序列化。 ObjcetInputStream采用默认的反序列化方式，对对User对象的非transient的实例变量进行反序列化。 （2）若User类仅仅实现了Serializable接口，并且还定义了readObject(ObjectInputStream in)和writeObject(ObjectOutputSteam out)，则采用以下方式进行序列化与反序列化。 ObjectOutputStream调用User对象的writeObject(ObjectOutputStream out)的方法进行序列化。 ObjectInputStream会调用User对象的readObject(ObjectInputStream in)的方法进行反序列化。 （3）若User类实现了Externalnalizable接口，且User类必须实现readExternal(ObjectInput in)和writeExternal(ObjectOutput out)方法，则按照以下方式进行序列化与反序列化。 ObjectOutputStream调用User对象的writeExternal(ObjectOutput out))的方法进行序列化。 ObjectInputStream会调用User对象的readExternal(ObjectInput in)的方法进行反序列化。 4、JDK类库中序列化的步骤 步骤一：创建一个对象输出流，它可以包装一个其它类型的目标输出流，如文件输出流： ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\"D:\\\\object.out\")); 步骤二：通过对象输出流的writeObject()方法写对象： oos.writeObject(new User(\"xuliugen\", \"123456\", \"male\")); 5、JDK类库中反序列化的步骤 步骤一：创建一个对象输入流，它可以包装一个其它类型输入流，如文件输入流： ObjectInputStream ois= new ObjectInputStream(new FileInputStream(\"object.out\")); 步骤二：通过对象输出流的readObject()方法读取对象： User user = (User) ois.readObject(); 说明：为了正确读取数据，完成反序列化，必须保证向对象输出流写对象的顺序与从对象输入流中读对象的顺序一致。 6、序列化和反序列化的示例 为了更好地理解Java序列化与反序列化，举一个简单的示例如下： public class SerialDemo { public static void main(String[] args) throws IOException, ClassNotFoundException { //序列化 FileOutputStream fos = new FileOutputStream(\"object.out\"); ObjectOutputStream oos = new ObjectOutputStream(fos); User user1 = new User(\"xuliugen\", \"123456\", \"male\"); oos.writeObject(user1); oos.flush(); oos.close(); //反序列化 FileInputStream fis = new FileInputStream(\"object.out\"); ObjectInputStream ois = new ObjectInputStream(fis); User user2 = (User) ois.readObject(); System.out.println(user2.getUserName()+ \" \" + user2.getPassword() + \" \" + user2.getSex()); //反序列化的输出结果为：xuliugen 123456 male } } public class User implements Serializable { private String userName; private String password; private String sex; //全参构造方法、get和set方法省略 } 序列化底层原理 writeObject0方法里 程序会 生成一个描述被序列化对象类的类元信息的ObjectStreamClass对象 根据传入的需要序列化的对象的实际类型进行不同的序列化操作。从代码里面可以很明显的看到， 对于String类型、数组类型和Enum可以直接进行序列化 如果被序列化对象实现了Serializable对象，则会调用writeOrdinaryObject()方法进行序列化 这里可以解释一个问题:Serializbale接口是个空的接口，并没有定义任何方法，为什么需要序列化的接口只要实现Serializbale接口就能够进行序列化。 答案是: Serializable接口这是一个标识，告诉程序所有实现了”我”的对象都需要进行序列化。 Java对象序列化底层原理源码解析 其他注意事项 序列化时，只对对象的状态进行保存，而不管对象的方法； 当一个父类实现序列化，子类自动实现序列化，不需要显式实现Serializable接口； 当一个对象的实例变量引用其他对象，序列化该对象时也把引用对象进行序列化； 并非所有的对象都可以序列化，至于为什么不可以，有很多原因了，比如： 安全方面的原因，比如一个对象拥有private，public等field，对于一个要传输的对象，比如写到文件，或者进行RMI传输等等，在序列化进行传输的过程中，这个对象的private等域是不受保护的； 资源分配方面的原因，比如socket，thread类，如果可以序列化，进行传输或者保存，也无法对他们进行重新的资源分配，而且，也是没有必要这样实现； 声明为static和transient类型的成员数据不能被序列化。因为static代表类的状态，transient代表对象的临时数据。 序列化运行时使用一个称为 serialVersionUID 的版本号与每个可序列化类相关联，该序列号在反序列化过程中用于验证序列化对象的发送者和接收者是否为该对象加载了与序列化兼容的类。为它赋予明确的值。显式地定义serialVersionUID有两种用途： 在某些场合，希望类的不同版本对序列化兼容，因此需要确保类的不同版本具有相同的serialVersionUID； 在某些场合，不希望类的不同版本对序列化兼容，因此需要确保类的不同版本具有不同的serialVersionUID。 Java有很多基础类已经实现了serializable接口，比如String,Vector等。但是也有一些没有实现serializable接口的； 如果一个对象的成员变量是一个对象，那么这个对象的数据成员也会被保存！这是能用序列化解决深拷贝的重要原因； 序列化和反序列化的底层实现原理是什么？_Java_徐刘根的博客-CSDN博客 java序列化与反序列化原理 - 知乎 "},"docs/Guide/Redis整理.html":{"url":"docs/Guide/Redis整理.html","title":"Redis整理","keywords":"","body":"基础结构 Redis 常用的结构 Redis 设计与实现（第一版） — Redis 设计与实现 查看底层数据编码命令 OBJECT ENCODING Hash的底层实现 REDIS_HASH （哈希表）是 HSET 、 HLEN 等命令的操作对象， 它使用 REDIS_ENCODING_ZIPLIST 和 REDIS_ENCODING_HT 两种编码方式，即对应的ziplist和dict两个结构，但是只会使用其中的一种，在满足一些条件时会从ziplist转换为dict REDIS_ENCODING_ZIPLIST: 程序通过将键和值一同推入压缩列表， 从而形成保存哈希表所需的键-值对结构。 新添加的 key-value 对会被添加到压缩列表的表尾。 当进行查找/删除或更新操作时，程序先定位到键的位置，然后再通过对键的位置来定位值的位置。 REDIS_ENCODING_HT: 创建空白哈希表时， 程序默认使用 REDIS_ENCODING_ZIPLIST 编码，当满足下面任意一个条件时，编码转化为REDIS_ENCODING_HT。 哈希表中某个键或某个值的长度大于 server.hash_max_ziplist_value （默认值为 64 ）。 压缩列表中的节点数量大于 server.hash_max_ziplist_entries （默认值为 512 ）。 zset的底层实现 REDIS_ZSET （有序集）是 ZADD 、 ZCOUNT 等命令的操作对象， 它使用 REDIS_ENCODING_ZIPLIST 和 REDIS_ENCODING_SKIPLIST 两种方式编码，即对应的ziplist和skiplist两个结构，其中skiplist又是用dict和zskiplist组成。 编码选择 在通过 ZADD 命令添加第一个元素到空 key 时， 程序通过检查输入的第一个元素来决定该创建什么编码的有序集。 如果第一个元素符合以下条件的话， 就创建一个 REDIS_ENCODING_ZIPLIST 编码的有序集： 服务器属性 server.zset_max_ziplist_entries 的值大于 0 （默认为 128 ）。 元素的 member 长度小于服务器属性 server.zset_max_ziplist_value 的值（默认为 64 ）。 否则，程序就创建一个 REDIS_ENCODING_SKIPLIST 编码的有序集。 ZIPLIST 编码的有序集 当使用 REDIS_ENCODING_ZIPLIST 编码时， 有序集将元素保存到 ziplist 数据结构里面。 其中，每个有序集元素以两个相邻的 ziplist 节点表示， 第一个节点保存元素的 member 域， 第二个元素保存元素的 score 域。 多个元素之间按 score 值从小到大排序， 如果两个元素的 score 相同， 那么按字典序对 member 进行对比， 决定那个元素排在前面， 那个元素排在后面。 |||| +---------+---------+--------+---------+--------+---------+---------+---------+ | ZIPLIST | | | | | | | ZIPLIST | | ENTRY | member1 | score1 | member2 | score2 | ... | ... | ENTRY | | HEAD | | | | | | | END | +---------+---------+--------+---------+--------+---------+---------+---------+ score1 虽然元素是按 score 域有序排序的， 但对 ziplist 的节点指针只能线性地移动， 所以在 REDIS_ENCODING_ZIPLIST 编码的有序集中， 查找某个给定元素的复杂度为 O(N)O(N) 。 每次执行添加/删除/更新操作都需要执行一次查找元素的操作， 因此这些函数的复杂度都不低于 O(N)O(N) ， 至于这些操作的实际复杂度， 取决于它们底层所执行的 ziplist 操作。 SKIPLIST 编码的有序集 当使用 REDIS_ENCODING_SKIPLIST 编码时， 有序集元素由 redis.h/zset 结构来保存： /* * 有序集 */ typedef struct zset { // 字典 dict *dict; // 跳跃表 zskiplist *zsl; } zset; zset 同时使用字典和跳跃表两个数据结构来保存有序集元素。 在Redis中，zset是一个复合结构： 使用hash来存储value和score的映射关系 使用跳跃表来提供按照score进行排序的功能，同时可以指定score范围来获取value列表 其中， 元素的成员由一个 redisObject 结构表示， 而元素的 score 则是一个 double 类型的浮点数， 字典和跳跃表两个结构通过将指针共同指向这两个值来节约空间 （不用每个元素都复制两份）。 下图展示了一个 REDIS_ENCODING_SKIPLIST 编码的有序集： 通过使用字典结构， 并将 member 作为键， score 作为值， 有序集可以在 O(1) 复杂度内： 检查给定 member 是否存在于有序集（被很多底层函数使用）； 取出 member 对应的 score 值（实现 ZSCORE 命令）。 另一方面， 通过使用跳跃表， 可以让有序集支持以下两种操作： 在 O(log⁡N) 期望时间、 O(N) 最坏时间内根据 score 对 member 进行定位（被很多底层函数使用）； 范围性查找和处理操作，这是（高效地）实现 ZRANGE 、 ZRANK 和 ZINTERSTORE 等命令的关键。 通过同时使用字典和跳跃表， 有序集可以高效地实现按成员查找和按顺序查找两种操作。 跳跃列表的实现 随机层数 对于每一个新插入的节点，都需要调用一个随机算法给它分配一个合理的层数。直观上期望的目标是 50% 的 Level1，25% 的 Level2，12.5% 的 Level3，一直到最顶层2^-63，因为这里每一层的晋升概率是 50%。 /* Returns a random level for the new skiplist node we are going to create. * The return value of this function is between 1 and ZSKIPLIST_MAXLEVEL * (both inclusive), with a powerlaw-alike distribution where higher * levels are less likely to be returned. */ int zslRandomLevel(void) { int level = 1; while ((random()&0xFFFF) 不过 Redis 标准源码中的晋升概率只有 25%，也就是代码中的 ZSKIPLIST_P 的值。所以官方的跳跃列表更加的扁平化，层高相对较低，在单个层上需要遍历的节点数量会稍多一点。 也正是因为层数一般不高，所以遍历的时候从顶层开始往下遍历会非常浪费。跳跃列表会记录一下当前的最高层数maxLevel，遍历时从这个 maxLevel 开始遍历性能就会提高很多。 跳跃表节点的level数组可以包含多个元素，每个元素都包含一个指向其他节点的指针，程序可以通过这些层来加快访问其他节点的速度，一般来说，层的数量越多，访问其他节点的速度就越快。   每次创建一个新跳跃表节点的时候，程序根据幂次定律(power law，越大的数出现的概率越小)随机生成一个介于1和32之间的值作为level数组的大小，这个大小就是层的“高度”。   下图分别展示了三个高度为1层、3层和5层的节点，因为C语言的数组索引总是从0开始的，所以节点的第一层是level[0]，而第二层是level[1]，依次类推。 元素排名 假设我们在这个skiplist中查找score=89.0的元素（即Bob的成绩数据），在查找路径中，我们会跨域图中标红的指针，这些指针上面的span值累加起来，就得到了Bob的排名(2+2+1)-1=4（减1是因为rank值以0起始）。需要注意这里算的是从小到大的排名，而如果要算从大到小的排名，只需要用skiplist长度减去查找路径上的span累加值，即6-(2+2+1)=1。 可见，在查找skiplist的过程中，通过累加span值的方式，我们就能很容易算出排名。相反，如果指定排名来查找数据（类似zrange和zrevrange那样），也可以不断累加span并时刻保持累加值不超过指定的排名，通过这种方式就能得到一条O(log n)的查找路径。 Redis跳跃表 - 知乎 Redis 的底层数据结构（跳跃表） - 掘金 关于SkipList和Redis的实现 - S.L's Blog | S.L Blog redis为什么采用跳表而不是红黑树 在做范围查找的时候，平衡树比skiplist操作要复杂。 平衡树需要以中序遍历的顺序继续寻找其它不超过大值的节点。 skiplist进行范围查找非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现。 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。 skiplist需要更少的指针内存。平均每个节点包含1.33个指针，比平衡树更有优势。 从算法实现难度上来比较，skiplist比平衡树要简单得多。 为啥 redis 使用跳表(skiplist)而不是使用 red-black？ - 知乎 redis——为什么选择了跳表而不是红黑树？_数据库_hebtu666-CSDN博客 缓存淘汰策略 采用了 定期删除+惰性删除 的策略 定期删除： Redis 默认会每秒进行十次过期扫描，过期扫描不会遍历过期字典中所有的 key，而是采用了一种简单的贪心策略。 从过期字典中随机 20 个 key； 删除这 20 个 key 中已经过期的 key； 如果过期的 key 比率超过 1/4，那就重复步骤 1； 同时，为了保证过期扫描不会出现循环过度，导致线程卡死现象，算法还增加了扫描时间的上限，默认不会超过 25ms。 Redis 内存淘汰的机制有以下几种方案可供选择： volatile-lru：从设置过期的数据集中淘汰最近最少使用的 key allkeys-lru：从所有的数据集中淘汰最近最少使用的key volatile-lfu：从设置过期的数据集中淘汰最不常使用的 key allkeys-lfu：从所有的数据集中淘汰最不常使用的key volatile-random：从设置过期的数据集中随机选取 key 淘汰 allkyes-random：从所有的数据集中随机选取key淘汰 volatile-ttl：从设置过期的数据集中淘汰即将过期的 key no-envicition：不进行淘汰，只是返回一个写操作错误，读和删除动作正常 缓存雪崩、缓存穿透、缓存击穿 缓存穿透 缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。 有很多种方法可以有效地解决缓存穿透问题。 最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。另外也有一个更为简单粗暴的方法（我们采用的就是这种），如果一个查询返回的数据为空（不管是数 据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟； 加强参数校验； 对不存在值存储空值； 缓存雪崩 缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到DB，DB瞬时压力过重雪崩。 缓存失效时的雪崩效应对底层系统的冲击非常可怕。大多数系统设计者考虑用加锁或者队列的方式保证缓存的单线程(进程)写，从而避免失效时大量的并发请求落到底层存储系统上。这里分享一个简单方案就是将缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 缓存击穿 对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题，这个和缓存雪崩的区别在于这里针对某一key缓存，前者则是很多key。 缓存在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 使用互斥锁(MUTEX KEY) 业界比较常用的做法，是使用mutex。简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redis的SETNX或者Memcache的ADD）去set一个mutex key，当操作返回成功时，再进行load db的操作并回设缓存；否则，就重试整个get缓存的方法。 “提前”使用互斥锁(MUTEX KEY) 在value内部设置1个超时值(timeout1), timeout1比实际的memcache timeout(timeout2)小。当从cache读取到timeout1发现它已经过期时候，马上延长timeout1并重新设置到cache。然后再从数据库加载数据并设置到cache中。 “永远不过期” 这里的“永远不过期”包含两层意思： (1) 从redis上看，确实没有设置过期时间，这就保证了，不会出现热点key过期问题，也就是“物理”不过期。 (2) 从功能上看，如果不过期，那不就成静态的了吗？所以我们把过期时间存在key对应的value里，如果发现要过期了，通过一个后台的异步线程进行缓存的构建，也就是“逻辑”过期 Redis为什么快？ 单线程为什么就快了？ 纯内存 单线程（原子性、避免线程切换浪费资源） IO多路复用 合理的数据结构（dict） Redis为什么是单线程，高并发快的3大原因详解 - 知乎 IO多路复用和线程池哪个效率更高，更有优势_Java_snoweaglelord的博客-CSDN博客 IO多路复用和线程池在提高并发性上应用场景的区别 - 简书 IO多路复用和线程池哪个效率更高，更有优势？ - 知乎 分布式锁 Redis如何实现分布式锁 redis版本在2.6.12之前，set是不支持nx参数的，如果想要完成一个锁，那么需要两条命令： 1. setnx Test uuid 2. expire Test 30 即放入Key和设置有效期，是分开的两步，理论上会出现1刚执行完，程序挂掉，无法保证原子性。 但是早在2013年，也就是7年前，Redis就发布了2.6.12版本，并且官网(set命令页)，也早早就说明了“SETNX, SETEX, PSETEX可能在未来的版本中，会弃用并永久删除”。 setnx // NX是指如果key不存在就成功，key存在返回false，PX可以指定过期时间 // value设置一个唯一的客户端ID，或者用UUID这种随机数。 // 当解锁的时候，先获取value判断是否是当前线程加的锁，再去删除 // 因为每次get和del并非原子操作，还是有线程安全问题 SET anyLock unique_value NX PX 30000 Lua脚本 一个命令(eval/evalsha)去执行的，一条命令没执行完，其他客户端是看不到的，保证原子性。 -- lua删除锁： -- KEYS和ARGV分别是以集合方式传入的参数，对应上文的Test和uuid。 -- 如果对应的value等于传入的uuid。 if redis.call('get', KEYS[1]) == ARGV[1] then -- 执行删除操作 return redis.call('del', KEYS[1]) else -- 不成功，返回0 return 0 end 开源框架：Redission、RedLock RedissonClient redisson = Redisson.create(config); RLock lock = redisson.getLock(\"anyLock\"); // RLock是可重入锁 lock.lock(); lock.unlock(); --- 就是这么简单，我们只需要通过它的 API 中的 Lock 和 Unlock 即可完成分布式锁，他帮我们考虑了很多细节： Redisson 所有指令都通过 Lua 脚本执行，Redis 支持 Lua 脚本原子性执行。 Redisson 设置一个 Key 的默认过期时间为 30s，如果某个客户端持有一个锁超过了 30s 怎么办？ Redisson 中有一个 Watchdog 的概念，翻译过来就是看门狗，它会在你获取锁之后，每隔 10s 帮你把 Key 的超时时间设为 30s。 这样的话，就算一直持有锁也不会出现 Key 过期了，其他线程获取到锁的问题了。 Redisson 的“看门狗”逻辑保证了没有死锁发生。(如果机器宕机了，看门狗也就没了。此时就不会延长 Key 的过期时间，到了 30s 之后就会自动过期了，其他线程可以获取到锁) 在redis中加锁有两种思路。 一种是设置key的值，并且不对key设置过期时间。这种情况下如果加锁的线程在没有解锁之前崩溃了，那么这个锁会出现死锁的状态。 另外一种是设置key的值，并且对key设置过期时间。这种情况下如果加锁的线程在没有解锁之前崩溃了，那么这个锁在过期时间之后自然解锁，不会发生死锁的现象。但是这样也引入了另外一个问题，如果加锁的线程在过期时间之内没有完成操作，这时候锁就会被另外的线程获取，从而发生同时有两个线程同时在临界区运行的状况。为了避免这种情况发生，Redisson内部提供了一个监控锁的看门狗，它的作用是在Redisson实例被关闭前，不断的延长锁的有效期。默认情况下，看门狗的检查锁的超时时间是30秒钟，也可以通过修改Config.lockWatchdogTimeout来另行指定。 Redisson分布式锁的实现 | wangqi的blog zk如何实现分布式锁 在 ZooKeeper 中，节点类型可以分为持久节点（PERSISTENT ）、临时节点（EPHEMERAL），以及时序节点（SEQUENTIAL ），具体在节点创建过程中，一般是组合使用，可以生成 4 种节点类型：持久节点（PERSISTENT），持久顺序节点（PERSISTENT_SEQUENTIAL），临时节点（EPHEMERAL），临时顺序节点（EPHEMERAL_SEQUENTIAL）；具体节点含义，谷歌之。 大家都是上来直接创建一个锁节点下的一个接一个的临时顺序节点 如果自己不是第一个节点，就对自己上一个节点加监听器 只要上一个节点释放锁，自己就排到前面去了，相当于是一个排队机制。 而且用临时顺序节点的另外一个用意就是，如果某个客户端创建临时顺序节点之后，不小心自己宕机了也没关系，zk感知到那个客户端宕机，会自动删除对应的临时顺序节点，相当于自动释放锁，或者是自动取消自己的排队。 zookeeper笔记之基于zk实现分布式锁 - CC11001100 - 博客园 七张图彻底讲清楚ZooKeeper分布式锁的实现原理【石杉的架构笔记】 - 掘金 两者的分布式锁区别 对于 Redis 的分布式锁而言，它有以下缺点： 它获取锁的方式简单粗暴，获取不到锁直接不断尝试获取锁，比较消耗性能。 另外来说的话，Redis 的设计定位决定了它的数据并不是强一致性的，在某些极端情况下，可能会出现问题。锁的模型不够健壮。 即便使用 Redlock 算法来实现，在某些复杂场景下，也无法保证其实现 100% 没有问题，关于 Redlock 的讨论可以看 How to do distributed locking。 Redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。 但是另一方面使用 Redis 实现分布式锁在很多企业中非常常见，而且大部分情况下都不会遇到所谓的“极端复杂场景”。 所以使用 Redis 作为分布式锁也不失为一种好的方案，最重要的一点是 Redis 的性能很高，可以支撑高并发的获取、释放锁操作。 对于 ZK 分布式锁而言: ZK 天生设计定位就是分布式协调，强一致性。锁的模型健壮、简单易用、适合做分布式锁。 如果获取不到锁，只需要添加一个监听器就可以了，不用一直轮询，性能消耗较小。 但是 ZK 也有其缺点：如果有较多的客户端频繁的申请加锁、释放锁，对于 ZK 集群的压力会比较大。 集群 持久化的原理 主从复制原理 哨兵模式的原理 我们可以将 Redis Sentinel 集群看成是一个 ZooKeeper 集群，它是集群高可用的心脏，它一般是由 3～5 个节点组成，这样挂了个别节点集群还可以正常运转。 它负责持续监控主从节点的健康，当主节点挂掉时，自动选择一个最优的从节点切换为主节点。客户端来连接集群时，会首先连接 sentinel，通过 sentinel 来查询主节点的地址，然后再去连接主节点进行数据交互。当主节点发生故障时，客户端会重新向 sentinel 要地址，sentinel 会将最新的主节点地址告诉客户端。如此应用程序将无需重启即可自动完成节点切换。 消息丢失怎么处理 Redis 主从采用异步复制，意味着当主节点挂掉时，从节点可能没有收到全部的同步消息，这部分未同步的消息就丢失了。如果主从延迟特别大，那么丢失的数据就可能会特别多。Sentinel 无法保证消息完全不丢失，但是也尽可能保证消息少丢失。它有两个选项可以限制主从延迟过大。 min-slaves-to-write 1 min-slaves-max-lag 10 第一个参数表示主节点必须至少有一个从节点在进行正常复制，否则就停止对外写服务，丧失可用性。 何为正常复制，何为异常复制？这个就是由第二个参数控制的，它的单位是秒，表示如果 10s 没有收到从节点的反馈，就意味着从节点同步不正常，要么网络断开了，要么一直没有给反馈。 Redis Cluster集群的原理 RedisCluster 是 Redis 的亲儿子，它是 Redis 作者自己提供的 Redis 集群化方案。 相对于 Codis 的不同，它是去中心化的，如图所示，该集群有三个 Redis 节点组成，每个节点负责整个集群的一部分数据，每个节点负责的数据多少可能不一样。这三个节点相互连接组成一个对等的集群，它们之间通过一种特殊的二进制协议相互交互集群信息。 Redis Cluster 将所有数据划分为 16384 的 slots，它比 Codis 的 1024 个槽划分的更为精细，每个节点负责其中一部分槽位。槽位的信息存储于每个节点中，它不像 Codis，它不需要另外的分布式存储来存储节点槽位信息。 当 Redis Cluster 的客户端来连接集群时，它也会得到一份集群的槽位配置信息。这样当客户端要查找某个 key 时，可以直接定位到目标节点。 这点不同于 Codis，Codis 需要通过 Proxy 来定位目标节点，RedisCluster 是直接定位。客户端为了可以直接定位某个具体的 key 所在的节点，它就需要缓存槽位相关信息，这样才可以准确快速地定位到相应的节点。同时因为槽位的信息可能会存在客户端与服务器不一致的情况，还需要纠正机制来实现槽位信息的校验调整。 另外，RedisCluster 的每个节点会将集群的配置信息持久化到配置文件中，所以必须确保配置文件是可写的，而且尽量不要依靠人工修改配置文件。 槽位定位算法 跳转 迁移 迁移数据过程中的状态，importing和migrating 从源节点获取内容 => 存到目标节点 => 从源节点删除内容 在迁移期间有数据访问，先访问旧节点，没有的话发asking状态，询问数据实际在哪个节点 扩容 启动新的redis主节点 将新的redis主节点加入cluster集群 分片hash槽（可以从一个节点或者所有节点分出指定的slot个数） 缩容 把需下线的redis主节点（源节点）上的hash 槽均匀的移到其他主节点(目标节点)上，也可以全部移到一个节点上 从cluster中移除需下线的redis主节点 停止已移除的redis主节点 容错 主节点需要有从节点在主节点挂了的时候替换，但是没有从节点的话，可以通过配置可以允许部分节点故障，其它节点还可以继续提供对外访问 Redis进阶实践之十二 Redis的Cluster集群动态扩容 - 可均可可 - 博客园 浅析Redis分布式集群倾斜问题 - Nosql-炼数成金-Dataguru专业数据分析社区 Redis集群扩容和缩容_数据库_zsj777的专栏-CSDN博客 Redis Cluster 集群扩容与收缩 - jiangz222 - 博客园 redis集群cluster搭建，扩容缩容 - 燃犀的个人空间 - OSCHINA Redis 学习笔记（十五）Redis Cluster 集群扩容与收缩_数据库_men_wen的博客-CSDN博客 redis集群cluster搭建，扩容缩容 - 燃犀的个人空间 - OSCHINA 为什么要分16384个slot ? 如果槽位为65536，发送心跳信息的消息头达8k，发送的心跳包过于庞大。 在消息头中，最占空间的是 myslots[CLUSTER_SLOTS/8]。当槽位为65536时，这块的大小是: 65536÷8=8kb因为每秒钟，redis节点需要发送一定数量的ping消息作为心跳包，如果槽位为65536，这个ping消息的消息头太大了，浪费带宽。 redis的集群主节点数量基本不可能超过1000个。 如上所述，集群节点越多，心跳包的消息体内携带的数据越多。如果节点过1000个，也会导致网络拥堵。因此redis作者，不建议redis cluster节点数量超过1000个。那么，对于节点数在1000以内的redis cluster集群，16384个槽位够用了。没有必要拓展到65536个。 槽位越小，节点少的情况下，压缩率高。 Redis主节点的配置信息中，它所负责的哈希槽是通过一张bitmap的形式来保存的，在传输过程中，会对bitmap进行压缩，但是如果bitmap的填充率slots / N很高的话(N表示节点数)，bitmap的压缩率就很低。如果节点数很少，而哈希槽数量很多的话，bitmap的压缩率就很低。而16384÷8=2kb，怎么样，神奇不！ 综上所述，作者决定取16384个槽，不多不少，刚刚好！ Redis-Sentinel 和 Redis Cluster对比 Redis-Sentinel Redis-Sentinel(哨兵模式)是Redis官方推荐的高可用性(HA)解决方案，当用Redis做Master-slave的高可用方案时，假如master宕机了，Redis本身(包括它的很多客户端)都没有实现自动进行主备切换，而Redis-sentinel本身也是一个独立运行的进程，它能监控多个master-slave集群，发现master宕机后能进行自懂切换。 优点 1、Master 状态监测 2、如果Master 异常，则会进行Master-slave 转换，将其中一个Slave作为Master，将之前的Master作为Slave 3、Master-Slave切换后，master_redis.conf、slave_redis.conf和sentinel.conf的内容都会发生改变，即master_redis.conf中会多一行slaveof的配置，sentinel.conf的监控目标会随之调换 缺点： 1、如果是从节点下线了，sentinel是不会对其进行故障转移的，连接从节点的客户端也无法获取到新的可用从节点 2、无法实现动态扩容 Redis Cluster 使用Redis Sentinel 模式架构的缓存体系，在使用的过程中，随着业务的增加不可避免的要对Redis进行扩容，熟知的扩容方式有两种，一种是垂直扩容，一种是水平扩容。垂直扩容表示通过加内存方式来增加整个缓存体系的容量比如将缓存大小由2G调整到4G,这种扩容不需要应用程序支持；水平扩容表示表示通过增加节点的方式来增加整个缓存体系的容量比如本来有1个节点变成2个节点，这种扩容方式需要应用程序支持。垂直扩容看似最便捷的扩容，但是受到机器的限制，一个机器的内存是有限的，所以垂直扩容到一定阶段不可避免的要进行水平扩容，如果预留出很多节点感觉又是对资源的一种浪费因为对业务的发展趋势很快预测。Redis Sentinel 水平扩容一直都是程序猿心中的痛点，因为水平扩容牵涉到数据的迁移。迁移过程一方面要保证自己的业务是可用的，一方面要保证尽量不丢失数据所以数据能不迁移就尽量不迁移。针对这个问题，Redis Cluster就应运而生了。 优点： 1、有效的解决了redis在分布式方面的需求 2、遇到单机内存，并发和流量瓶颈等问题时，可采用Cluster方案达到负载均衡的目的 3、可实现动态扩容 4、P2P模式，无中心化 5、通过Gossip协议同步节点信息 6、自动故障转移、Slot迁移中数据可用 缺点： 1、架构比较新，最佳实践较少 2、为了性能提升，客户端需要缓存路由表信息 3、节点发现、reshard操作不够自动化 Redis Sentinal着眼于高可用，在master宕机时会自动将slave提升为master，继续提供服务。 Redis Cluster着眼于扩展性，在单个redis内存不足时，使用Cluster进行分片存储。 "},"docs/Guide/Redis数据结构与对象.html":{"url":"docs/Guide/Redis数据结构与对象.html","title":"Redis数据结构与对象","keywords":"","body":"对象 Redis有五种基本数据结构：字符串、hash、set、zset、list。但是你知道构成这五种结构的底层数据结构是怎样的吗？ Redis创建一个键值对时至少会创建两个对象，一个对象用作键值对的键(键对象)，另一个对象用作键值对的值(值对象)。其中键总是一个字符串对象，值则可以是以下五种对象中的一种。 类型常量 对象的名称 type命令输出 REDIS_STRING 字符串对象 \"string\" REDIS_LIST 列表对象 \"list\" REDIS_HASH 哈希对象 \"hash\" REDIS_SET 集合对象 \"set\" REDIS_ZSET 有序集合对象 \"zset\" 结构 Redis中每个对象都由RedisObject结构表示： typedef struct redisObject { // 类型 unsigned type:4; // 不使用(对齐位) unsigned notused:2; // 编码方式 unsigned encoding:4; // LRU 时间（相对于 server.lruclock） unsigned lru:22; // 引用计数 int refcount; // 指向底层实现数据结构的指针 void *ptr; } robj; 编码和底层实现 编码常量 底层数据结构 object encoding命令输出 REDIS_ENCODING_INT long 类型的整数 \"int\" REDIS_ENCODING_EMBSTR embstr 编码的简单动态字符串 \"embstr\" REDIS_ENCODING_RAW 简单动态字符串 \"raw\" REDIS_ENCODING_HT 字典 \"hashtable\" REDIS_ENCODING_LINKEDLIST 双端链表 \"linkedlist\" REDIS_ENCODING_ZIPLIST 压缩列表 \"ziplist\" REDIS_ENCODING_INTSET 整数集合 \"intset\" REDIS_ENCODING_SKIPLIST 跳跃表和字典 \"skiplist\" 对象和底层结构对应关系 其实他们都至少对应两种底层结构，只是会进行类型转换，比如长度达到多少或者内存占用达到多少。 字符串 其中：embstr和raw都是由SDS动态字符串构成的。唯一区别是：raw是分配内存的时候，redisobject和 sds 各分配一块内存，而embstr是redisobject和raw在一块儿内存中。 列表 hash set zset 空转时间 redisObject 结构包含的最后一个属性为 lru 属性， 该属性记录了对象最后一次被命令程序访问的时间。 OBJECT IDLETIME 命令可以打印出给定键的空转时长， 这一空转时长就是通过将当前时间减去键的值对象的 lru 时间计算得出的。 OBJECT IDLETIME 命令的实现是特殊的， 这个命令在访问键的值对象时， 不会修改值对象的 lru 属性。 除了可以被 OBJECT IDLETIME 命令打印出来之外， 键的空转时长还有另外一项作用： 如果服务器打开了 maxmemory 选项， 并且服务器用于回收内存的算法为 volatile-lru 或者 allkeys-lru ， 那么当服务器占用的内存数超过了 maxmemory 选项所设置的上限值时， 空转时长较高的那部分键会优先被服务器释放， 从而回收内存。 简单动态字符串（SDS） 结构体定义 每个 sds.h/sdshdr 结构表示一个 SDS 值： struct sdshdr { // 记录 buf 数组中已使用字节的数量 // 等于 SDS 所保存字符串的长度 int len; // 记录 buf 数组中未使用字节的数量 int free; // 字节数组，用于保存字符串 char buf[]; }; free 属性的值为 0 ， 表示这个 SDS 没有分配任何未使用空间。 len 属性的值为 5 ， 表示这个 SDS 保存了一个五字节长的字符串。 buf 属性是一个 char 类型的数组， 数组的前五个字节分别保存了 'R' 、 'e' 、 'd' 、 'i' 、 's' 五个字符， 而最后一个字节则保存了空字符 '\\0' 。 特性 常数复杂度获取字符长度 通过int len属性 杜绝缓冲区溢出 SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性： 当 SDS API 需要对 SDS 进行修改时， API 会先检查 SDS 的空间是否满足修改所需的要求， 如果不满足的话， API 会自动将 SDS 的空间扩展至执行修改所需的大小， 然后才执行实际的修改操作， 所以使用 SDS 既不需要手动修改 SDS 的空间大小， 也不会出现前面所说的缓冲区溢出问题。 减少修改字符串时带来的内存重分配次数 空间预分配 空间预分配用于优化 SDS 的字符串增长操作： 当 SDS 的 API 对一个 SDS 进行修改， 并且需要对 SDS 进行空间扩展的时候， 程序不仅会为 SDS 分配修改所必须要的空间， 还会为 SDS 分配额外的未使用空间。 其中， 额外分配的未使用空间数量由以下公式决定： 如果对 SDS 进行修改之后， SDS 的长度（也即是 len 属性的值）将小于 1 MB ， 那么程序分配和 len 属性同样大小的未使用空间， 这时 SDS len 属性的值将和 free 属性的值相同。 举个例子， 如果进行修改之后， SDS 的 len 将变成 13 字节， 那么程序也会分配 13 字节的未使用空间， SDS 的 buf 数组的实际长度将变成 13 + 13 + 1 = 27 字节（额外的一字节用于保存空字符）。 如果对 SDS 进行修改之后， SDS 的长度将大于等于 1 MB ， 那么程序会分配 1 MB 的未使用空间。 举个例子， 如果进行修改之后， SDS 的 len 将变成 30 MB ， 那么程序会分配 1 MB 的未使用空间， SDS 的 buf 数组的实际长度将为 30 MB + 1 MB + 1 byte 。 惰性空间释放 惰性空间释放用于优化 SDS 的字符串缩短操作： 当 SDS 的 API 需要缩短 SDS 保存的字符串时， 程序并不立即使用内存重分配来回收缩短后多出来的字节， 而是使用 free 属性将这些字节的数量记录起来， 并等待将来使用。 安全的二进制 ​ C 字符串中的字符必须符合某种编码（比如 ASCII）， 并且除了字符串的末尾之外， 字符串里面不能包含空字符， 否则最先被程序读入的空字符将被误认为是字符串结尾 —— 这些限制使得 C 字符串只能保存文本数据， 而不能保存像图片、音频、视频、压缩文件这样的二进制数据。 ​ 为了确保 Redis 可以适用于各种不同的使用场景， SDS 的 API 都是二进制安全的（binary-safe）： 所有 SDS API 都会以处理二进制的方式来处理 SDS 存放在 buf 数组里的数据， 程序不会对其中的数据做任何限制、过滤、或者假设 —— 数据在写入时是什么样的， 它被读取时就是什么样。 兼容部分C字符串函数 与C字符串的区别 C 字符串 SDS 获取字符串长度的复杂度为 O(N) 。 获取字符串长度的复杂度为 O(1) 。 API 是不安全的，可能会造成缓冲区溢出。 API 是安全的，不会造成缓冲区溢出。 修改字符串长度 N 次必然需要执行 N 次内存重分配。 修改字符串长度 N 次最多需要执行 N 次内存重分配。 只能保存文本数据。 可以保存文本或者二进制数据。 可以使用所有 `` 库中的函数。 可以使用一部分 `` 库中的函数。 链表 链表提供了高效的节点重排能力， 以及顺序性的节点访问方式， 并且可以通过增删节点来灵活地调整链表的长度。 作为一种常用数据结构， 链表内置在很多高级的编程语言里面， 因为 Redis 使用的 C 语言并没有内置这种数据结构， 所以 Redis 构建了自己的链表实现。 链表在 Redis 中的应用非常广泛， 比如列表键的底层实现之一就是链表： 当一个列表键包含了数量比较多的元素， 又或者列表中包含的元素都是比较长的字符串时， Redis 就会使用链表作为列表键的底层实现。 结构体 每个链表节点使用一个 adlist.h/listNode 结构来表示： typedef struct listNode { // 前置节点 struct listNode *prev; // 后置节点 struct listNode *next; // 节点的值 void *value; } listNode; 虽然仅仅使用多个 listNode 结构就可以组成链表， 但使用 adlist.h/list 来持有链表的话， 操作起来会更方便： typedef struct list { // 表头节点 listNode *head; // 表尾节点 listNode *tail; // 链表所包含的节点数量 unsigned long len; // 节点值复制函数 void *(*dup)(void *ptr); // 节点值释放函数 void (*free)(void *ptr); // 节点值对比函数 int (*match)(void *ptr, void *key); } list; list 结构为链表提供了表头指针 head 、表尾指针 tail ， 以及链表长度计数器 len ， 而 dup 、 free 和 match 成员则是用于实现多态链表所需的类型特定函数： dup 函数用于复制链表节点所保存的值； free 函数用于释放链表节点所保存的值； match 函数则用于对比链表节点所保存的值和另一个输入值是否相等。 小结 Redis 的链表实现的特性可以总结如下： 双端： 链表节点带有 prev 和 next 指针， 获取某个节点的前置节点和后置节点的复杂度都是 O(1) 。 无环： 表头节点的 prev 指针和表尾节点的 next 指针都指向 NULL ， 对链表的访问以 NULL 为终点。 带表头指针和表尾指针： 通过 list 结构的 head 指针和 tail 指针， 程序获取链表的表头节点和表尾节点的复杂度为 O(1) 。 带链表长度计数器： 程序使用 list 结构的 len 属性来对 list 持有的链表节点进行计数， 程序获取链表中节点数量的复杂度为 O(1) 。 多态： 链表节点使用 void* 指针来保存节点值， 并且可以通过 list 结构的 dup 、 free 、 match 三个属性为节点值设置类型特定函数， 所以链表可以用于保存各种不同类型的值。 跳跃表 Redis 的跳跃表由 server.h/zskiplistNode 和 server.h/zskiplist 两个结构定义， 其中 zskiplistNode 结构用于表示跳跃表节点， 而 zskiplist 结构则用于保存跳跃表节点的相关信息， 比如节点的数量， 以及指向表头节点和表尾节点的指针， 等等。 结构 跳跃表节点的实现由 redis.h/zskiplistNode 结构定义： typedef struct zskiplistNode { // 后退指针 struct zskiplistNode *backward; // 分值 double score; // 成员对象 robj *obj; // 层 struct zskiplistLevel { // 前进指针 struct zskiplistNode *forward; // 跨度 unsigned int span; } level[]; } zskiplistNode; 虽然仅靠多个跳跃表节点就可以组成一个跳跃表，但通过使用一个 zskiplist 结构来持有这些节点， 程序可以更方便地对整个跳跃表进行处理， 比如快速访问跳跃表的表头节点和表尾节点， 又或者快速地获取跳跃表节点的数量（也即是跳跃表的长度）等信息。 zskiplist 结构的定义如下： typedef struct zskiplist { // 表头节点和表尾节点 struct zskiplistNode *header, *tail; // 表中节点的数量 unsigned long length; // 表中层数最大的节点的层数 int level; } zskiplist; header 和 tail 指针分别指向跳跃表的表头和表尾节点， 通过这两个指针， 程序定位表头节点和表尾节点的复杂度为 O(1) 。 通过使用 length 属性来记录节点的数量， 程序可以在 O(1) 复杂度内返回跳跃表的长度。 level 属性则用于在 O(1) 复杂度内获取跳跃表中层高最大的那个节点的层数量， 注意表头节点的层高并不计算在内。 层 跳跃表节点的 level 数组可以包含多个元素， 每个元素都包含一个指向其他节点的指针， 程序可以通过这些层来加快访问其他节点的速度， 一般来说， 层的数量越多， 访问其他节点的速度就越快。 每次创建一个新跳跃表节点的时候， 程序都根据幂次定律 （power law，越大的数出现的概率越小） 随机生成一个介于 1 和 32 之间的值作为 level 数组的大小， 这个大小就是层的“高度”。 前进指针 每个层都有一个指向表尾方向的前进指针（level[i].forward 属性）， 用于从表头向表尾方向访问节点。 后退指针 节点的后退指针（backward 属性）用于从表尾向表头方向访问节点： 跟可以一次跳过多个节点的前进指针不同， 因为每个节点只有一个后退指针， 所以每次只能后退至前一个节点。 跨度 层的跨度（level[i].span 属性）用于记录两个节点之间的距离： 两个节点之间的跨度越大， 它们相距得就越远。 指向 NULL 的所有前进指针的跨度都为 0 ， 因为它们没有连向任何节点。 分值和成员 节点的分值（score 属性）是一个 double 类型的浮点数， 跳跃表中的所有节点都按分值从小到大来排序。 节点的成员对象（obj 属性）是一个指针， 它指向一个字符串对象， 而字符串对象则保存着一个 SDS 值。 在同一个跳跃表中， 各个节点保存的成员对象必须是唯一的， 但是多个节点保存的分值却可以是相同的： 分值相同的节点将按照成员对象在字典序中的大小来进行排序， 成员对象较小的节点会排在前面（靠近表头的方向）， 而成员对象较大的节点则会排在后面（靠近表尾的方向）。 小结 跳跃表是有序集合的底层实现之一， 除此之外它在 Redis 中没有其他应用。 Redis 的跳跃表实现由 zskiplist 和 zskiplistNode 两个结构组成， 其中 zskiplist 用于保存跳跃表信息（比如表头节点、表尾节点、长度）， 而 zskiplistNode 则用于表示跳跃表节点。 每个跳跃表节点的层高都是 1 至 32 之间的随机数。 在同一个跳跃表中， 多个节点可以包含相同的分值， 但每个节点的成员对象必须是唯一的。 跳跃表中的节点按照分值大小进行排序， 当分值相同时， 节点按照成员对象的大小进行排序。 字典 结构体 Redis 字典所使用的哈希表由 dict.h/dictht 结构定义： typedef struct dictht { // 哈希表数组 dictEntry **table; // 哈希表大小 unsigned long size; // 哈希表大小掩码，用于计算索引值 // 总是等于 size - 1 unsigned long sizemask; // 该哈希表已有节点的数量 unsigned long used; } dictht; table 属性是一个数组， 数组中的每个元素都是一个指向 dict.h/dictEntry 结构的指针， 每个 dictEntry 结构保存着一个键值对。 size 属性记录了哈希表的大小， 也即是 table 数组的大小， 而 used 属性则记录了哈希表目前已有节点（键值对）的数量。 sizemask 属性的值总是等于 size - 1 ， 这个属性和哈希值一起决定一个键应该被放到 table 数组的哪个索引上面。 哈希表节点使用 dictEntry 结构表示， 每个 dictEntry 结构都保存着一个键值对： typedef struct dictEntry { // 键 void *key; // 值 union { void *val; uint64_t u64; int64_t s64; } v; // 指向下个哈希表节点，形成链表 struct dictEntry *next; } dictEntry; key 属性保存着键值对中的键， 而 v 属性则保存着键值对中的值， 其中键值对的值可以是一个指针， 或者是一个 uint64_t 整数， 又或者是一个 int64_t 整数。 next 属性是指向另一个哈希表节点的指针， 这个指针可以将多个哈希值相同的键值对连接在一次， 以此来解决键冲突（collision）的问题。 Redis 中的字典由 dict.h/dict 结构表示： typedef struct dict { // 类型特定函数 dictType *type; // 私有数据 void *privdata; // 哈希表 dictht ht[2]; // rehash 索引 // 当 rehash 不在进行时，值为 -1 int rehashidx; /* rehashing not in progress if rehashidx == -1 */ } dict; type 属性和 privdata 属性是针对不同类型的键值对， 为创建多态字典而设置的： type 属性是一个指向 dictType 结构的指针， 每个 dictType 结构保存了一簇用于操作特定类型键值对的函数， Redis 会为用途不同的字典设置不同的类型特定函数。 而 privdata 属性则保存了需要传给那些类型特定函数的可选参数。 typedef struct dictType { // 计算哈希值的函数 unsigned int (*hashFunction)(const void *key); // 复制键的函数 void *(*keyDup)(void *privdata, const void *key); // 复制值的函数 void *(*valDup)(void *privdata, const void *obj); // 对比键的函数 int (*keyCompare)(void *privdata, const void *key1, const void *key2); // 销毁键的函数 void (*keyDestructor)(void *privdata, void *key); // 销毁值的函数 void (*valDestructor)(void *privdata, void *obj); } dictType; ht 属性是一个包含两个项的数组， 数组中的每个项都是一个 dictht 哈希表， 一般情况下， 字典只使用 ht[0] 哈希表， ht[1] 哈希表只会在对 ht[0] 哈希表进行 rehash 时使用。 除了 ht[1] 之外， 另一个和 rehash 有关的属性就是 rehashidx ： 它记录了 rehash 目前的进度， 如果目前没有在进行 rehash ， 那么它的值为 -1 。 哈希算法 当字典被用作数据库的底层实现， 或者哈希键的底层实现时， Redis 使用 MurmurHash2 算法来计算键的哈希值。 哈希冲突 链地址法 因为 dictEntry 节点组成的链表没有指向链表表尾的指针， 所以为了速度考虑， 程序总是将新节点添加到链表的表头位置（复杂度为 O(1)）， 排在其他已有节点的前面。 rehash 扩展和收缩哈希表的工作可以通过执行 rehash （重新散列）操作来完成， Redis 对字典的哈希表执行 rehash 的步骤如下： 为字典的ht[1]哈希表分配空间， 这个哈希表的空间大小取决于要执行的操作， 以及ht[0]当前包含的键值对数量 （也即是ht[0].used属性的值）： 如果执行的是扩展操作， 那么 ht[1] 的大小为第一个大于等于 ht[0].used * 2 的 2^n^ （2 的 n 次方幂）； 如果执行的是收缩操作， 那么 ht[1] 的大小为第一个大于等于 ht[0].used 的 2^n^ 。 将保存在 ht[0] 中的所有键值对 rehash 到 ht[1] 上面： rehash 指的是重新计算键的哈希值和索引值， 然后将键值对放置到 ht[1] 哈希表的指定位置上。 当 ht[0] 包含的所有键值对都迁移到了 ht[1] 之后 （ht[0] 变为空表）， 释放 ht[0] ， 将 ht[1] 设置为 ht[0] ， 并在 ht[1] 新创建一个空白哈希表， 为下一次 rehash 做准备。 扩容和收缩的条件 当以下条件中的任意一个被满足时， 程序会自动开始对哈希表执行扩展操作 服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 1 ； 服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 5 ； 其中哈希表的负载因子可以通过公式： # 负载因子 = 哈希表已保存节点数量 / 哈希表大小 load_factor = ht[0].used / ht[0].size 计算得出。 根据 BGSAVE 命令或 BGREWRITEAOF 命令是否正在执行， 服务器执行扩展操作所需的负载因子并不相同， 这是因为在执行 BGSAVE 命令或 BGREWRITEAOF 命令的过程中， Redis 需要创建当前服务器进程的子进程， 而大多数操作系统都采用写时复制（copy-on-write）技术来优化子进程的使用效率， 所以在子进程存在期间， 服务器会提高执行扩展操作所需的负载因子， 从而尽可能地避免在子进程存在期间进行哈希表扩展操作， 这可以避免不必要的内存写入操作， 最大限度地节约内存。 另一方面， 当哈希表的负载因子小于 0.1 时， 程序自动开始对哈希表执行收缩操作。 渐进式哈希 扩展或收缩哈希表需要将 ht[0] 里面的所有键值对 rehash 到 ht[1] 里面， 但是， 这个 rehash 动作并不是一次性、集中式地完成的， 而是分多次、渐进式地完成的。 这样做的原因在于， 如果 ht[0] 里只保存着四个键值对， 那么服务器可以在瞬间就将这些键值对全部 rehash 到 ht[1] ； 但是， 如果哈希表里保存的键值对数量不是四个， 而是四百万、四千万甚至四亿个键值对， 那么要一次性将这些键值对全部 rehash 到 ht[1] 的话， 庞大的计算量可能会导致服务器在一段时间内停止服务。 因此， 为了避免 rehash 对服务器性能造成影响， 服务器不是一次性将 ht[0] 里面的所有键值对全部 rehash 到 ht[1] ， 而是分多次、渐进式地将 ht[0] 里面的键值对慢慢地 rehash 到 ht[1] 。 以下是哈希表渐进式 rehash 的详细步骤： 为 ht[1] 分配空间， 让字典同时持有 ht[0] 和 ht[1] 两个哈希表。 在字典中维持一个索引计数器变量 rehashidx ， 并将它的值设置为 0 ， 表示 rehash 工作正式开始。 在 rehash 进行期间， 每次对字典执行添加、删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ， 当 rehash 工作完成之后， 程序将 rehashidx 属性的值增一。 随着字典操作的不断执行， 最终在某个时间点上， ht[0] 的所有键值对都会被 rehash 至 ht[1] ， 这时程序将 rehashidx 属性的值设为 -1 ， 表示 rehash 操作已完成。 渐进式 rehash 的好处在于它采取分而治之的方式， 将 rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量。 渐进式 rehash 执行期间的哈希表操作 因为在进行渐进式 rehash 的过程中， 字典会同时使用 ht[0] 和 ht[1] 两个哈希表， 所以在渐进式 rehash 进行期间， 字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行： 比如说， 要在字典里面查找一个键的话， 程序会先在 ht[0] 里面进行查找， 如果没找到的话， 就会继续到 ht[1] 里面进行查找， 诸如此类。 另外， 在渐进式 rehash 执行期间， 新添加到字典的键值对一律会被保存到 ht[1] 里面， 而 ht[0] 则不再进行任何添加操作： 这一措施保证了 ht[0] 包含的键值对数量会只减不增， 并随着 rehash 操作的执行而最终变成空表。 小结 字典被广泛用于实现 Redis 的各种功能， 其中包括数据库和哈希键。 Redis 中的字典使用哈希表作为底层实现， 每个字典带有两个哈希表， 一个用于平时使用， 另一个仅在进行 rehash 时使用。 当字典被用作数据库的底层实现， 或者哈希键的底层实现时， Redis 使用 MurmurHash2 算法来计算键的哈希值。 哈希表使用链地址法来解决键冲突， 被分配到同一个索引上的多个键值对会连接成一个单向链表。 在对哈希表进行扩展或者收缩操作时， 程序需要将现有哈希表包含的所有键值对 rehash 到新哈希表里面， 并且这个 rehash 过程并不是一次性地完成的， 而是渐进式地完成的。 整数集合 结构 整数集合（intset）是 Redis 用于保存整数值的集合抽象数据结构， 它可以保存类型为 int16_t 、 int32_t 或者 int64_t 的整数值， 并且保证集合中不会出现重复元素。 每个 intset.h/intset 结构表示一个整数集合： typedef struct intset { // 编码方式 uint32_t encoding; // 集合包含的元素数量 uint32_t length; // 保存元素的数组 int8_t contents[]; } intset; contents 数组是整数集合的底层实现： 整数集合的每个元素都是 contents 数组的一个数组项（item）， 各个项在数组中按值的大小从小到大有序地排列， 并且数组中不包含任何重复项。 length 属性记录了整数集合包含的元素数量， 也即是 contents 数组的长度。 虽然 intset 结构将 contents 属性声明为 int8_t 类型的数组， 但实际上 contents 数组并不保存任何 int8_t 类型的值 —— contents 数组的真正类型取决于 encoding 属性的值： 如果 encoding 属性的值为 INTSET_ENC_INT16 ， 那么 contents 就是一个 int16_t 类型的数组， 数组里的每个项都是一个 int16_t 类型的整数值 （最小值为 -32,768 ，最大值为 32,767 ）。 如果 encoding 属性的值为 INTSET_ENC_INT32 ， 那么 contents 就是一个 int32_t 类型的数组， 数组里的每个项都是一个 int32_t 类型的整数值 （最小值为 -2,147,483,648 ，最大值为 2,147,483,647 ）。 如果 encoding 属性的值为 INTSET_ENC_INT64 ， 那么 contents 就是一个 int64_t 类型的数组， 数组里的每个项都是一个 int64_t 类型的整数值 （最小值为 -9,223,372,036,854,775,808 ，最大值为 9,223,372,036,854,775,807 ）。 升级 每当我们要将一个新元素添加到整数集合里面， 并且新元素的类型比整数集合现有所有元素的类型都要长时， 整数集合需要先进行升级（upgrade）， 然后才能将新元素添加到整数集合里面。 升级整数集合并添加新元素共分为三步进行： 根据新元素的类型， 扩展整数集合底层数组的空间大小， 并为新元素分配空间。 将底层数组现有的所有元素都转换成与新元素相同的类型， 并将类型转换后的元素放置到正确的位上， 而且在放置元素的过程中， 需要继续维持底层数组的有序性质不变。（后移） 将新元素添加到底层数组里面。 升级的好处 提升灵活性 节约内存 降级 只可升级，不可降级 小结 整数集合是集合键的底层实现之一。 整数集合的底层实现为数组， 这个数组以有序、无重复的方式保存集合元素， 在有需要时， 程序会根据新添加元素的类型， 改变这个数组的类型。 升级操作为整数集合带来了操作上的灵活性， 并且尽可能地节约了内存。 整数集合只支持升级操作， 不支持降级操作。 压缩链表 结构 previous_entry_length 节点的 previous_entry_length 属性以字节为单位， 记录了压缩列表中前一个节点的长度。 previous_entry_length 属性的长度可以是 1 字节或者 5 字节： 如果前一节点的长度小于 254 字节， 那么 previous_entry_length 属性的长度为 1 字节： 前一节点的长度就保存在这一个字节里面。 如果前一节点的长度大于等于 254 字节， 那么 previous_entry_length 属性的长度为 5 字节： 其中属性的第一字节会被设置为 0xFE （十进制值 254）， 而之后的四个字节则用于保存前一节点的长度。 encoding 节点的 encoding 属性记录了节点的 content 属性所保存数据的类型以及长度： 一字节、两字节或者五字节长， 值的最高位为 00 、 01 或者 10 的是字节数组编码： 这种编码表示节点的 content 属性保存着字节数组， 数组的长度由编码除去最高两位之后的其他位记录； 一字节长， 值的最高位以 11 开头的是整数编码： 这种编码表示节点的 content 属性保存着整数值， 整数值的类型和长度由编码除去最高两位之后的其他位记录； content 节点的 content 属性负责保存节点的值， 节点值可以是一个字节数组或者整数， 值的类型和长度由节点的 encoding 属性决定。 连锁更新 每个节点的 previous_entry_length 属性都记录了前一个节点的长度： 如果前一节点的长度小于 254 字节， 那么 previous_entry_length 属性需要用 1 字节长的空间来保存这个长度值。 如果前一节点的长度大于等于 254 字节， 那么 previous_entry_length 属性需要用 5 字节长的空间来保存这个长度值。 现在， 考虑这样一种情况： 在一个压缩列表中， 有多个连续的、长度介于 250 字节到 253 字节之间的节点 e1 至 eN ，因为 e1 至 eN 的所有节点的长度都小于 254 字节， 所以记录这些节点的长度只需要 1 字节长的 previous_entry_length 属性， 换句话说， e1 至 eN 的所有节点的 previous_entry_length 属性都是 1 字节长的。 这时， 如果我们将一个长度大于等于 254 字节的新节点 new 设置为压缩列表的表头节点， 那么 new 将成为 e1 的前置节点，因为 e1 的 previous_entry_length 属性仅长 1 字节， 它没办法保存新节点 new 的长度， 所以程序将对压缩列表执行空间重分配操作， 并将 e1 节点的 previous_entry_length 属性从原来的 1 字节长扩展为 5 字节长。 现在， 麻烦的事情来了 —— e1 原本的长度介于 250 字节至 253 字节之间， 在为 previous_entry_length 属性新增四个字节的空间之后， e1 的长度就变成了介于 254 字节至 257 字节之间， 而这种长度使用 1 字节长的 previous_entry_length 属性是没办法保存的。 因此， 为了让 e2 的 previous_entry_length 属性可以记录下 e1 的长度， 程序需要再次对压缩列表执行空间重分配操作， 并将 e2 节点的 previous_entry_length 属性从原来的 1 字节长扩展为 5 字节长。 正如扩展 e1 引发了对 e2 的扩展一样， 扩展 e2 也会引发对 e3 的扩展， 而扩展 e3 又会引发对 e4 的扩展……为了让每个节点的 previous_entry_length 属性都符合压缩列表对节点的要求， 程序需要不断地对压缩列表执行空间重分配操作， 直到 eN 为止。 Redis 将这种在特殊情况下产生的连续多次空间扩展操作称之为“连锁更新”（cascade update）。 因为连锁更新在最坏情况下需要对压缩列表执行 N 次空间重分配操作， 而每次空间重分配的最坏复杂度为 O(N) ， 所以连锁更新的最坏复杂度为 O(N^2) 。 要注意的是， 尽管连锁更新的复杂度较高， 但它真正造成性能问题的几率是很低的： 首先， 压缩列表里要恰好有多个连续的、长度介于 250 字节至 253 字节之间的节点， 连锁更新才有可能被引发， 在实际中， 这种情况并不多见； 其次， 即使出现连锁更新， 但只要被更新的节点数量不多， 就不会对性能造成任何影响： 比如说， 对三五个节点进行连锁更新是绝对不会影响性能的； 因为以上原因， ziplistPush 等命令的平均复杂度仅为 O(N) ， 在实际中， 我们可以放心地使用这些函数， 而不必担心连锁更新会影响压缩列表的性能。 小结 压缩列表是一种为节约内存而开发的顺序型数据结构。 压缩列表被用作列表键和哈希键的底层实现之一。 压缩列表可以包含多个节点，每个节点可以保存一个字节数组或者整数值。 添加新节点到压缩列表， 或者从压缩列表中删除节点， 可能会引发连锁更新操作， 但这种操作出现的几率并不高。 "},"docs/Guide/MySQL整理.html":{"url":"docs/Guide/MySQL整理.html","title":"MySQL整理","keywords":"","body":"[TOC] 索引 mysql一次查询只能使用一个索引。如果要对多个字段使用索引，建立复合(联合)索引。 索引的作用 索引主要有以下几个作用 即上述所说，索引能极大地减少扫描行数 索引可以帮助服务器避免排序和临时表 如果这张临时表的大小大于 tmp_table_size 的值（默认为 16 M），内存临时表会转为磁盘临时表，性能会更差，如果加了索引，索引本身是有序的 ，所以从磁盘读的行数本身就是按 age 排序好的，也就不会生成临时表，就不用再额外排序 ，无疑提升了性能。 索引可以将随机 IO 变成顺序 IO 随机 IO 和顺序 IO 大概相差百倍 (随机 IO：10 ms/ page, 顺序 IO 0.1ms / page)，可见顺序 IO 性能之高，索引带来的性能提升显而易见！ 索引的分类 Hash索引 HASH索引只有精确匹配索引所有列的查询才有效。 因为索引自身只需要存储对应的哈希值，所以索引的结构十分紧凑，这也让哈希索引查找的速度非常快，然而，哈希索引也有限制，如下： 哈希索引只包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行（即不能使用哈希索引来做覆盖索引扫描），不过，访问内存中的行的速度很快（因为memory引擎的数据都保存在内存里），所以大部分情况下这一点对性能的影响并不明显。 哈希索引数据并不是按照索引列的值顺序存储的，所以也就无法用于排序 哈希索引也不支持部分索引列匹配查找，因为哈希索引始终是使用索引的全部列值内容来计算哈希值的。如：数据列（a,b）上建立哈希索引，如果只查询数据列a，则无法使用该索引。 哈希索引只支持等值比较查询，如：=,in(),(注意，<>和是不同的操作)，不支持任何范围查询（必须给定具体的where条件值来计算hash值，所以不支持范围查询）。 访问哈希索引的数据非常快，除非有很多哈希冲突，当出现哈希冲突的时候，存储引擎必须遍历链表中所有的行指针，逐行进行比较，直到找到所有符合条件的行。 如果哈希冲突很多的话，一些索引维护操作的代价也很高，如：如果在某个选择性很低的列上建立哈希索引（即很多重复值的列），那么当从表中删除一行时，存储引擎需要遍历对应哈希值的链表中的每一行，找到并删除对应的引用，冲突越多，代价越大。 为什么在InnoDB创建HASH索引失败？ 下面是MYSQL官方的回答： 1.不支持HASH索引（但是InnoDB在内部利用哈希索引来实现其自适应哈希索引功能。） 2.也就是InnoDB会根据表的使用情况自动为表生成hash索引，不能人为干预是否在InnoDB一张表中创建HASH索引 3.或者说，如果InnoDB注意到某些索引值被使用的特别频繁时， 它会在内存中基于Btree的索引之上再创建一个HASH索引，这样BTREE索引也具备了HASH索引的一些优点 MEMORY引擎是支持的hash索引的。 B+树索引 B+ 树是以 N 叉树的形式存在的，这样有效降低了树的高度，查找数据也不需要全表扫描了，顺着根节点层层往下查找能很快地找到我们的目标数据，每个节点的大小即一个磁盘块的大小，一次 IO 会将一个页（每页包含多个磁盘块，innodb_page_size | 16384 = 16KB）的数据都读入（即磁盘预读，程序局部性原理:读到了某个值，很大可能这个值周围的数据也会被用到，干脆一起读入内存），叶子节点通过指针的相互指向连接，能有效减少顺序遍历时的随机 IO，而且我们也可以看到，叶子节点都是按索引的顺序排序好的，这也意味着根据索引查找或排序都是排序好了的，不会再在内存中形成临时表。 索引的使用情况 or查询到底走不走索引？ where 语句里面如果带有or条件, MyIsam表能用到索引， Innodb不行。 必须所有的or条件都必须是独立索引（建立在MyIsam引擎） 用UNION替换OR （适用于索引列，会产生临时表useing temporary） 用in来替换or（推荐） 联合索引——where走索引情况 SQL题：4个查询语句，分别判断走没走索引？ t idx(a,b,c) 1. select * from t where a=x and b=x; （走索引） 2. select * from t where c=x and a=x; （优化器优化顺序后，走索引，只走a） 3. select * from t where b=x and c=x; （不满足最左前缀） 4. select * from t where a>x and b=x; （走索引，只走a，因为a是范围查询，到它就会停止匹配） mysql会一直向右匹配直到遇到范围查询（>、 3 and d = 4，如果建立（a,b,c,d）顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 order by走索引情况 在ORDER BY操作中，MySQL只有在排序条件不是一个查询条件表达式的情况下才使用索引。 #1. 只order by 且sort有索引 SELECT [column1],[column2],…. FROM [TABLE] ORDER BY [sort]; #2. WHERE + ORDER BY的索引优化，联合索引(columnX,sort)来实现order by 优化 SELECT [column1],[column2],…. FROM [TABLE] WHERE [columnX] = [value] ORDER BY [sort]; #3. WHERE + 多个字段ORDER BY，联合索引(uid,x,y)实现order by的优化 SELECT * FROM [table] WHERE uid=1 ORDER x,y LIMIT 0,10; ORDER BY 优化 子句，尽量使用Index方式排序，避免使用FileSort方式排序 MySQL支持两种方式的排序，FileSort和Index,Index效率高。它指MySQL扫描索引本身完成排序。FileSort方式效率较低。 ORDER BY 满足两种情况，会使用Index方式排序 ①ORDER BY语句使用索引最左前列 ②使用WHERE 子句与ORDER BY子句条件列组合满足索引最左前列 提高ORDER BY速度 提高Order By的速度 Order By时select *是一个大忌，只Query需要的字段，这点非常重要。在这里的影响是： 1.1 当Query的字段大小总和小于max_length_for_sort_data而且排序字段不是TEXT|BLOB类型时，会用改进后的算法--------单路排序，否则用老算法一一多路排序。 1.2两种算法的数据都有可能超出sort_buffer的容量，超出之后，会创建tmp文件进行合并排序，导致多次I/O，但是用单路排序 算法的风险会更大一些，所以要提高sort_buffer_size。 尝试提高 sort_buffer_size 不管用哪种算法，提高这个参数都会提高效率，当然，要根据系统的能力去提高，因为这个参数是针对每个进程的。 尝试提高 max_length_for_sort_data 提高这个参数，会增加用改进算法的概率。但是如果设的太高，数据总容量超出sort_buffer_size的概率就增大，明显症状是高的磁盘I/O活动和低的处理器使用率。 GROUP BY 关键字优化 group by 实质是先排序后分组，遵照索引的最佳左前缀。 当无法使用索引列，增大max_length_for_sort_data参数的设置+增大sort_buffer_size参数的设置 where 高于having，能写在where限定的条件就不要去having去限定了。 MySql（二十）--为排序使用索引Order By优化_csdn_kenneth的博客-CSDN博客_mysql 索引排序 疑问： 1. where key_1 = 1 and key_2 = 2 order by key_1 走索引吗（a,b,a）情况，索引是联合索引（a,b） 2. 联合索引结构是什么样子：非叶子节点存储联合索引Key还是最左第一个的key ? （好像是存储是联合的key） 不走索引的情况 #1. key1,key2分别建立索引 SELECT * FROM t1 WHERE key2=constant ORDER BY key1; SELECT * FROM t1 ORDER BY key1, key2; #2. key_part1,key_part2建立联合索引;key2建立索引 SELECT * FROM t1 WHERE key2=constant ORDER BY key_part2; #3. key_part1,key_part2建立联合索引,同时使用了 ASC 和 DESC SELECT * FROM t1 ORDER BY key_part1 DESC, key_part2 ASC; # 在8.0之前的版本中, DESC 是无效的，索引 (a ASC, b DESC, c DESC) 等于 (a ASC, b ASC, c ASC)，故而无法使用整个联合索引进行排序。 # 8.0之后允许索引降序，抛开 sql 优化等细节，只要 order by 顺序和索引顺序一致，那么还是可以用到索引排序的。 #4. 如果在WHERE和ORDER BY的栏位上应用表达式(函数)时，则无法利用索引来实现order by的优化 SELECT * FROM t1 ORDER BY YEAR(logindate) LIMIT 0,10; 又说法是group by c1 | order by c1，由于没有where的铺垫，不使用任何索引。 分页优化 select * from orders_history where type = 8 and id >= ( select id from orders_history where type = 8 limit 100000,1 ) limit 100; # 子查询走主键ID聚簇索引比较快的找出深分页的那个主键ID，然后再查别的 sql优化之大数据量分页查询（mysql） - 杨冠标 - 博客园 Explain再深入 一张图彻底搞定 explain | MySQL 技术论坛 Explain详解与索引优化实践_数据库技术_Linux公社-Linux系统门户网站 mysql调优--根据explain结果分析索引有效性,正确使用索引_数据库_嘎嘎的博客-CSDN博客 你确定真正理解联合索引和最左前缀原则？_数据库_weixin_44476888的博客-CSDN博客 一本彻底搞懂MySQL索引优化EXPLAIN百科全书 - 个人文章 - SegmentFault 思否 Explain详解与索引最佳实践 - 简书 MySQL中explain执行计划中额外信息字段(Extra)详解_poxiaonie的博客-CSDN博客_explain extra 疑问： Explain中的type字段，为ref的时候，我们知道是肯定是走索引的，但是为 range、index的时候走不走索引啊？？？ 索引合并 索引下推 创建索引的原则 列的离散型（区分度）： 离散型的计算公式：count(distinct col) : count(col)，离散型越高，选择型越好。 最左匹配原则并且优先创建联合索引原则 覆盖索引 两个字段都建立了索引，会使用哪一个？_诚-CSDN博客 前缀索引 # 3,4,5,6,7为city字段的前缀长度，通过这个来比较索引选择性提升的比例变化 SELECT COUNT(DISTINCT LEFT(city,3))/COUNT(*) as sel3, COUNT(DISTINCT LEFT(city,4))/COUNT(*) as sel4, COUNT(DISTINCT LEFT(city,5))/COUNT(*) as sel5, COUNT(DISTINCT LEFT(city,6))/COUNT(*) as sel6, COUNT(DISTINCT LEFT(city,7))/COUNT(*) as sel7 FROM city_demo 占用空间小且快 无法使用前缀索引做 ORDER BY 和 GROUP BY 无法使用前缀索引做覆盖扫描 有可能增加扫描行数 比如身份证加索引，可以加哈希索引或者倒序存储后加前缀索引。 联合索引 上面的创建规则同样适用于联合索引。 # staff_id_selectivity: 0.0001 # customer_id_selectivity: 0.0373 # COUNT(*): 16049 # 通过结果发现，customer_id 的选择性更高，所以应该选择 customer_id 作为联合索引的第一列 SELECT COUNT(DISTINCT staff_id)/COUNT(*) as staff_id_selectivity, COUNT(DISTINCT customer_id)/COUNT(*) as customer_id_selectivity, COUNT(*) FROM payment 索引的长度计算 1. 所有的索引字段，如果没有设置not null，则需要加一个字节。 2. 定长字段，int占四个字节、date占三个字节、char(n)占n个字符。 3. 对于变成字段varchar(n)，则有n个字符+两个字节。 4. 不同的字符集，一个字符占用的字节数不同。latin1编码的，一个字符占用一个字节，gbk编码的，一个字符占用两个字节，utf8编码的，一个字符占用三个字节。 5. 索引长度 char()、varchar()索引长度的计算公式： (Character Set：utf8mb4=4,utf8=3,gbk=2,latin1=1) * 列长度 + 1(允许null) + 2(变长列) 索引的长度限制 在MySQL5.6里默认 innodb_large_prefix=0 限制单列索引长度不能超过767bytes 在MySQL5.7里默认 innodb_large_prefix=1 解除了767bytes长度限制，但是单列索引长度最大还是不能超过3072bytes 为什么3072，原因如下： 我们知道InnoDB一个page的默认大小是16k。由于是Btree组织，要求叶子节点上一个page至少要包含两条记录（否则就退化链表了）。 所以一个记录最多不能超过8k。 又由于InnoDB的聚簇索引结构，一个二级索引要包含主键索引，因此每个单个索引不能超过4k （极端情况，primay-key和某个二级索引都达到这个限制）。 由于需要预留和辅助空间，扣掉后不能超过3500，取个“整数”就是 (1024bytes*3=3072bytes) MySQL 中索引的长度的限制 事务 事务的四大特性ACID 原子性(Atomicity) 原子性是指事务包含的所有操作要么全部成功,要么全部失败回滚。失败回滚的操作事务,将不能对事务有任何影响。 一致性(Consistency) 一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态,也就是说一个事务执行之前和执行之后都必须处于一致性状态。 例如: A和B进行转账操作,A有200块钱,B有300块钱;当A转了100块钱给B之后,他们2个人的总额还是500块钱,不会改变。 隔离性(Isolation) 隔离性是指当多个用户并发访问数据库时,比如同时访问一张表,数据库每一个用户开启的事务,不能被其他事务所做的操作干扰(也就是事务之间的隔离),多个并发事务之间,应当相互隔离。 例如:同时有T1和T2两个并发事务,从T1角度来看,T2要不在T1执行之前就已经结束,要么在T1执行完成后才开始。将多个事务隔离开,每个事务都不能访问到其他事务操作过程中的状态;就好比上锁操作,只有一个事务做完了,另外一个事务才能执行。 持久性(Durability) 持久性是指事务的操作,一旦提交,对于数据库中数据的改变是永久性的,即使数据库发生故障也不能丢失已提交事务所完成的改变。 事务的隔离级别 mysql默认的隔离级别是:可重复读。 oracle中只支持2个隔离级别:读已提交和串行化, 默认是读已提交。 隔离级别的设置只对当前链接有效; 未提交读(READ UNCOMMITTED) 未提交事务隔离级别满足一级封锁协议, 即写数据的时候添加一个X锁(排他锁),也就是在写数据的时候不允许其他事务进行写操作,但是读不受限制,读不加锁。 这样就可以解决了多个人一起写数据而导致了”数据丢失”的问题,但是会引发新的问题——脏读。 脏读:读取了别人未提交的数据。 读已提交(READ COMMITTED) 读已提交满足二级封锁协议, 即写数据的时候加上X锁(排他锁),读数据的时候添加S锁(共享锁),且如果一个数据加了X锁就没法加S锁;同理如果加了S锁就没法加X锁,但是一个数据可以同时存在多个S锁(因为只是读数据),并且规定S锁读取数据,一旦读取完成就立刻释放S锁(不管后续是否还有很多其他的操作,只要是读取了S锁的数据后,就立刻释放S锁)。 这样就解决了脏读的问题,但是又有新的问题出现——不可重复读。 不可重复读:同一个事务对数据的多次读取的结果不一致。 可重复读(REPEATABLE READ) 可重复读满足第三级封锁协议, 即对S锁进行修改,之前的S锁是:读取了数据之后就立刻释放S锁,现在修改是:在读取数据的时候加上S锁,但是要直到事务准备提交了才释放该S锁,X锁还是一致。 这样就解决了不可重复读的问题了,但是又有新的问题出现——幻读。 可串行化(SERIALIZABLE) 事务只能一件一件的进行,不能并发进行。 隔离级别 数据丢失 脏读 不可重复读 幻读 读未提交 ❌ ✅ ✅ ✅ 读已提交 ❌ ❌ ✅ ✅ 可重复读 ❌ ❌ ❌ ✅ 可串行化 ❌ ❌ ❌ ❌ mysql专题16 事务隔离级别及ACID知识回顾 | 一线攻城狮 MySQL 事务的四种隔离级别 （图示每一步的操作），非常重要_码神龙-CSDN博客_数据库隔离级别图 小结下MVCC、快照读、当前读和事务隔离级别（RC、RR）之间的联系： 这两个隔离级别的一个很大不同就是：生成ReadView的时机不同，READ COMMITTD在每一次进行普通SELECT操作前都会生成一个ReadView，而REPEATABLE READ只在第一次进行普通SELECT操作前生成一个ReadView，数据的可重复读其实就是ReadView的重复使用。 所以为什么RR可重复读就在于上面说的生成ReadView时机不同、并且在更新操作的时候使用的是当前读所以可以对最新数据进行操作。 MVCC机制 MVCC，Multi-Version Concurrency Control，多版本并发控制。MVCC 是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问；在编程语言中实现事务内存。 MVCC 使用了一种不同的手段，每个连接到数据库的读者，在某个瞬间看到的是数据库的一个快照，写者写操作造成的变化在写操作完成之前（或者数据库事务提交之前）对于其他的读者来说是不可见的。 数据库默认隔离级别：RR（Repeatable Read，可重复读），MVCC主要适用于Mysql的RC（读取已提交）,RR（可重复读）隔离级别 MVCC就是行级锁的一个变种(升级版)。 在表锁中我们读写是阻塞的，基于提升并发性能的考虑，MVCC一般读写是不阻塞的(所以说MVCC很多情况下避免了加锁的操作) MVCC实现的读写不阻塞正如其名：多版本并发控制--->通过一定机制生成一个数据请求时间点的一致性数据快照（Snapshot)，并用这个快照来提供一定级别（语句级或事务级）的一致性读取。从用户的角度来看，好像是数据库可以提供同一数据的多个版本。 快照有两个级别： 语句级 针对于Read committed隔离级别 事务级别 针对于Repeatable read隔离级别 各种事务隔离级别下的Read view 工作方式 RC(read commit) 级别下同一个事务里面的每一次查询都会获得一个新的read view副本。这样就可能造成同一个事务里前后读取数据可能不一致的问题（幻读） RR(重复读)级别下的一个事务里只会获取一次read view副本，从而保证每次查询的数据都是一样的。 READ_UNCOMMITTED 级别的事务不会获取read view 副本。 所谓的MVCC（Multi-Version Concurrency Control ，多版本并发控制）指的就是在使用读已提交（READ COMMITTD）、可重复读（REPEATABLE READ）这两种隔离级别的事务在执行普通的SELECT操作时访问记录的版本链的过程，这样子可以使不同事务的读-写、写-读操作并发执行，从而提升系统性能。 这两个隔离级别的一个很大不同就是：生成ReadView的时机不同，READ COMMITTD在每一次进行普通SELECT操作前都会生成一个ReadView，而REPEATABLE READ只在第一次进行普通SELECT操作前生成一个ReadView，数据的可重复读其实就是ReadView的重复使用。 快照读和当前读 快照读 快照读是指读取数据时不是读取最新版本的数据，而是基于历史版本读取的一个快照信息（mysql读取undo log历史版本) ，快照读可以使普通的SELECT 读取数据时不用对表数据进行加锁，从而解决了因为对数据库表的加锁而导致的两个如下问题 1、解决了因加锁导致的修改数据时无法对数据读取问题; 2、解决了因加锁导致读取数据时无法对数据进行修改的问题; 在一个支持MVCC并发控制的系统中，哪些读操作是快照读？哪些操作又是当前读呢？以mysql InnoDB为例: 快照读:简单的select操作，属于快照读，不加锁。 select * from table where ?; 当前读:特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。 select * from table where ? lock in share mode; # S锁 (共享锁) select * from table where ? for update; # X锁 (排它锁) insert into table values (…); update table set ? where ?; delete from table where ?; 所有以上的语句，都属于当前读，读取记录的最新版本。并且，读取之后，还需要保证其他并发事务不能修改当前记录，对读取记录加锁。其中，除了第一条语句，对读取记录加S锁 (共享锁)外，其他的操作，都加的是X锁 (排它锁)。 对于快照读来说，幻读的解决是依赖mvcc解决。而对于当前读则依赖于gap-lock解决。 当前读 当前读是读取的数据库最新的数据，当前读和快照读不同，因为要读取最新的数据而且要保证事务的隔离性，所以当前读是需要对数据进行加锁的（Update delete insert select ....lock in share mode select for update 为当前读） 总之在RC隔离级别下，是每个快照读都会生成并获取最新的Read View；而在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View, 之后的快照读获取的都是同一个Read View。 【MySQL（5）| 五分钟搞清楚 MVCC 机制】 - 掘金 MYSQL MVCC实现原理 - 简书 MySQL InnoDB MVCC 机制的原理及实现 - 知乎 innodb MVCC实现原理 - 知乎 阿里面试问MVCC,原来是这么回答的 - 掘金 redo log 、 undo log、bin log MySQL中有六种日志文件，分别是：重做日志（redo log）、回滚日志（undo log）、二进制日志（binlog）、错误日志（errorlog）、慢查询日志（slow query log）、一般查询日志（general log），中继日志（relay log）。 其中重做日志和回滚日志与事务操作息息相关，二进制日志也与事务操作有一定的关系，这三种日志，对理解MySQL中的事务操作有着重要的意义。这里简单总结一下这三者具有一定相关性的日志。 mysql日志系统 SQL 逻辑日志 物理日志 - binyang - 博客园 MySQL中的重做日志（redo log），回滚日志（undo log），以及二进制日志（binlog）的简单总结 redo log（重做日志） 作用： 　　确保事务的持久性。 　　防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redo log进行重做，从而达到事务的持久性这一特性。 redo log是MySQL的 InnoDB引擎所特有产生的。 在修改的数据的时候，redo log也会记载着变更的内容，binlog会记载着变更的内容。（只不过一个存储的是物理变化，一个存储的是逻辑变化）。那他们的写入顺序是什么样的呢？ redo log事务开始的时候，就开始记录每次的变更信息，而binlog是在事务提交的时候才记录。 MySQL通过两阶段提交来保证redo log和binlog的数据是一致的。 过程： 阶段1：InnoDBredo log 写盘，InnoDB 事务进入 prepare 状态 阶段2：binlog 写盘，InooDB 事务进入 commit 状态 每个事务binlog的末尾，会记录一个 XID event，标志着事务是否提交成功，也就是说，恢复过程中，binlog 最后一个 XID event 之后的内容都应该被 purge。 InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么这块\"粉板\"总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写。 有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 bin log（二进制日志） binlog的作用是复制和恢复而生的。 主从服务器需要保持数据的一致性，通过binlog来同步数据。 如果整个数据库的数据都被删除了，binlog存储着所有的数据变更情况，那么可以通过binlog来对数据进行恢复。 说说MySQL中的Redo log Undo log都在干啥 - 苏家小萝卜 - 博客园 两种日志特点对比： redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。 redo log是物理日志，记录的是\"在某个数据页上做了什么修改\"；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如\"给ID=2这一行的c字段加1 \"。 redo log是循环写的，空间固定会用完；binlog是可以追加写入的。\"追加写\"是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志 为什么日志需要\"两阶段提交？ ​ 重点： ​ Binlog 的完成在redolog 的prepare和commit之间 ​ 在binlog之前 redolog已经准备好在内存中，但是未写入磁盘 ​ 在binlog之后 redolog 才处于提交状态准备写入磁盘。 ​ Redolog 和binlog是两个独立的阶段，并不依赖 ​ 即，数据已经在内存中修改完毕，修改数据的操作也记录完了，但是数据库的磁盘文件还没有真正写入。 假设当前ID=2的行，字段c的值是0，再假设执行update语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash： 先写redo log后写binlog。假设在redo log写完，binlog还没有写完的时候，MySQL进程异常重启。由于我们前面说过的，redo log写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行c的值是1。 但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份日志的时候，存起来的binlog里面就没有这条语句。 然后你会发现，如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同。 先写binlog后写redo log。如果在binlog写完之后crash，由于redo log还没写，崩溃恢复以后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了\"把c从0改成1\"这个日志。所以，在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是1，与原库的值不同。 注意 redo log用于保证crash-safe能力。innodb_flush_log_at_trx_commit这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。这个参数建议你设置成1，这样可以保证MySQL异常重启之后数据不丢失。 sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。这个参数建议你设置成1，这样可以保证MySQL异常重启之后binlog不丢失。 详细分析MySQL事务日志(redo log和undo log) - 骏马金龙 - 博客园 MySQL之Redo Log - 知乎 undo log（回滚日志） undo log主要有两个作用：回滚和多版本控制(MVCC)，记录了一条修改的反的操作日志。保证事务的【原子性】 undo日志用于存放数据修改被修改前的值，假设修改 tba 表中 id=2的行数据，把Name='B' 修改为Name = 'B2' ，那么undo日志就会用来存放Name='B'的记录，如果这个修改出现异常，可以使用undo日志来实现回滚操作，保证事务的一致性。 对数据的变更操作，主要来自 INSERT UPDATE DELETE，而UNDO LOG中分为两种类型，一种是 INSERT_UNDO（INSERT操作），记录插入的唯一键值；一种是 UPDATE_UNDO（包含UPDATE及DELETE操作），记录修改的唯一键值以及old column记录。 MVCC实现的是读写不阻塞，读的时候只要返回前一个版本的数据就行了。 什么时候产生： 　　事务开始之前，将当前是的版本生成undo log，undo 也会产生 redo 来保证undo log的可靠性 什么时候释放： 　　当事务提交之后，undo log并不能立马被删除， 　　而是放入待清理的链表，由purge线程判断是否由其他事务在使用undo段中表的上一个事务之前的版本信息，决定是否可以清理undo log的日志空间。 数据库的备份和恢复怎么实现的，主从复制怎么做的，什么时候会出现数据不一致，如何解决。 (1)首先，mysql主库在事务提交时会把数据库变更作为事件Events记录在二进制文件binlog中；mysql主库上的sys_binlog控制binlog日志刷新到磁盘。 (2)主库推送二进制文件binlog中的事件到从库的中继日志relay log,之后从库根据中继日志重做数据库变更操作。通过逻辑复制，以此来达到数据一致。 ​ Mysql通过3个线程来完成主从库之间的数据复制：其中BinLog Dump线程跑在主库上，I/O线程和SQL线程跑在从库上。当从库启动复制（start slave）时，首先创建I/O线程连接主库，主库随后创建Binlog Dump线程读取数据库事件并发给I/O线程，I/O线程获取到数据库事件更新到从库的中继日志Realy log中去，之后从库上的SQL线程读取中继日志relay log 中更新的数据库事件并应用。 binlog 主库中可以设置binlog，binlog是主库中保存更新事件日志的二进制文件。 主节点 binary log dump 线程 如上图所示：当从节点连接主节点时，主节点会创建一个log dump 线程，用于发送bin-log的内容。在读取bin-log中的操作时，此线程会对主节点上的bin-log加锁，当读取完成，甚至在发动给从节点之前，锁会被释放。 从节点I/O线程 当从节点上执行start slave命令之后，从节点会创建一个I/O线程用来连接主节点，请求主库中更新的bin-log。I/O线程接收到主节点binlog dump 进程发来的更新之后，保存在本地relay-log中。 从节点SQL线程 SQL线程负责读取relay log中的内容，解析成具体的操作并执行，最终保证主从数据的一致性。 对于每一个主从连接，都需要三个进程来完成。当主节点有多个从节点时，主节点会为每一个当前连接的从节点建一个binary log dump 进程，而每个从节点都有自己的I/O进程，SQL进程。从节点用两个线程将从主库拉取更新和执行分成独立的任务，这样在执行同步数据任务的时候，不会降低读操作的性能。比如，如果从节点没有运行，此时I/O进程可以很快从主节点获取更新，尽管SQL进程还没有执行。如果在SQL进程执行之前从节点服务停止，至少I/O进程已经从主节点拉取到了最新的变更并且保存在本地relay日志中，当服务再次起来之后，就可以完成数据的同步。 1.节点上的I/O 进程连接主节点，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容； 2.主节点接收到来自从节点的I/O请求后，通过负责复制的I/O进程根据请求信息读取指定日志指定位置之后的日志信息，返回给从节点。返回信息中除了日志所包含的信息之外，还包括本次返回的信息的bin-log file 的以及bin-log position； 3.从节点的I/O进程接收到内容后，将接收到的日志内容更新到本机的relay log中，并将读取到的binary log文件名和位置保存到master-info 文件中，以便在下一次读取的时候能够清楚的告诉Master“我需要从某个bin-log 的哪个位置开始往后的日志内容，请发给我”； 4.Slave 的 SQL线程检测到relay-log 中新增加了内容后，会将relay-log的内容解析成在祝节点上实际执行过的操作，并在本数据库中执行。 主从不一致的情况 详细分析MySQL事务日志(redo log和undo log) - 后端 - 掘金 原来MySQL面试还会问这些... 锁 如果没有索引，所以update会锁表，如果加了索引，就会锁行。 锁粒度 mysql不同存储引擎支持的锁粒度不同, InnoDB存储引擎支持表锁及行锁, InnoDB存储引擎可支持三种行锁定方式, 默认加锁方式是next-key 锁。 行锁(Record Lock):锁直接加在索引记录上面，锁住的是key。 行锁又分为共享锁(S)与排他锁(X); 间隙锁(Gap Lock):锁定索引记录间隙，确保索引记录的间隙不变。间隙锁是针对事务隔离级别为可重复读或以上级别而已的。 Next-Key Lock: 行锁和间隙锁组合起来就叫Next-Key Lock。 默认情况下，InnoDB工作在可重复读(Repeatable Read)隔离级别下，并且会以Next-Key Lock的方式对数据行进行加锁，这样可以有效防止幻读的发生。Next-Key Lock是行锁和间隙锁的组合，当InnoDB扫描索引记录的时候，会首先对索引记录加上行锁（Record Lock），再对索引记录两边的间隙加上间隙锁（Gap Lock）。加上间隙锁之后，其他事务就不能在这个间隙修改或者插入记录。 首先，从锁的粒度，我们可以分成两大类： 表锁 开销小，加锁快；不会出现死锁；锁定力度大，发生锁冲突概率高，并发度最低 行锁 开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率低，并发度高 不同的存储引擎支持的锁粒度是不一样的： InnoDB行锁和表锁都支持！ MyISAM只支持表锁！ InnoDB只有通过索引条件检索数据才使用行级锁，否则，InnoDB将使用表锁 也就是说，InnoDB的行锁是基于索引的！ 表锁下又分为两种模式： 表读锁（Table Read Lock） 表写锁（Table Write Lock） 从下图可以清晰看到，在表读锁和表写锁的环境下： 读读不阻塞，读写阻塞，写写阻塞 读读不阻塞：当前用户在读数据，其他的用户也在读数据，不会加锁 读写阻塞：当前用户在读数据，其他的用户不能修改当前用户读的数据，会加锁！ 写写阻塞：当前用户在修改数据，其他的用户不能修改当前用户正在修改的数据，会加锁！ 我们应该更加关注行锁的内容，因为InnoDB一大特性就是支持行锁！ InnoDB实现了以下两种类型的行锁。 共享锁（S锁）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。 也叫做读锁：读锁是共享的，多个客户可以同时读取同一个资源，但不允许其他客户修改。 排他锁（X锁)：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。 也叫做写锁：写锁是排他的，写锁会阻塞其他的写锁和读锁。 看完上面的有没有发现，在一开始所说的：X锁，S锁，读锁，写锁，共享锁，排它锁其实总共就两个锁，只不过它们有多个名字罢了~~~ 另外，为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁： 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。 意向锁也是数据库隐式帮我们做了，不需要程序员操心！ 事务的隔离级别就是通过锁的机制来实现，只不过隐藏了加锁细节 乐观锁和悲观锁 无论是Read committed还是Repeatable read隔离级别，都是为了解决读写冲突的问题。 悲观锁 所以，按照上面的例子。我们使用悲观锁的话其实很简单(手动加行锁就行了)： select * from xxxx for update 在select 语句后边加了 for update相当于加了排它锁(写锁)，加了写锁以后，其他的事务就不能对它修改了！需要等待当前事务修改完之后才可以修改. 乐观锁 乐观锁不是数据库层面上的锁，是需要自己手动去加的锁。一般我们添加一个版本字段来实现： 张三select * from table --->会查询出记录出来，同时会有一个version字段 间隙锁GAP 当我们用范围条件检索数据而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合范围条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”。InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁。 值得注意的是：间隙锁只会在Repeatable read隔离级别下使用~ Select * from emp where empid > 100 for update; 上面是一个范围查询，InnoDB不仅会对符合条件的empid值为101的记录加锁，也会对empid大于101（这些记录并不存在）的“间隙”加锁。 InnoDB使用间隙锁的目的有两个： 为了防止幻读(上面也说了，Repeatable read隔离级别下再通过GAP锁即可避免了幻读) 满足恢复和复制的需要 MySQL的恢复机制要求：在一个事务未提交前，其他并发事务不能插入满足其锁定条件的任何记录，也就是不允许出现幻读 要禁止间隙锁的话，可以把隔离级别降为Read Committed，或者开启参数innodb_locks_unsafe_for_binlog。 死锁 并发的问题就少不了死锁，在MySQL中同样会存在死锁的问题。 但一般来说MySQL通过回滚帮我们解决了不少死锁的问题了，但死锁是无法完全避免的，可以通过以下的经验参考，来尽可能少遇到死锁： 1）以固定的顺序访问表和行。比如对两个job批量更新的情形，简单方法是对id列表先排序，后执行，这样就避免了交叉等待锁的情形；将两个事务的sql顺序调整为一致，也能避免死锁。 2）大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。 3）在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。 4）降低隔离级别。如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。 5）为表添加合理的索引。可以看到如果不走索引将会为表的每一行记录添加上锁，死锁的概率大大增大。 数据库两大神器【索引和锁】 - 掘金 数据库常见死锁原因及处理_数据库_zhoxing-CSDN博客 解决死锁之路（终结篇） - 再见死锁 - aneasystone's blog MySQL死锁解决之道 - 知乎 Mysql加锁过程详解_天空之城-CSDN博客 超全面MySQL语句加锁分析（上篇） 底层表的存储结构 varchar相关 varchar(n)，n表示什么？ MySQL5.0.3之前varchar(n)这里的n表示字节数 MySQL5.0.3之后varchar(n)这里的n表示字符数，比如varchar（200），不管是英文还是中文都可以存放200个 n最大可以是多少 MySQL行长度 MySQL要求一个行定义长度不能超过65535个字节，不包括text、blob等大字段类型，varchar长度受此长度限制，和其他非大字段加起来不能超过65535个字节. 超过以上限制则会报错 varchar(n)占用几个字节 varchar(n)占用几个字节跟字符集有关系： 字符类型若为gbk，每个字符占用2个字节， 字符类型若为utf8，每个字符最多占用3个字节 varchar最大长度可以是多少 根据字符集，字符类型若为gbk，每个字符占用2个字节，最大长度不能超过65535/2 =32766； 字符类型若为utf8，每个字符最多占用3个字节，最大长度不能超过 65535/3 =21845，若超过这个限制，则会自动将varchar类型转为mediumtext或longtext, 字符与字节的区别 一个字符由于所使用的字符集的不同，会并存储在一个或多个字节中，所以一个字符占用多少个字节取决于所使用的字符集 注意：char（len）与varchar（len）后面接的数据大小为存储的字符数，而不是字节数； 一、存储区别性 char（len）夸号中存储写的是字符长度，最大值为255，如果在存储的时你实际存储的字符长度低于夸号中填写的长度，那它在存储的时候会以空格补全位数进行存储 varchar，则不具备这样的特性，最大长度取值为65535，不会空格补全进行存储； 二、取数据的区别性 char在取值的时候会把存值后面的空格去除掉，varchar 如果后面有空格则会保留； 三、存储占用内存性 不同的字符集字符和字节换算是不同的，拉丁字符换算规律1字符=1字节，utf8是1字符3字节，gbk是1字符2字节； 与char的区别 char(n) 固定长度，最多28−128−1个字符，28−128−1个字节 varchar(n) 可变长度，最多216−1216−1个字符，216−1216−1个字节 取数据的时候，char类型的要用trim()去掉多余的空格，而varchar是不需要的。 char的存储方式是，对英文字符（ASCII）占用1个字节，对一个汉字占用两个字节；而varchar的存储方式是，对每个英文字符占用2个字节，汉字也占用2个字节，两者的存储数据都非unicode的字符数据。 存储的容量不同 对 char 来说，最多能存放的字符个数 255，和编码无关。 而 varchar 呢，最多能存放 65532 个字符。varchar的最大有效长度由最大行大小和使用的字符集确定。整体最大长度是 65,532字节 内连接，外连接（左连接，右连接），union，union all的区别 内连接 inner join...on join...on cross join...on # 同时可以使用以下的三种方法： mysql> select * from a_table, b_table where a_table.a_id = b_table.b_id; mysql> select * from a_table a cross join b_table b on a.a_id=b.b_id; mysql> select * from a_table a join b_table b on a.a_id=b.b_id; 组合两个表中的记录，返回关联字段相符的记录，也就是返回两个表的交集（阴影）部分。 左连接（左外连接） left join ... on ... 说明：left join 是left outer join的简写，它的全称是左外连接，是外连接中的一种。 左(外)连接，左表(a_table)的记录将会全部表示出来，而右表(b_table)只会显示符合搜索条件的记录。右表记录不足的地方均为NULL。 右连接（右外连接） right join ... on ... 说明：right join是right outer join的简写，它的全称是右外连接，是外连接中的一种。 与左(外)连接相反，右(外)连接，左表(a_table)只会显示符合搜索条件的记录，而右表(b_table)的记录将会全部表示出来。左表记录不足的地方均为NULL。 union UNION 操作符用于合并两个或多个 SELECT 语句的结果集。 请注意，UNION 内部的每个 SELECT 语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每个 SELECT 语句中的列的顺序必须相同。 mysql> select * from a_table union select * from b_table; # 一张表中显示了两张表的数据（不重复的数据），先查询的表，其结果放在前面。 union all mysql> select * from a_table union all select * from b_table; # 一张表中显示了两张表的所有数据，先查询的表，其结果放在前面。 在线数据迁移怎么做 步骤：双写数据库、搬历史数据、切换写入、灰度验证（99.9999%）、删除历史； 在线数据迁移_大数据_秀才的专栏-CSDN博客 高并发（二）---数据迁移方案 - 肖冬 - 博客园 如何进行大规模在线数据迁移（来自Stripe公司的经验） - followflows - 博客园 谈谈自己的大数据迁移经历 - 简书 在线数据迁移的一点想法_嗯。-CSDN博客 分库分表？如何做到永不迁移数据和避免热点？ 问题 MySQL可不可以不设置主键ID、自增ID？ 不以自增ID为主键的话，会造成页分裂和页合并，造成性能下降。 MySQL最多能设置多少个索引？添加的索引是越多越好吗？ MySQL count(*)的情况 SQL 选用索引的执行成本如何计算 实际上针对无 where_clause 的 COUNT(*)，MySQL 是有优化的，优化器会选择成本最小的辅助索引查询计数，其实反而性能最高， 不管是 COUNT(1)，还是 COUNT(*)，MySQL 都会用成本最小的辅助索引查询方式来计数，也就是使用 COUNT(*) 由于 MySQL 的优化已经保证了它的查询性能是最好的！ MySQL 会选择成本最小原则来选择使用对应的索引，这里的成本主要包含两个方面。 IO 成本 CPU成本 https://mp.weixin.qq.com/s/iKifzfuKgBGBguCCdq-B3g MySQL 什么情况走了索引还很慢？为啥有时候明明添加了索引却不生效？ 索引列是表示式的一部分，或是函数的一部分 使用 order by 造成的全表扫描 隐式类型转换 隐式编码转换 如何评判一个索引设计的好坏 "},"docs/Guide/Zookeeper.html":{"url":"docs/Guide/Zookeeper.html","title":"Zookeeper","keywords":"","body":"基本概念 节点 持久节点：一直存在于Zookeeper上 临时节点：生命周期与客户端会话绑定 持久顺序节点：PERSISTENT_SEQUENTIAL 临时顺序节点：EPHEMERAL_SEQUENTIAL 特点 顺序一致性： 从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。 原子性： 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。 单一系统映像 ： 无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。 可靠性： 一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。 Session会话 ​ Session是指当Client创建一个同Server的连接时产生的会话。连接Connected之后Session状态就开启，Zookeeper服务器和Client采用长连接方式（Client会不停地向Server发送心跳）保证session在不出现网络问题、服务器宕机或Client宕机情况下可以一直存在。因此，在正常情况下，session会一直有效，并且ZK集群上所有机器都会保存这个Session信息。 ZooKeeper常常发生下面两种系统异常 org.apache.ZooKeeper.KeeperException.ConnectionLossException, 客户端与其中的一台服务器socket连接出现异常，连接丢失； org.apache.ZooKeeper.KeeperException.SessionExpiredException, 客户端的session已经超过sessionTimeout, 未进行任何操作。 ConnectionLossException异常可以通过重试进行处理，客户端会根据初始化ZooKeeper时传递的服务列表，自动尝试下一个服务端节点，而在这段时间内，服务端节点变更的事件就丢失。 SessionExpiredException异常不能通过重试进行解决，需要应用重新创建一 个新的客户端(new Zoo keeper()), 这时所有的watcher和EPHEMERAL节点都将失效。 如何使用curator实现session expired异常的捕获和处理? Connection Loss和Session Expired的关系 ​ 在ZK中，很多数据和状态都是和会话绑定的，一旦会话失效，那么ZK就开始清除和这个会话有关的信息，包括这个会话创建的临时节点和注册的所有Watcher。 一旦网络连接因为某种原因断开或者zk集群发生宕机，ZK Client会马上捕获到这个异常，封装为一个ConnectionLoss的事件，然后启动自动重连机制在地址列表中选择新的地址进行重连。重连会有三种结果： （1）在session timeout时间内重连成功，client会重新收到一个syncconnected的event，并将连接重新持久化为connected状态 （2）超过session timeout时间段后重连成功，client会收到一个expired的event，并将连接持久化为closed状态 （3）一直重连不上，client将不会收到任何event ​ 很显然，无论重连成功与否，在session timeout那个重要的时间点，ZK Client是接收不到任何ZK Server清理临时节点的信息的。这也就导致ZK会通知了B C节点A已经不再是Leader，A自身却没有接收到这样的信息，依旧对外提供服务，进而产生脑裂的问题。 会话时间（Session Time） ​ 在《ZooKeeper API 使用》一文中已经提到，在实例化一个ZK客户端的时候，需要设置一个会话的超时时间。这里需要注意的一点是，客户端并不是可以随意设置这个会话超时时间，在ZK服务器端对会话超时时间是有限制的，主要是minSessionTimeout和maxSessionTimeout这两个参数设置的。（详细查看这个文章《ZooKeeper管理员指南》）Session超时时间限制，如果客户端设置的超时时间不在这个范围，那么会被强制设置为最大或最小时间。 默认的Session超时时间是在2 tickTime ~ 20 tickTime。所以，如果应用对于这个会话超时时间有特殊的需求的话，一定要和ZK管理员沟通好，确认好服务端是否设置了对会话时间的限制。 ​ ZooKeeper的Server端会有两个配置，minSessionTimeout和 maxSess1onTimeout minSessionTimeout的值默认为2 倍 tickTim e , maxSessionTimeout的值默认为20 倍 tickTime ,单位都为ms。tickTime也是服务端的一个配置项 ，是 Server内部控制时间逻辑的最小时间单位 。 经过源码分析，得出SessionTimeOut的协商如下： 情况1: 配置文件配置了maxSessionTimeOut和minSessionTimeOut 最终SessionTimeOut,必须在minSessionTimeOut和maxSessionTimeOut区间里，如果跨越上下界，则以跨越的上届或下界为准。 情况2:配置文件没有配置maxSessionTimeOut和minSessionTimeOut maxSessionTimeout没配置则 maxSessionTimeOut设置为 20 * tickTime minSessionTimeOut没配置则 minSessionTimeOut设置为 2 * tickTime 也就是默认情况下, SessionTimeOut的合法范围为 4秒~40秒，默认配置中tickTime为2秒。 如果tickTime也没配置，那么tickTime缺省为3秒。 遇到问题从源码分析一定是最好的，能使得理解更深入记忆更深刻。 笃行杂记之Zookeeper SessionTimeOut分析 - 网络安全研究&源码分析 - SegmentFault 思否 选举 ZAB协议 Paxos 算法应该可以说是 ZooKeeper 的灵魂了。但是，ZooKeeper 并没有完全采用 Paxos算法 ，而是使用 ZAB 协议作为其保证数据一致性的核心算法。另外，在ZooKeeper的官方文档中也指出，ZAB协议并不像 Paxos 算法那样，是一种通用的分布式一致性算法，它是一种特别为Zookeeper设计的崩溃可恢复的原子消息广播算法。 ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。 ZAB 协议两种基本的模式：崩溃恢复和消息广播 ZAB协议包括两种基本的模式，分别是 崩溃恢复和消息广播。当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的Leader服务器。当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致。 当集群中已经有过半的Follower服务器完成了和Leader服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。 当一台同样遵守ZAB协议的服务器启动后加人到集群中时，如果此时集群中已经存在一个Leader服务器在负责进行消息广播，那么新加人的服务器就会自觉地进入数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。正如上文介绍中所说的，ZooKeeper设计成只允许唯一的一个Leader服务器来进行事务请求的处理。Leader服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议；而如果集群中的其他机器接收到客户端的事务请求，那么这些非Leader服务器会首先将这个事务请求转发给Leader服务器。 关于 ZAB 协议&Paxos算法 需要讲和理解的东西太多了，说实话，笔主到现在不太清楚这俩兄弟的具体原理和实现过程。推荐阅读下面两篇文章： 图解 Paxos 一致性协议 Zookeeper ZAB 协议分析 如何避免脑裂 集群角色 最典型集群模式： Master/Slave 模式（主备模式）。在这种模式中，通常 Master服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。 但是，在 ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入了Leader、Follower 和 Observer 三种角色。 ZooKeeper 集群中的所有机器通过一个 Leader 选举过程来选定一台称为 “Leader” 的机器，Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，Follower 和 Observer 都只能提供读服务。Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。 当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的Leader服务器。这个过程大致是这样的： Leader election（选举阶段）：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。 Discovery（发现阶段）：在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。 Synchronization（同步阶段）:同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准 leader 才会成为真正的 leader。 Broadcast（广播阶段） 到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。 应用 分布式锁 实现分布式锁步骤 ZK原生方案 Curator方案 需要注意的地方 node节点选择为EPHEMERAL_SEQUENTIAL很重要。自增长的特性，可以方便构建一个基于Fair特性的锁，前一个节点唤醒后一个节点，形成一个链式的触发过程。可以有效的避免\"惊群效应\"(一个锁释放，所有等待的线程都被唤醒)`，有针对性的唤醒，提升性能。 选择一个EPHEMERAL临时节点的特性。因为和zookeeper交互是一个网络操作，不可控因素过多，比如网络断了，上一个节点释放锁的操作会失败。临时节点是和对应的session挂接的，session一旦超时或者异常退出其节点就会消失，类似于ReentrantLock中等待队列Thread的被中断处理。 获取lock操作是一个阻塞的操作，而对应的Watcher是一个异步事件，所以需要使用互斥信号共享锁BooleanMutex进行通知，可以比较方便的解决锁重入的问题。(锁重入可以理解为多次读操作，锁释放为写抢占操作) 使用EPHEMERAL会引出一个风险：在非正常情况下，网络延迟比较大会出现session timeout，zookeeper就会认为该client已关闭，从而销毁其id标示，竞争资源的下一个id就可以获取锁。这时可能会有两个process同时拿到锁在跑任务，所以设置好session timeout很重要 Zookeeper面试题 - lanqiu5ge - 博客园 "},"docs/Guide/RabbitMQ整理.html":{"url":"docs/Guide/RabbitMQ整理.html","title":"RabbitMQ","keywords":"","body":"RabbitMQ的结构 Producer(生产者) :生产消息的一方（邮件投递者） Consumer(消费者) :消费消息的一方（邮件收件人） Exchange(交换器) 用来接收生产者发送的消息并将这些消息路由给服务器中的队列中，如果路由不到，或许会返回给 Producer(生产者) ，或许会被直接丢弃掉 。这里可以将RabbitMQ中的交换器看作一个简单的实体。 生产者将消息发给交换器的时候，一般会指定一个 RoutingKey(路由键)，用来指定这个消息的路由规则，而这个 RoutingKey 需要与交换器类型和绑定键(BindingKey)联合使用才能最终生效。 RabbitMQ 中通过 Binding(绑定) 将 Exchange(交换器) 与 Queue(消息队列) 关联起来，在绑定的时候一般会指定一个 BindingKey(绑定建) ,这样 RabbitMQ 就知道如何正确将消息路由到队列了,如下图所示。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。Exchange 和 Queue 的绑定可以是多对多的关系。 四种exchange type类型direct(默认)，fanout, topic, 和 headers RabbitMQ常用的Exchange Type有三种：fanout、direct、topic。 fanout（广播消息）： 把所有发送到该Exchange的消息投递到所有与它绑定的队列中。 direct： 把消息投递到那些binding key与routing key完全匹配的队列中。 topic： 将消息路由到binding key与routing key模式匹配的队列中。 headers（不推荐）：根据发送的消息内容中的 headers 属性进行匹配 多个消费者可以订阅同一个队列，这时队列中的消息会被平均分摊（Round-Robin，即轮询）给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理，这样避免的消息被重复消费。 vhost:虚拟主机,一个broker里可以有多个vhost，用作不同用户的权限分离。 RabbitMQ基础知识详细解读 如何保证消息的可靠性 事务机制 ​ AMQP协议在建立之初就考虑到这种情况而提供了事务机制。RabbitMQ客户端中与事务机制相关的方法有三个：channel.txSelect、channel.txCommit以及channel.txRollback。但是事务会影响RabbitMQ的性能。 发送方确认机制（publisher confirm） ​ 生产者将信道设置成confirm（确认）模式，一旦信道进入confirm模式，所有在该信道上面发布的消息都会被指派一个唯一的ID（从1开始），一旦消息被投递到所有匹配的队列之后，RabbitMQ就会发送一个确认（Basic.Ack）给生产者（包含消息的唯一ID），这就使得生产者知晓消息已经正确到达了目的地了。RabbitMQ回传给生产者的确认消息中的deliveryTag包含了确认消息的序号，此外RabbitMQ也可以设置channel.basicAck方法中的multiple参数，表示到这个序号之前的所有消息都已经得到了处理。相比之下，发送方确认机制最大的好处在于它是异步的，一旦发布一条消息，生产者应用程序就可以在等信道返回确认的同时继续发送下一条消息，当消息最终得到确认之后，生产者应用便可以通过回调方法来处理该确认消息，如果RabbitMQ因为自身内部错误导致消息丢失，就会发送一条nack（Basic.Nack）命令，生产者应用程序同样可以在回调方法中处理该nack命令。 ​ 事务机制和publisher confirm机制确保的是消息能够正确的发送至RabbitMQ，这里的“发送至RabbitMQ”的含义是指消息被正确的发往至RabbitMQ的交换器，如果此交换器没有匹配的队列的话，那么消息也将会丢失。所以在使用这两种机制的时候要确保所涉及的交换器能够有匹配的队列。更进一步的讲，发送方要配合mandatory参数或者备份交换器一起使用来提高消息传输的可靠性。 mandatory ​ 当mandatory参数设为true时，交换器无法根据自身的类型和路由键找到一个符合条件的队列的话，那么RabbitMQ会调用Basic.Return命令将消息返回给生产者。当mandatory参数设置为false时，出现上述情形的话，消息直接被丢弃。 那么生产者如何获取到没有被正确路由到合适队列的消息呢？这时候可以通过调用channel.addReturnListener来添加ReturnListener监听器实现。 ​ 生产者可以通过ReturnListener中返回的消息来重新投递或者其它方案来提高消息的可靠性。 备份交换器 ​ 如果你不想复杂化生产者的编程逻辑，又不想消息丢失，那么可以使用备份交换器，这样可以将未被路由的消息存储在RabbitMQ中，再在需要的时候去处理这些消息。 可以通过在声明交换器（调用channel.exchangeDeclare方法）的时候添加alternate-exchange参数来实现，也可以通过策略的方式实现。如果两者同时使用的话，前者的优先级更高，会覆盖掉Policy的设置。 但是消息存入队列之后的可靠性又如何保证？ 队列、消息持久化 ​ 队列的持久化是通过在声明队列时将durable参数置为true实现的。 ​ 队列的持久化能保证其本身的元数据不会因异常情况而丢失，但是并不能保证内部所存储的消息不会丢失。要确保消息不会丢失，需要将其设置为持久化。通过将消息的投递模式（BasicProperties中的deliveryMode属性）设置为2即可实现消息的持久化。 镜像队列 在镜像队列中，如果主节点（master）在此特殊时间内挂掉，可以自动切换到从节点（slave），这样有效的保证了高可用性，除非整个集群都挂掉。 如何保证消息的不重复 业务端自己去重，用唯一标识的字段 维护一个消费过的列表 如何实现延迟队列 比如说常见的淘宝当下了一个订单后，订单支付时间为半个小时，如果半个小时没有支付，则关闭该订单。 死信队列（Dead Letter Exchanges） 将延迟队列（queue）在声明的时候设置参数 “ x-dead-letter-exchange ”，“ x-message-ttl “ 分别对应 死信路由器（dlx_exchange） 和 消息过期时间(比如说30分钟)。 一个消息从生产者发送到延迟队列 ，在延迟队列里等待，等待30分钟后，会去绑定的死信路由（dlx_exchange）。通过死信路由的规则，走到死信队列。 这时候监听死信队列的消费者就可以接收到消息，消费消息。比如说查看该订单是否支付，如果没有支付，则关闭该订单。 所以在考虑使用RabbitMQ来实现延迟任务队列的时候，需要确保业务上每个任务的延迟时间是一致的。如果遇到不同的任务类型需要不同的延时的话，需要为每一种不同延迟时间的消息建立单独的消息队列。 RabbitMQ 延迟任务（限时订单） 原理 以及代码 实战 - 残剑今生 - 博客园 SpringBoot+RabbitMq实现延时消息队列 - 个人文章 - SegmentFault 思否 使用RabbitMQ实现延迟任务 - 简书 双重死信队列（Retry Later） 消息处理异常、客户端reject消息进入死信队列 死信队列消息过期重新入队 消息延迟ID极限是多少？ 2^32-1 = 4294967295 毫秒 ≈ 49.7天 "},"docs/Guide/Kafka整理.html":{"url":"docs/Guide/Kafka整理.html","title":"Kafka整理","keywords":"","body":"Kafka集群架构 概念 Producer：Producer即生产者，消息的产生者，是消息的入口。 Broker：Broker是kafka实例，每个服务器上有一个或多个kafka的实例，我们姑且认为每个broker对应一台服务器。每个kafka集群内的broker都有一个不重复的编号，如图中的broker-0、broker-1等…… Topic：消息的主题，可以理解为消息的分类，kafka的数据就保存在topic。在每个broker上都可以创建多个topic。一个topic是一类消息。 Partition：Topic的分区，每个topic可以有多个分区，分区的作用是做负载，提高kafka的吞吐量。同一个topic在不同的分区的数据是不重复的，partition的表现形式就是一个一个的文件夹！ Replication: 每一个分区都有多个副本，副本的作用是做备胎。当主分区（Leader）故障的时候会选择一个备胎（Follower）上位，成为Leader。在kafka中默认副本的最大数量是10个，且副本的数量不能大于Broker的数量，follower和leader绝对是在不同的机器，同一机器对同一个分区也只可能存放一个副本（包括自己）。 Message：每一条发送的消息主体。 Consumer：消费者，即消息的消费方，是消息的出口。 Consumer Group：我们可以将多个消费组组成一个消费者组，在kafka的设计中同一个分区的数据只能被消费者组中的某一个消费者消费。同一个消费者组的消费者可以消费同一个topic的不同分区的数据，这也是为了提高kafka的吞吐量！ Zookeeper：kafka集群依赖zookeeper来保存集群的的元信息，来保证系统的可用性。管理集群配置、选举Leader，以及在consumer group变化时进行rebalance。 消息队列的好处 削峰 解耦 异步 Partition（分区） kafka为每个主题维护了分布式的分区(partition)日志文件，每个partition在kafka存储层面是append log。任何发布到此partition的消息都会被追加到log文件的尾部，在分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，也就是我们的offset,offset是一个long型的数字，我们通过这个offset可以确定一条在该partition下的唯一消息。在partition下面是保证了有序性，但是在topic下面没有保证有序性。 在上图中在我们的生产者会决定发送到哪个Partition。 如果没有Key值则进行轮询发送。 如果有Key值，对Key值进行Hash，然后对分区数量取余，保证了同一个Key值的会被路由到同一个分区，如果想队列的强顺序一致性，可以让所有的消息都设置为同一个Key。 分区的主要目的 1、 方便扩展。因为一个topic可以有多个partition，所以我们可以通过扩展机器去轻松的应对日益增长的数据量。 2、 提高并发。以partition为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率。 Broker设计原理 Broker其实就是Kafka的Server。负责集群里的数据存储、消费者的请求，节点的管理等。其中启动时会选举出一个Leader即为Kafka controller，来管理集群和各个Broker rpc通信同步消息以及负责各个topic、partition的副本信息同步。 Controller 消息存储 Kafka的消息以二进制的方式紧凑地存储，节省了很大空间 此外消息存在ByteBuffer而不是堆，这样broker进程挂掉时，数据不会丢失，同时避免了gc问题 通过零拷贝和顺序寻址，让消息存储和读取速度都非常快 处理fetch请求的时候通过zero-copy 加快速度。 负载均衡 分区数量负载：各台broker的partition数量应该均匀 partition Replica分配算法如下： 将所有Broker（假设共n个Broker）和待分配的Partition排序 将第i个Partition分配到第（i mod n）个Broker上 将第i个Partition的第j个Replica分配到第（(i + j) mod n）个Broker上. 容量大小负载：每台broker的硬盘占用大小应该均匀 在kafka1.1之前，Kafka能够保证各台broker上partition数量均匀，但由于每个partition内的消息数不同，可能存在不同硬盘之间内存占用差异大的情况。在Kafka1.1中增加了副本跨路径迁移功能kafka-reassign-partitions.sh，我们可以结合它和监控系统，实现自动化的负载均衡 Producer 设计原理 发送消息的流程 序列化消息 && 计算partition 根据key和value的配置对消息进行序列化,然后计算partition： ProducerRecord对象中如果指定了partition，就使用这个partition。否则根据key和topic的partition数目取余，如果key也没有的话就随机生成一个counter，使用这个counter来和partition数目取余。这个counter每次使用的时候递增。 发送到batch && 唤醒Sender线程 根据topic-partition获取对应的batchs（Deque），然后将消息append到batch中.如果有batch满了则唤醒Sender 线程。队列的操作是加锁执行，所以batch内消息时有序的。后续的Sender操作当前方法异步操作。 Sender把消息有序发到 broker（tp replia leader） 3.1 确定tp relica leader 所在的broker Kafka中 每台broker都保存了kafka集群的metadata信息，metadata信息里包括了每个topic的所有partition的信息: leader, leader_epoch, controller_epoch, isr, replicas等;Kafka客户端从任一broker都可以获取到需要的metadata信息;sender线程通过metadata信息可以知道tp leader的brokerId producer也保存了metada信息，同时根据metadata更新策略（定期更新metadata.max.age.ms、失效检测，强制更新：检查到metadata失效以后，调用metadata.requestUpdate()强制更新 3.2 幂等性发送 为实现Producer的幂等性，Kafka引入了Producer ID（即PID）和Sequence Number。对于每个PID，该Producer发送消息的每个都对应一个单调递增的Sequence Number。同样，Broker端也会为每个维护一个序号，并且每Commit一条消息时将其对应序号递增。对于接收的每条消息，如果其序号比Broker维护的序号）大一，则Broker会接受它，否则将其丢弃： 如果消息序号比Broker维护的序号差值比一大，说明中间有数据尚未写入，即乱序，此时Broker拒绝该消息，Producer抛出InvalidSequenceNumber 如果消息序号小于等于Broker维护的序号，说明该消息已被保存，即为重复消息，Broker直接丢弃该消息，Producer抛出DuplicateSequenceNumber Sender发送失败后会重试，这样可以保证每个消息都被发送到broker Sender处理broker发来的produce response 一旦broker处理完Sender的produce请求，就会发送produce response给Sender，此时producer将执行我们为send（）设置的回调函数。至此producer的send执行完毕。 sender线程和长连接 每初始化一个producer实例，都会初始化一个Sender实例，新增到broker的长连接。 代码角度：每初始化一次KafkaProducer，都赋一个空的client。 Consermer 设计原理 Consermer Group（消费者组） 消费者组（Consumer Group）是由一个或多个消费者实例（Consumer Instance）组成的群组，具有可扩展性和可容错性的一种机制。消费者组内的消费者共享一个消费者组ID，这个ID 也叫做 Group ID，组内的消费者共同对一个主题进行订阅和消费，同一个组中的消费者只能消费一个分区的消息，多余的消费者会闲置，派不上用场。 kafka保证了同一个消费组中只有一个消费者实例会消费某条消息，实际上，kafka保证的是稳定状态下每一个消费者实例只会消费一个或多个特定partition数据，而某个partition的数据只会被某一特定的consumer实例消费，这样设计的劣势是无法让同一个消费组里的consumer均匀消费，优势是每个consumer不用跟大量的broker通信，减少通信开销，也降低了分配难度。而且，同一个partition数据是有序的，保证了有序被消费。根据consumer group中的consumer数量和partition数量，可以分为以下3种情况： 若consumer group中的consumer数量少于partition数量，则至少有1个consumer会消费多个partition数据 若consumer group中的consumer数量多于partition数量，则会有部分consumer无法消费该topic中任何一条消息 若consumer group中的consumer数量等于partition数量，则正好一个consumer消费一个partition数据 Consumer Rebalance（分区重平衡） 可以看到，当新的消费者加入消费组，它会消费一个或多个分区，而这些分区之前是由其他消费者负责的；另外，当消费者离开消费组（比如重启、宕机等）时，它所消费的分区会分配给其他分区。这种现象称为重平衡（rebalance）。重平衡是 Kafka 一个很重要的性质，这个性质保证了高可用和水平扩展。不过也需要注意到，在重平衡期间，所有消费者都不能消费消息，因此会造成整个消费组短暂的不可用。而且，将分区进行重平衡也会导致原来的消费者状态过期，从而导致消费者需要重新更新状态，这段期间也会降低消费性能。后面我们会讨论如何安全的进行重平衡以及如何尽可能避免。 消费者通过定期发送心跳（hearbeat）到一个作为组协调者（group coordinator）的 broker 来保持在消费组内存活。这个 broker 不是固定的，每个消费组都可能不同。当消费者拉取消息或者提交时，便会发送心跳。 如果消费者超过一定时间没有发送心跳，那么它的会话（session）就会过期，组协调者会认为该消费者已经宕机，然后触发重平衡。可以看到，从消费者宕机到会话过期是有一定时间的，这段时间内该消费者的分区都不能进行消息消费；通常情况下，我们可以进行优雅关闭，这样消费者会发送离开的消息到组协调者，这样组协调者可以立即进行重平衡而不需要等待会话过期。 如果过了一段时间 Kafka 停止发送心跳了，会话（Session）就会过期，组织协调者就会认为这个 Consumer 已经死亡，就会触发一次重平衡。如果消费者宕机并且停止发送消息，组织协调者会等待几秒钟，确认它死亡了才会触发重平衡。在这段时间里，死亡的消费者将不处理任何消息。在清理消费者时，消费者将通知协调者它要离开群组，组织协调者会触发一次重平衡，尽量降低处理停顿。 重平衡是一把双刃剑，它为消费者群组带来高可用性和伸缩性的同时，还有有一些明显的缺点(bug)，而这些 bug 到现在社区还无法修改。 重平衡的过程对消费者组有极大的影响。因为每次重平衡过程中都会导致万物静止，参考 JVM 中的垃圾回收机制，也就是 Stop The World ，STW 在 0.10.1 版本，Kafka 对心跳机制进行了修改，将发送心跳与拉取消息进行分离，这样使得发送心跳的频率不受拉取的频率影响。另外更高版本的 Kafka 支持配置一个消费者多长时间不拉取消息但仍然保持存活，这个配置可以避免活锁（livelock）。活锁，是指应用没有故障但是由于某些原因不能进一步消费。 偏移量 消费者会向一个叫做 _consumer_offset 的特殊主题中发送消息，这个主题会保存每次所发送消息中的分区偏移量，这个主题的主要作用就是消费者触发重平衡后记录偏移使用的，消费者每次向这个主题发送消息，正常情况下不触发重平衡，这个主题是不起作用的，当触发重平衡后，消费者停止工作，每个消费者可能会分到对应的分区，这个主题就是让消费者能够继续处理消息所设置的。 如果提交的偏移量小于客户端最后一次处理的偏移量，那么位于两个偏移量之间的消息就会被重复处理 重平衡后，消费者重复消费； 如果提交的偏移量大于最后一次消费时的偏移量，那么处于两个偏移量中间的消息将会丢失 重平衡后，消费者丢失数据； 既然_consumer_offset 如此重要，那么它的提交方式是怎样的呢？下面我们就来说一下。 KafkaConsumer API 提供了多种方式来提交偏移量 自动提交 最简单的方式就是让消费者自动提交偏移量。如果 enable.auto.commit 被设置为true，那么每过 5s，消费者会自动把从 poll() 方法轮询到的最大偏移量提交上去。提交时间间隔由 auto.commit.interval.ms 控制，默认是 5s。与消费者里的其他东西一样，自动提交也是在轮询中进行的。消费者在每次轮询中会检查是否提交该偏移量了，如果是，那么就会提交从上一次轮询中返回的偏移量。 提交当前偏移量 把 auto.commit.offset 设置为 false，可以让应用程序决定何时提交偏移量。使用 commitSync() 提交偏移量。这个 API 会提交由 poll() 方法返回的最新偏移量，提交成功后马上返回，如果提交失败就抛出异常。 commitSync() 将会提交由 poll() 返回的最新偏移量，如果处理完所有记录后要确保调用了 commitSync()，否则还是会有丢失消息的风险，如果发生了在均衡，从最近一批消息到发生在均衡之间的所有消息都将被重复处理。 异步提交 异步提交 commitAsync() 与同步提交 commitSync() 最大的区别在于异步提交不会进行重试，同步提交会一直进行重试。 同步和异步组合提交 一般情况下，针对偶尔出现的提交失败，不进行重试不会有太大的问题，因为如果提交失败是因为临时问题导致的，那么后续的提交总会有成功的。但是如果在关闭消费者或再均衡前的最后一次提交，就要确保提交成功。 因此，在消费者关闭之前一般会组合使用commitAsync和commitSync提交偏移量。 提交特定的偏移量 消费者API允许调用 commitSync() 和 commitAsync() 方法时传入希望提交的 partition 和 offset 的 map，即提交特定的偏移量。 消费模型 消息由生产者发送到kafka集群后，会被消费者消费。一般来说我们的消费模型有两种:推送模型(push)和拉取模型(pull) 基于推送模型的消息系统 由消息代理记录消费状态。消息代理将消息推送到消费者后，标记这条消息为已经被消费，但是这种方式无法很好地保证消费的处理语义。比如当我们把已经把消息发送给消费者之后，由于消费进程挂掉或者由于网络原因没有收到这条消息，如果我们在消费代理将其标记为已消费，这个消息就永久丢失了。如果我们利用生产者收到消息后回复这种方法，消息代理需要记录消费状态，这种不可取。如果采用push，消息消费的速率就完全由消费代理控制，一旦消费者发生阻塞，就会出现问题。 Kafka采取拉取模型(poll) 由自己控制消费速度，以及消费的进度，消费者可以按照任意的偏移量进行消费。比如消费者可以消费已经消费过的消息进行重新处理，或者消费最近的消息等等。 高可靠分布式存储模型 高性能的日志存储 kafka一个topic下面的所有消息都是以partition的方式分布式的存储在多个节点上。同时在kafka的机器上，每个Partition其实都会对应一个日志目录，在目录下面会对应多个日志分段(LogSegment)。LogSegment文件由两部分组成，分别为“.index”文件和“.log”文件，分别表示为segment索引文件和数据文件。这两个文件的命令规则为：partition全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值，数值大小为64位，20位数字字符长度，没有数字用0填充，如下，假设有1000条消息，每个LogSegment大小为100。 由于kafka消息数据太大，如果全部建立索引，即占了空间又增加了耗时，所以kafka选择了稀疏索引的方式，这样的话索引可以直接进入内存，加快偏查询速度。 简单介绍一下如何读取数据，如果我们要读取第911条数据首先第一步，找到他是属于哪一段的，根据二分法查找到他属于的文件，找到0000900.index和00000900.log之后，然后去index中去查找 (911-900) =11这个索引或者小于11最近的索引,在这里通过二分法我们找到了索引是[10,1367]然后我们通过这条索引的物理位置1367，开始往后找，直到找到911条数据。 上面讲的是如果要找某个offset的流程，但是我们大多数时候并不需要查找某个offset,只需要按照顺序读即可，而在顺序读中，操作系统会对内存和磁盘之间添加page cahe，也就是我们平常见到的预读操作，所以我们的顺序读操作时速度很快。但是kafka有个问题，如果分区过多，那么日志分段也会很多，写的时候由于是批量写，其实就会变成随机写了，随机I/O这个时候对性能影响很大。所以一般来说Kafka不能有太多的partition。针对这一点，RocketMQ把所有的日志都写在一个文件里面，就能变成顺序写，通过一定优化，读也能接近于顺序读。 Kafka的多副本机制 Kafka的副本机制是多个服务端节点对其他节点的主题分区的日志进行复制。当集群中的某个节点出现故障，访问故障节点的请求会被转移到其他正常节点(这一过程通常叫Reblance)，kafka每个主题的每个分区都有一个主副本以及0个或者多个副本，副本保持和主副本的数据同步，当主副本出故障时就会被替代。 在Kafka中并不是所有的副本都能被拿来替代主副本，所以在kafka的leader节点中维护着一个ISR(In sync Replicas)集合，翻译过来也叫正在同步中集合，在这个集合中的需要满足两个条件: 节点必须和ZK保持连接 在同步的过程中这个副本不能落后主副本太多 另外还有个AR(Assigned Replicas)用来标识副本的全集，OSR用来表示由于落后被剔除的副本集合，所以公式如下: ISR = leader + 没有落后太多的副本; AR = OSR+ ISR 这里先要说下两个名词：HW(高水位)是consumer能够看到的此partition的位置，LEO是每个partition的log最后一条Message的位置。HW能保证leader所在的broker失效，该消息仍然可以从新选举的leader中获取，不会造成消息丢失。 消息的可靠性 当producer向leader发送数据时，可以通过request.required.acks参数来设置数据可靠性的级别： 1（默认）：这意味着producer在ISR中的leader已成功收到的数据并得到确认后发送下一条message。如果leader宕机了，则会丢失数据。 0：这意味着producer无需等待来自broker的确认而继续发送下一批消息。这种情况下数据传输效率最高，但是数据可靠性确是最低的。 -1：producer需要等待ISR中的所有follower都确认接收到数据后才算一次发送完成，可靠性最高。但是这样也不能保证数据不丢失，比如当ISR中只有leader时(其他节点都和zk断开连接，或者都没追上)，这样就变成了acks=1的情况。 工作流程分析 发送数据 Producer在写入数据的时候永远的找leader，不会直接将数据写入follower！Consumer也是。 Producer怎么保证数据不丢失 通过ACK应答机制！在生产者向队列写入数据的时候可以设置参数来确定是否确认kafka接收到数据，这个参数可设置的值为0、1、-1。 0代表producer往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是效率最高。 1代表producer往集群发送数据只要leader应答就可以发送下一条，只确保leader发送成功。 -1代表producer往集群发送数据需要所有的follower都完成从leader的同步才会发送下一条，确保leader发送成功和所有的副本都完成备份。安全性最高，但是效率最低。 最后要注意的是，如果往不存在的topic写数据，能不能写入成功呢？kafka会自动创建topic，分区和副本的数量根据默认配置都是1。 保存数据   Producer将数据写入kafka后，集群就需要对数据进行保存了！kafka将数据保存在磁盘，可能在我们的一般的认知里，写入磁盘是比较耗时的操作，不适合这种高并发的组件。Kafka初始会单独开辟一块磁盘空间，顺序写入数据（效率比随机写入高）。 Partition在服务器上的表现形式就是一个一个的文件夹，每个partition的文件夹下面会有多组segment文件，每组segment文件又包含.index文件、.log文件、.timeindex文件（早期版本中没有）三个文件， log文件就实际是存储message的地方，而index和timeindex文件为索引文件，用于检索消息。 如上图，这个partition有三组segment文件，每个log文件的大小是一样的，但是存储的message数量是不一定相等的（每条的message大小不一致）。文件的命名是以该segment最小offset来命名的，如000.index存储offset为0~368795的消息，kafka就是利用 分段 + 索引的方式（稀疏索引 + 二分查找）来解决查找效率的问题。 index文件中并没有为每一条message建立索引。而是采用了稀疏存储的方式，每隔一定字节的数据建立一条索引，这样的话就是避免了索引文件占用过多的空间和资源，从而可以将索引文件保留到内存中。缺点是没有建立索引的数据在查询的过程中需要小范围内的顺序扫描操作。索引文件映射到内存的话，从而提高了查找的速度信息。 Message结构 上面说到log文件就实际是存储message的地方，我们在producer往kafka写入的也是一条一条的message，那存储在log中的message是什么样子的呢？消息主要包含消息体、消息大小、offset、压缩类型……等等！我们重点需要知道的是下面三个： 1、 offset：offset是一个占8byte的有序id号，它可以唯一确定每条消息在parition内的位置！ 2、 消息大小：消息大小占用4byte，用于描述消息的大小。 3、 消息体：消息体存放的是实际的消息数据（被压缩过），占用的空间根据具体的消息而不一样。 存储策略 无论消息是否被消费，kafka都会保存所有的消息。那对于旧数据有什么删除策略呢？ 1、 基于时间，默认配置是168小时（7天）。 2、 基于大小，默认配置是1073741824。 需要注意的是，kafka读取特定消息的时间复杂度是O(1)，所以这里删除过期的文件并不会提高kafka的性能！ 消费数据 多个消费者可以组成一个消费者组（consumer group），每个消费者组都有一个组id！同一个消费组者的消费者可以消费同一topic下不同分区的数据，但是不会组内多个消费者消费同一分区的数据！！！是不是有点绕。我们看下图： 图示是消费者组内的消费者小于partition数量的情况，所以会出现某个消费者消费多个partition数据的情况，消费的速度也就不及只处理一个partition的消费者的处理速度！如果是消费者组的消费者多于partition的数量，那会不会出现多个消费者消费同一个partition的数据呢？上面已经提到过不会出现这种情况！多出来的消费者不消费任何partition的数据。所以在实际的应用中，建议消费者组的consumer的数量与partition的数量一致！ 查找消息 利用segment+offset 假如现在需要查找一个offset为368801的message是什么样的过程呢？ 1、 先找到offset的368801message所在的segment文件（利用二分法查找），这里找到的就是在第二个segment文件。 2、 打开找到的segment中的.index文件（也就是368796.index文件，该文件起始偏移量为368796+1，我们要查找的offset为368801的message在该index内的偏移量为368796+5=368801，所以这里要查找的相对offset为5）。由于该文件采用的是稀疏索引的方式存储着相对offset及对应message物理偏移量的关系，所以直接找相对offset为5的索引找不到，这里同样利用二分法查找相对offset小于或者等于指定的相对offset的索引条目中最大的那个相对offset，所以找到的是相对offset为4的这个索引。 3、 根据找到的相对offset为4的索引确定message存储的物理偏移位置为256。打开数据文件，从位置为256的那个地方开始顺序扫描直到找到offset为368801的那条Message。 这套机制是建立在offset为有序的基础上，利用segment+有序offset+稀疏索引+二分查找+顺序查找等多种手段来高效的查找数据！至此，消费者就能拿到需要处理的数据进行处理了。那每个消费者又是怎么记录自己消费的位置呢？在早期的版本中，消费者将消费到的offset维护zookeeper中，consumer每间隔一段时间上报一次，这里容易导致重复消费，且性能不好！在新的版本中消费者消费到的offset已经直接维护在kafk集群的__consumer_offsets这个topic中！ 日志文件格式存储 名字 含义 备注 00000000000009475939 文件中第一条消息的offset *.log 存储消息实体的文件 *.index 记录消息的offset以及消息在log文件中的position的索引 稀疏存储 *.timeindex 记录消息的timestamp和offset的索引 稀疏存储 使用kafka-run-class工具调用kafka.tools.DumpLogSegments,查看kafka消息落盘后信息。 如下 ： /usr/hdp/current/kafka-broker/bin/kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files **.log(index,timeindex) Zookeeper 在 Kafka 中的作用 探测broker和consumer的添加或移除。 负责维护所有partition的领导者/从属者关系（主分区和备份分区），如果主分区挂了，需要选举出备份分区作为主分区。 维护topic、partition等元配置信息 Kafka 如何保证消息的消费顺序？ 没有办法。Kafka 只会保证在 Partition 内消息是有序的，而不管全局的情况。 1 个 Topic 只对应一个 Partition。 （推荐）发送消息的时候指定 key/Partition（业务分片）。 Kafka中的消息是否会丢失和重复消费？ 要确定Kafka的消息是否丢失或重复，从两个方面分析入手：消息发送和消息消费。 解决办法： 针对消息丢失： 同步模式下，确认机制设置为-1，即让消息写入Leader和Follower之后再确认消息发送成功； 异步模式下，为防止缓冲区满，可以在配置文件设置不限制阻塞超时时间，当缓冲区满时让生产者一直处于阻塞状态； 针对消息重复： 将消息的唯一标识保存到外部介质中，每次消费时判断是否处理过即可。 消息重复消费及解决参考：https://www.javazhiyin.com/22910.html Kafka 如何保证消息不重复消费？ 为了避免消息丢失，我们需要付出两方面的代价：一方面是性能的损耗；一方面可能造成消息重复消费。 从消费的最终结果来看和只消费一次是等同的就好了，也就是保证在消息的生产和消费的过程是“幂等”的。 你可以看到，无论是生产端的幂等性保证方式，还是消费端通用的幂等性保证方式，它们的共同特点都是为每一个消息生成一个唯一的 ID，然后在使用这个消息的时候，先比对这个 ID 是否已经存在，如果存在，则认为消息已经被使用过。 Kafka 如何保证可靠性？ Kafka 中的可靠性保证有如下四点： 对于一个分区来说，它的消息是有序的。如果一个生产者向一个分区先写入消息A，然后写入消息B，那么消费者会先读取消息A再读取消息B。 当消息写入所有in-sync状态的副本后，消息才会认为已提交（committed）。这里的写入有可能只是写入到文件系统的缓存，不一定刷新到磁盘。生产者可以等待不同时机的确认，比如等待分区主副本写入即返回，后者等待所有in-sync状态副本写入才返回。 一旦消息已提交，那么只要有一个副本存活，数据不会丢失。 消费者只能读取到已提交的消息。 Kafka 读写数据 前面讲解到了生产者往topic里丢数据是存在partition上的，而partition持久化到磁盘是IO顺序访问的，并且是先写缓存，隔一段时间或者数据量足够大的时候才批量写入磁盘的。 消费者在读的时候也很有讲究：正常的读磁盘数据是需要将内核态数据拷贝到用户态的，而Kafka 通过调用sendfile()直接从内核空间（DMA的）到内核空间（Socket的），少做了一步拷贝的操作。 Kafka 为何如此之快 Kafka 实现了零拷贝原理来快速移动数据，避免了内核之间的切换。Kafka 可以将数据记录分批发送，从生产者到文件系统（Kafka 主题日志）到消费者，可以端到端的查看这些批次的数据。 批处理能够进行更有效的数据压缩并减少 I/O 延迟，Kafka 采取顺序写入磁盘的方式，避免了随机磁盘寻址的浪费，更多关于磁盘寻址的了解，请参阅 程序员需要了解的硬核知识之磁盘。 总结一下其实就是四个要点 顺序读写 零拷贝 消息压缩 分批发送 Kafka 如何提高消费速率 这个问题就是如何高并发下提升处理能力的问题了。KAFKA本质是拉取模型的消息队列 一般来说有几类 增加分区（题上不让） 关闭autocommit（偏移量手工提交可以按需减少分区偏移量的更新，有利于提升消费速度） 增加单次拉取消息的大小（大量消息的场景下可减少拉取消息的次数） 比较另类的： 如果不考虑数据一致性，可以将key值平均一下，这样每个分区的消息大小都差不多，有利于负载均衡 如果没有开启压缩，最好开启压缩（需要重启集群），可大大提高通信效率，有得消费速度提升 如果不考虑顺序消息的话：多线程消费 Kafka producer如何提高写入速率 增加线程 提高 batch.size 增加更多 producer 实例 增加 partition 数 设置 acks=-1 时，如果延迟增大：可以增大 num.replica.fetchers（follower 同步数据的线程数）来调解； 跨数据中心的传输：增加 socket 缓冲区设置以及 OS tcp 缓冲区设置。 _consumer_offsets里怎么获取对应的consumer group？ Kafka 如何读取offset topic内容 (__consumer_offsets) - huxihx - 博客园 配置 主题默认配置 Kafka 为新创建的主题提供了很多默认配置参数，下面就来一起认识一下这些参数 num.partitions num.partitions 参数指定了新创建的主题需要包含多少个分区。如果启用了主题自动创建功能（该功能是默认启用的），主题分区的个数就是该参数指定的值。该参数的默认值是 1。要注意，我们可以增加主题分区的个数，但不能减少分区的个数。 default.replication.factor 这个参数比较简单，它表示 kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务default.replication.factor 的默认值为1，这个参数在你启用了主题自动创建功能后有效。 log.retention.ms Kafka 通常根据时间来决定数据可以保留多久。默认使用 log.retention.hours 参数来配置时间，默认是 168 个小时，也就是一周。除此之外，还有两个参数 log.retention.minutes 和 log.retentiion.ms 。这三个参数作用是一样的，都是决定消息多久以后被删除，推荐使用 log.retention.ms。 log.retention.bytes 另一种保留消息的方式是判断消息是否过期。它的值通过参数 log.retention.bytes 来指定，作用在每一个分区上。也就是说，如果有一个包含 8 个分区的主题，并且 log.retention.bytes 被设置为 1GB，那么这个主题最多可以保留 8GB 数据。所以，当主题的分区个数增加时，整个主题可以保留的数据也随之增加。 log.segment.bytes 上述的日志都是作用在日志片段上，而不是作用在单个消息上。当消息到达 broker 时，它们被追加到分区的当前日志片段上，当日志片段大小到达 log.segment.bytes 指定上限（默认为 1GB）时，当前日志片段就会被关闭，一个新的日志片段被打开。如果一个日志片段被关闭，就开始等待过期。这个参数的值越小，就越会频繁的关闭和分配新文件，从而降低磁盘写入的整体效率。 log.segment.ms 上面提到日志片段经关闭后需等待过期，那么 log.segment.ms 这个参数就是指定日志多长时间被关闭的参数和，log.segment.ms 和 log.retention.bytes 也不存在互斥问题。日志片段会在大小或时间到达上限时被关闭，就看哪个条件先得到满足。 message.max.bytes broker 通过设置 message.max.bytes 参数来限制单个消息的大小，默认是 1000 000， 也就是 1MB，如果生产者尝试发送的消息超过这个大小，不仅消息不会被接收，还会收到 broker 返回的错误消息。跟其他与字节相关的配置参数一样，该参数指的是压缩后的消息大小，也就是说，只要压缩后的消息小于 mesage.max.bytes，那么消息的实际大小可以大于这个值 这个值对性能有显著的影响。值越大，那么负责处理网络连接和请求的线程就需要花越多的时间来处理这些请求。它还会增加磁盘写入块的大小，从而影响 IO 吞吐量。 retention.ms 规定了该主题消息被保存的时常，默认是7天，即该主题只能保存7天的消息，一旦设置了这个值，它会覆盖掉 Broker 端的全局参数值。 retention.bytes retention.bytes：规定了要为该 Topic 预留多大的磁盘空间。和全局参数作用相似，这个值通常在多租户的 Kafka 集群中会有用武之地。当前默认值是 -1，表示可以无限使用磁盘空间。 引用： 阿里大牛实战归纳——Kafka架构原理 再过半小时，你就能明白kafka的工作原理了 - 掘金 全网最通俗易懂的Kafka入门！ - 掘金 Kafka—JavaGuide 真的，关于 Kafka 入门看这一篇就够了 Kafka 日志存储、清理规则、消息大小估算 - 简书 面试题： 八年面试生涯，整理了一套Kafka面试题 - 编辑小猿 - 博客园 Kafka常见面试题_大数据_qq_28900249的博客-CSDN博客 "},"docs/Guide/Kafka进阶.html":{"url":"docs/Guide/Kafka进阶.html","title":"Kafka进阶","keywords":"","body":"Kafka的各种选举 在kafka里有很多Leader，我们常常会对这些Leader选举概念有混淆，下面将分别讲一下三种不同身份的Leader。 Broker集群中的Leader——KafkaController 概念： Kafka Controller负责管理整个集群中分区和副本的状态，比如partition的leader 副本故障，由controller 负责为该partition重新选举新的leader 副本；当检测到ISR列表发生变化，由controller通知集群中所有broker更新其MetadataCache信息；或者增加某个topic分区的时候也会由controller管理分区的重新分配工作。 选举： 当broker启动的时候，都会创建KafkaController对象，但是集群中只能有一个leader对外提供服务，这些每个节点上的KafkaController会在指定的zookeeper路径下创建临时节点，只有第一个成功创建的节点的KafkaController才可以成为leader，其余的都是follower。当leader故障后，所有的follower会收到通知，再次竞争在该路径下创建节点从而选举新的leader。 /controller_epoch 记录了当前Controller Leader的年代信息 /controller 记录了当前Controller Leader的id，也用于Controller Leader的选择 Controller所在在Leader的broker宕机了怎么办？ 重新非公平的竞争zk节点写入法选举新的Leader； 对于老的Leader上的分片，找到分片对应的ISR，从ISR中选举新的Leader，如果ISR为空，则可以从ISR外的非同步节点中选举，但是数据会有缺失，不建议为了可用性牺牲一致性。 历史问题改进： 在kafka中多个broker构成的集群里，Leader的选举是利用zk的抢占写/controller临时节点数据是否成功判断的，在zk的/controller节点会保存相关的broker.id的信息和变动的epoch次数信息。 如果网络问题或者异常情况宕机，集群的leader也就是/controller所在broker节点挂掉会发生什么呢？ 在早期版本，对于分区和副本的状态的管理依赖于zookeeper的Watcher和队列，每个broker都会在zookeeper注册Watcher，所以zookeeper就会出现大量的Watcher，如果宕机的broker上的partition很多比较多，会造成多个Watcher触发，造成集群内大规模调整；每一个replica都要去再次zookeeper上注册监视器，当集群规模很大的时候，zookeeper负担很重。这种设计很容易出现脑裂和羊群效应以及zookeeper集群过载。 在新版本中只有KafkaController Leader会向zookeeper上注册Watcher，其他broker几乎不用监听zookeeper的状态变化。 KafkaController介绍-CSDN博客 分片中的Leader Leader副本的选举 基本思路是按照AR集合中副本的顺序查找第一个存活的副本，并且这个副本在ISR集合中。一个分区的AR集合在分配的时候就被指定，并且只要不发生重分配的情况，集合内部副本的顺序是保持不变的，而分区的ISR集合中副本的顺序可能会改变。注意这里是根据AR的顺序而不是ISR的顺序进行选举的。这个说起来比较抽象，有兴趣的读者可以手动关闭/开启某个集群中的broker来观察一下具体的变化。 还有一些情况也会发生分区leader的选举，比如当分区进行重分配（reassign）的时候也需要执行leader的选举动作。这个思路比较简单：从重分配的AR列表中找到第一个存活的副本，且这个副本在目前的ISR列表中。3 Kafka科普系列 | 原来Kafka中的选举有这么多？_大数据_朱小厮的博客-CSDN博客 消费者组的Leader 在GroupCoordinator中消费者的信息是以HashMap的形式存储的，其中key为消费者的member_id，而value是消费者相关的元数据信息。leaderId表示leader消费者的member_id，它的取值为HashMap中的第一个键值对的key，这种选举的方式基本上和随机无异。总体上来说，消费组的leader选举过程是很随意的。 Partition副本分配算法 为了能让partition和replica均匀的分布在broker上，防止一台机器负载较高。有如下分配算法： 将所有N Broker和待分配的i个Partition排序. 将第i个Partition分配到第(i mod n)个Broker上. 将第i个Partition的第j个副本分配到第((i + j) mod n)个Broker上 topic初始创建后，就会符合上述分配。 但是在集群leader又宕机后，此机器的所有partition的leader都会变化，当原来宕机的机器恢复后，加入到集群变成了follower。此时partition的leader就没有均匀的分布。这时就可以使用partition的leader平衡工具来手动或者自动平衡Leader。 Kafka集群中，某broker节点进程挂掉之后，zk集群是如何感知到的？通过controller? ​ controller在启动时会注册zk监听器来监听zookeeper中的/brokers/ids节点下子节点的变化，即集群所有broker列表，而每台broker在启动时会向zk的/brokers/ids下写入一个名为broker.id的临时节点，当该broker挂掉或与zk断开连接时，此临时节点会被移除（这是zk提供的功能），之后controller端的监听器就会自动感知到这个变化并将BrokerChange事件写入到controller上的请求阻塞队列中。 一旦controller端从阻塞队列中获取到该事件，它会开启BrokerChange事件的处理逻辑，具体包含： 获取当前存活的所有broker列表 根据之前缓存的broker列表计算出当前“已挂掉”的broker列表 更新controller端缓存 对于当前所有存活的broker，更新元数据信息并且启动新broker上的分区和副本 对于“挂掉”的那些broker，处理这些broker上的分区副本（标记为offline以及执行offline逻辑并更新元数据等） kafka一个broker挂掉无法写入 因其中一个broker挂掉无法获取leader的brokerid 但3个broker中的其中一个挂掉，是可以将数据写入另外两个broker的。至于没有写入另外两个broker报错org.apache.kafka.common.errors.NotLeaderForPartition可能是因为可能我们的producer端的代码里没加 reties 参数，默认就发送一次，遇到leader选举时，找不到leader就会发送失败，造成程序停止 解决办法： producer端加上参数 reties=3， 重试发送三次（默认100ms重试一次 由 retry.backoff.ms控制）； 如果还需要保证消息发送的有序性，记得加上参数 max.in.flight.requests.per.connection = 1 限制客户端在单个连接上能够发送的未响应请求的个数，设置此值是1表示kafka broker在响应请求之前client不能再向同一个broker发送请求。（注意：设置此参数是为了满足必须顺序消费的场景，比如binlog数据） kafka如何保证同一个分区下的所有副本保存有相同的消息序列： 基于领导者（Leader-based）的副本机制 工作原理如图： 1、Kafka 中分成两类副本：领导者副本（Leader Replica）和追随者副本（Follower Replica）。每个分区在创建时都要选举一个副本，称为领导者副本，其余的副本自动称为追随者副本。 2、Kafka 中，追随者副本是不对外提供服务的。追随者副本不处理客户端请求，它唯一的任务就是从领导者副本，所有的读写请求都必须发往领导者副本所在的 Broker，由该 Broker 负责处理。（因此目前kafka只能享受到副本机制带来的第 1 个好处，也就是提供数据冗余实现高可用性和高持久性） 3、领导者副本所在的 Broker 宕机时，Kafka 依托于 ZooKeeper 提供的监控功能能够实时感知到，并立即开启新一轮的领导者选举，从追随者副本中选一个作为新的领导者。老 Leader 副本重启回来后，只能作为追随者副本加入到集群中。 kafka追随者副本到底在什么条件下才算与 Leader 同步 Kafka 引入了 In-sync Replicas，也就是所谓的 ISR 副本集合。ISR 中的副本都是与 Leader 同步的副本，相反，不在 ISR 中的追随者副本就被认为是与 Leader 不同步的 kafka In-sync Replicas（ISR） 1、ISR不只是追随者副本集合，它必然包括 Leader 副本。甚至在某些情况下，ISR 只有 Leader 这一个副本 2、通过Broker 端replica.lag.time.max.ms 参数（Follower 副本能够落后 Leader 副本的最长时间间隔）值来控制哪个追随者副本与 Leader 同步？只要一个 Follower 副本落后 Leader 副本的时间不连续超过 10 秒，那么 Kafka 就认为该 Follower 副本与 Leader 是同步的，即使此时 Follower 副本中保存的消息明显少于 Leader 副本中的消息。 3、ISR 是一个动态调整的集合，而非静态不变的。 某个追随者副本从领导者副本中拉取数据的过程持续慢于 Leader 副本的消息写入速度，那么在 replica.lag.time.max.ms 时间后，此 Follower 副本就会被认为是与 Leader 副本不同步的，因此不能再放入 ISR 中。此时，Kafka 会自动收缩 ISR 集合，将该副本“踢出”ISR。 倘若该副本后面慢慢地追上了 Leader 的进度，那么它是能够重新被加回 ISR 的。 4、ISR集合为空则leader副本也挂了，这个分区就不可用了，producer也无法向这个分区发送任何消息了。（反之leader副本挂了可以从ISR集合中选举leader副本） kafka leader副本所在broker挂了，leader副本如何选举 1、ISR不为空，从ISR中选举 2、ISR为空，Kafka也可以从不在 ISR 中的存活副本中选举，这个过程称为Unclean 领导者选举，通过Broker 端参数 unclean.leader.election.enable 控制是否允许 Unclean 领导者选举。开启 Unclean 领导者选举可能会造成数据丢失，但好处是，它使得分区 Leader 副本一直存在，不至于停止对外提供服务，因此提升了高可用性。反之，禁止 Unclean 领导者选举的好处在于维护了数据的一致性，避免了消息丢失，但牺牲了高可用性。 一个分布式系统通常只能同时满足一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）中的两个。显然，在这个问题上，Kafka 赋予你选择 C 或 A 的权利。 强烈建议不要开启unclean leader election，毕竟我们还可以通过其他的方式来提升高可用性。如果为了这点儿高可用性的改善，牺牲了数据一致性，那就非常不值当了。 ps1：leader副本的选举也可以理解为分区leader的选举 ps2：broker的leader选举与分区leader的选举不同， Kafka的Leader选举是通过在zookeeper上创建/controller临时节点来实现leader选举，并在该节点中写入当前broker的信息 {“version”:1,”brokerid”:1,”timestamp”:”1512018424988”} 利用Zookeeper的强一致性特性，一个节点只能被一个客户端创建成功，创建成功的broker即为leader，即先到先得原则，leader也就是集群中的controller，负责集群中所有大小事务。 当leader和zookeeper失去连接时，临时节点会删除，而其他broker会监听该节点的变化，当节点删除时，其他broker会收到事件通知，重新发起leader选举 Kafka副本机制 - jet_qiu - 博客园 "},"docs/Guide/ElasticSearch整理.html":{"url":"docs/Guide/ElasticSearch整理.html","title":"ElasticSearch","keywords":"","body":"1.为什么要使用Elasticsearch? 　　因为在我们商城中的数据，将来会非常多，所以采用以往的模糊查询，模糊查询前置配置，会放弃索引，导致商品查询是全表扫面，在百万级别的数据库中，效率非常低下，而我们使用ES做一个全文索引，我们将经常查询的商品的某些字段，比如说商品名，描述、价格还有id这些字段我们放入我们索引库里，可以提高查询速度。 2.Elasticsearch是如何实现Master选举的？ 　　Elasticsearch的选主是ZenDiscovery模块负责的，主要包含Ping（节点之间通过这个RPC来发现彼此）和Unicast（单播模块包含一个主机列表以控制哪些节点需要ping通）这两部分； 　　对所有可以成为master的节点（node.master: true）根据nodeId字典排序，每次选举，每个节点都把自己所知道节点排一次序，然后选出第一个（第0位）节点，暂且认为它是master节点。 　　如果对某个节点的投票数达到一定的值（可以成为master节点数n/2+1）并且该节点自己也选举自己，那这个节点就是master。否则重新选举一直到满足上述条件。 补充：master节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data节点可以关闭http功能。 3.Elasticsearch中的节点（比如共20个），其中的10个选了一个master，另外10个选了另一个master，怎么办？ 　　当集群master候选数量不小于3个时，可以通过设置最少投票通过数量（discovery.zen.minimum_master_nodes）超过所有候选节点一半以上来解决脑裂问题； 当候选数量为两个时，只能修改为唯一的一个master候选，其他作为data节点，避免脑裂问题。 4.详细描述一下Elasticsearch索引（写）文档的过程。 　　协调节点默认使用文档ID参与计算（也支持通过routing），以便为路由提供合适的分片。 　　shard = hash(document_id) % (num_of_primary_shards) 　　当分片所在的节点接收到来自协调节点的请求后，会将请求写入到Memory Buffer，然后定时（默认是每隔1秒）写入到Filesystem Cache，这个从Momery Buffer到Filesystem Cache的过程就叫做refresh； 　　当然在某些情况下，存在Momery Buffer和Filesystem Cache的数据可能会丢失，ES是通过translog的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到translog中，当Filesystem cache中的数据写入到磁盘中时，才会清除掉，这个过程叫做flush； 　　在flush过程中，内存中的缓冲将被清除，内容被写入一个新段，段的fsync将创建一个新的提交点，并将内容刷新到磁盘，旧的translog将被删除并开始一个新的translog。 　　flush触发的时机是定时触发（默认30分钟）或者translog变得太大（默认为512M）时； 5.详细描述一下Elasticsearch更新和删除文档的过程 　　删除和更新也都是写操作，但是Elasticsearch中的文档是不可变的，因此不能被删除或者改动以展示其变更； 　　磁盘上的每个段都有一个相应的.del文件。当删除请求发送后，文档并没有真的被删除，而是在.del文件中被标记为删除。该文档依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在.del文件中被标记为删除的文档将不会被写入新段。 　　在新的文档被创建时，Elasticsearch会为该文档指定一个版本号，当执行更新时，旧版本的文档在.del文件中被标记为删除，新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，但是会在结果中被过滤掉。 6.详细描述一下Elasticsearch搜索的过程 　　搜索被执行成一个两阶段过程，我们称之为 Query Then Fetch； 　　在初始查询阶段时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列。PS：在搜索的时候是会查询Filesystem Cache的，但是有部分数据还在Memory Buffer，所以搜索是近实时的。 　　每个分片返回各自优先队列中 所有文档的 ID 和排序值 给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。 　　接下来就是 取回阶段，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。每个分片加载并丰富文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。 　　补充：Query Then Fetch的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确，DFS Query Then Fetch增加了一个预查询的处理，询问Term和Document frequency，这个评分更准确，但是性能会变差。 9.Elasticsearch对于大数据量（上亿量级）的聚合如何实现？ 　　Elasticsearch 提供的首个近似聚合是cardinality 度量。它提供一个字段的基数，即该字段的distinct或者unique值的数目。它是基于HLL算法的。HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。其特点是：可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）；小的数据集精度是非常高的；我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关 . 10.在并发情况下，Elasticsearch如果保证读写一致？ 　　可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突； 　　另外对于写操作，一致性级别支持quorum/one/all，默认为quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。 　　对于读操作，可以设置replication为sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置replication为async时，也可以通过设置搜索请求参数_preference为primary来查询主分片，确保文档是最新版本。 14.ElasticSearch中的集群、节点、索引、文档、类型是什么？ 　　群集是一个或多个节点（服务器）的集合，它们共同保存您的整个数据，并提供跨所有节点的联合索引和搜索功能。群集由唯一名称标识，默认情况下为“elasticsearch”。此名称很重要，因为如果节点设置为按名称加入群集，则该节点只能是群集的一部分。 　　节点是属于集群一部分的单个服务器。它存储数据并参与群集索引和搜索功能。 　　索引就像关系数据库中的“数据库”。它有一个定义多种类型的映射。索引是逻辑名称空间，映射到一个或多个主分片，并且可以有零个或多个副本分片。 MySQL =>数据库 　　 ElasticSearch =>索引 　　文档类似于关系数据库中的一行。不同之处在于索引中的每个文档可以具有不同的结构（字段），但是对于通用字段应该具有相同的数据类型。 MySQL => Databases => 　 Tables => Columns / Rows ElasticSearch => Indices => Types =>具有属性的文档 　　类型是索引的逻辑类别/分区，其语义完全取决于用户。 15.ElasticSearch中的分片是什么? 　　在大多数环境中，每个节点都在单独的盒子或虚拟机上运行。 　　索引 - 在Elasticsearch中，索引是文档的集合。 　　分片 -因为Elasticsearch是一个分布式搜索引擎，所以索引通常被分割成分布在多个节点上的被称为分片的元素。 什么是ElasticSearch？ Elasticsearch是一个基于Lucene的搜索引擎。它提供了具有HTTP Web界面和无架构JSON文档的分布式，多租户能力的全文搜索引擎。Elasticsearch是用Java开发的，根据Apache许可条款作为开源发布。 Elasticsearch中的倒排索引是什么？ 倒排索引是搜索引擎的核心。搜索引擎的主要目标是在查找发生搜索条件的文档时提供快速搜索。倒排索引是一种像数据结构一样的散列图，可将用户从单词导向文档或网页。它是搜索引擎的核心。其主要目标是快速搜索从数百万文件中查找数据。 ElasticSearch中的集群、节点、索引、文档、类型是什么？ 群集是一个或多个节点（服务器）的集合，它们共同保存您的整个数据，并提供跨所有节点的联合索引和搜索功能。群集由唯一名称标识，默认情况下为“elasticsearch”。此名称很重要，因为如果节点设置为按名称加入群集，则该节点只能是群集的一部分。 节点是属于集群一部分的单个服务器。它存储数据并参与群集索引和搜索功能。 索引就像关系数据库中的“数据库”。它有一个定义多种类型的映射。索引是逻辑名称空间，映射到一个或多个主分片，并且可以有零个或多个副本分片。 MySQL =>数据库 ElasticSearch =>索引 文档类似于关系数据库中的一行。不同之处在于索引中的每个文档可以具有不同的结构（字段），但是对于通用字段应该具有相同的数据类型。 MySQL => Databases => Tables => Columns / Rows ElasticSearch => Indices => Types =>具有属性的文档 类型是索引的逻辑类别/分区，其语义完全取决于用户。 ElasticSearch是否有架构？ ElasticSearch可以有一个架构。架构是描述文档类型以及如何处理文档的不同字段的一个或多个字段的描述。Elasticsearch中的架构是一种映射，它描述了JSON文档中的字段及其数据类型，以及它们应该如何在Lucene索引中进行索引。因此，在Elasticsearch术语中，我们通常将此模式称为“映射”。 Elasticsearch具有架构灵活的能力，这意味着可以在不明确提供架构的情况下索引文档。如果未指定映射，则默认情况下，Elasticsearch会在索引期间检测文档中的新字段时动态生成一个映射。 ElasticSearch中的分片是什么？ 在大多数环境中，每个节点都在单独的盒子或虚拟机上运行。 索引 - 在Elasticsearch中，索引是文档的集合。 分片 -因为Elasticsearch是一个分布式搜索引擎，所以索引通常被分割成分布在多个节点上的被称为分片的元素。 ElasticSearch中的副本是什么？ 一个索引被分解成碎片以便于分发和扩展。副本是分片的副本。一个节点是一个属于一个集群的ElasticSearch的运行实例。一个集群由一个或多个共享相同集群名称的节点组成。 ElasticSearch中的分析器是什么？ 在ElasticSearch中索引数据时，数据由为索引定义的Analyzer在内部进行转换。 分析器由一个Tokenizer和零个或多个TokenFilter组成。编译器可以在一个或多个CharFilter之前。分析模块允许您在逻辑名称下注册分析器，然后可以在映射定义或某些API中引用它们。 Elasticsearch附带了许多可以随时使用的预建分析器。或者，您可以组合内置的字符过滤器，编译器和过滤器器来创建自定义分析器。 什么是ElasticSearch中的编译器？ 编译器用于将字符串分解为术语或标记流。一个简单的编译器可能会将字符串拆分为任何遇到空格或标点的地方。Elasticsearch有许多内置标记器，可用于构建自定义分析器。 启用属性，索引和存储的用途是什么？ enabled属性适用于各类ElasticSearch特定/创建领域，如index和size。用户提供的字段没有“已启用”属性。 存储意味着数据由Lucene存储，如果询问，将返回这些数据。 存储字段不一定是可搜索的。默认情况下，字段不存储，但源文件是完整的。因为您希望使用默认值(这是有意义的)，所以不要设置store属性 该指数属性用于搜索。 索引属性只能用于搜索。只有索引域可以进行搜索。差异的原因是在分析期间对索引字段进行了转换，因此如果需要的话，您不能检索原始数据。 ElasticSearch读写原理 写操作原理 Coordinating Node 协调节点默认使用文档ID参与计算（也支持通过routing），以便为路由提供合适的分片。 shard = hash(document_id) % (num_of_primary_shards) Shard 当分片所在的节点接收到来自协调节点的请求后，会将请求写入到Memory Buffer，然后定时（默认是每隔1秒）写入到Filesystem Cache，这个从Momery Buffer到Filesystem Cache的过程就叫做refresh。 Memory Buffer | Filesystem Cache 当然在某些情况下，存在Momery Buffer和Filesystem Cache的数据可能会丢失，ES是通过translog的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到translog中，当Filesystem cache中的数据写入到磁盘中时，才会清除掉，这个过程叫做flush。 Transaction Log 在flush过程中，内存中的缓冲将被清除，内容被写入一个新段，段的fsync将创建一个新的提交点，并将内容刷新到磁盘，旧的translog将被删除并开始一个新的translog。 flush触发的时机是定时触发（默认30分钟）或者translog变得太大（默认为512M）时。 在两次fsync操作之间，存储在内存和文件系统缓存中的文档是不安全的，一旦出现断电这些文档就会丢失。所以ES引入了translog来记录两次fsync之间所有的操作，这样机器从故障中恢复或者重新启动，ES便可以根据translog进行还原。 当然，translog本身也是文件，存在于内存当中，如果发生断电一样会丢失。因此，ES会在每隔5秒时间或是一次写入请求完成后将translog写入磁盘。可以认为一个对文档的操作一旦写入磁盘便是安全的可以复原的，因此只有在当前操作记录被写入磁盘，ES才会将操作成功的结果返回发送此操作请求的客户端。 此外，由于每一秒就会生成一个新的segment，很快将会有大量的segment。对于一个分片进行查询请求，将会轮流查询分片中的所有segment，这将降低搜索的效率。因此ES会自动启动合并segment的工作，将一部分相似大小的segment合并成一个新的大segment。合并的过程实际上是创建了一个新的segment，当新segment被写入磁盘，所有被合并的旧segment被清除。 读操作原理 搜索被执行成一个两阶段过程，我们称之为 Query Then Fetch； Coordinating Node 在初始查询阶段时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列。PS：在搜索的时候是会查询Filesystem Cache的，但是有部分数据还在Memory Buffer，所以搜索是近实时的。 Shard 每个分片返回各自优先队列中 所有文档的 ID 和排序值 给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。 Coordinating Node 接下来就是 取回阶段，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。每个分片加载并 丰富 文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。 补充：Query Then Fetch的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确，DFS Query Then Fetch增加了一个预查询的处理，询问Term和Document frequency，这个评分更准确，但是性能会变差。 更新（Update）和删除（Delete）文档 ES的索引是不能修改的，因此更新和删除操作并不是直接在原索引上直接执行。 每一个磁盘上的segment都会维护一个del文件，用来记录被删除的文件。每当用户提出一个删除请求，文档并没有被真正删除，索引也没有发生改变，而是在del文件中标记该文档已被删除。因此，被删除的文档依然可以被检索到，只是在返回检索结果时被过滤掉了。每次在启动segment合并工作时，那些被标记为删除的文档才会被真正删除。 更新文档会首先查找原文档，得到该文档的版本号。然后将修改后的文档写入内存，此过程与写入一个新文档相同。同时，旧版本文档被标记为删除，同理，该文档可以被搜索到，只是最终被过滤掉。 集群管理和监控 通过管理和监控部分的API，用户可以更改集群的设置。比如调整节点发现机制(discovery mechanism) 或者更改索引的分片策略。用户可以查看集群状态信息，或者每个节点和索引和统计信息。 如:获取集群健康信息 GET /cluster/health 获取集群配置信息 GET /cluster/settings 下图为chrome head插件的集群展示结果： 可以看到集群有多少个节点，集群健康值等。 Elasticsearch是如何做到快速索引的 ElasticSearch原理 - 神一样的存在 - 博客园 Posting List Elasticsearch分别为每个field都建立了一个倒排索引，Kate, John, 24, Female这些叫term，而[1,2]就是Posting List。Posting list就是一个int的数组，存储了所有符合某个term的文档id。 看到这里，不要认为就结束了，精彩的部分才刚开始... 通过posting list这种索引方式似乎可以很快进行查找，比如要找age=24的同学，爱回答问题的小明马上就举手回答：我知道，id是1，2的同学。但是，如果这里有上千万的记录呢？如果是想通过name来查找呢？ Term Dictionary Elasticsearch为了能快速找到某个term，将所有的term排个序，二分法查找term，logN的查找效率，就像通过字典查找一样，这就是Term Dictionary。现在再看起来，似乎和传统数据库通过B-Tree的方式类似啊，为什么说比B-Tree的查询快呢？ Term Index B-Tree通过减少磁盘寻道次数来提高查询性能，Elasticsearch也是采用同样的思路，直接通过内存查找term，不读磁盘，但是如果term太多，term dictionary也会很大，放内存不现实，于是有了Term Index，就像字典里的索引页一样，A开头的有哪些term，分别在哪页，可以理解term index是一颗树。 参考 ElasticSearch底层原理浅析_zkyfcx的博客-CSDN博客_elasticsearch原理 ElasticSearch常见经典面试题 - java未来的大佬 - 博客园 "},"docs/Guide/架构设计.html":{"url":"docs/Guide/架构设计.html","title":"架构设计","keywords":"","body":"系统设计 定时任务系统设计 秒杀系统 秒杀系统设计 ID生成器系统 分布式ID生成器系统设计 抢红包系统 如何设计一个小而美的红包秒杀系统？ Feed流系统 Feed流系统的设计 点赞系统 点赞系统设计 优惠券系统 浅谈优惠券系统设计_usst_lidawei的专栏-CSDN博客 优惠券系统设计 - 简书 谈谈优惠券系统的设计_文刀的博客天地-CSDN博客 怎么保证接口幂等 怎么解决商品中超卖的问题 微服务 怎么理解微服务 服务如何划分 可以从哪几个方面去划分 为什么这样划分 微服务带来了哪些好处，哪些坏处，如何看待这个问题 "},"docs/Guide/分布式.html":{"url":"docs/Guide/分布式.html","title":"分布式相关","keywords":"","body":"分布式原则 CAP定理 我们往往为了可用性和分区容错性，忍痛放弃强一致支持，转而追求最终一致性。大部分业务场景下，我们是可以接受短暂的不一致的。 一致性和可用性，为什么不可能同时成立？答案很简单，因为可能通信失败（即出现分区容错）。 如果保证 G2 的一致性，那么 G1 必须在写操作时，锁定 G2 的读操作和写操作。只有数据同步后，才能重新开放读写。锁定期间，G2 不能读写，没有可用性不。 如果保证 G2 的可用性，那么势必不能锁定 G2，所以一致性不成立。 综上所述，G2 无法同时做到一致性和可用性。系统设计时只能选择一个目标。如果追求一致性，那么无法保证所有节点的可用性；如果追求所有节点的可用性，那就没法做到一致性。 CAP三者不可兼得，该如何取舍： (1) CA: 优先保证一致性和可用性，放弃分区容错。 这也意味着放弃系统的扩展性，系统不再是分布式的，有违设计的初衷。 (2) CP: 优先保证一致性和分区容错性，放弃可用性。在数据一致性要求比较高的场合(譬如:zookeeper,Hbase) 是比较常见的做法，一旦发生网络故障或者消息丢失，就会牺牲用户体验，等恢复之后用户才逐渐能访问。 (3) AP: 优先保证可用性和分区容错性，放弃一致性。NoSQL中的Cassandra 就是这种架构。跟CP一样，放弃一致性不是说一致性就不保证了，而是逐渐的变得一致。 应用CAP分析 应用 类型 其他 Mysql CA 主从模式为AP ZooKeeper CP 在分区后，对于A，只有分区内节点大于quorum才对外服务 Eureka AP 最终一致性 Redis哨兵/集群模式 AP 最终一致性，单体肯定是CA RocketMQ主从 AP 最终一致性 分布式事务-2pc CP 锁住资源,该资源其他请求阻塞 分布式事务-TCC AP 最终一致性 分布式事务-最大努力尝试 AP 最终一致性 BASE理论 在分布式系统中，我们往往追求的是可用性，它的重要程序比一致性要高，那么如何实现高可用性呢？ 前人已经给我们提出来了另外一个理论，就是BASE理论，它是用来对CAP定理进行进一步扩充的。BASE理论指的是： Basically Available（基本可用） Soft state（软状态） Eventually consistent（最终一致性） BASE理论是对CAP中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。 有了以上理论之后，我们来看一下分布式事务的问题。 分布式事务 是指在分布式服务下，不同服务之间保持事务操作的。 对不同数据库的操作必然需要引入分布式事务。 常见的分布式事务解决方案 基于XA协议的两阶段提交 消息事务+最终一致性 TCC编程模式 微服务 几大主要构成框架组件： 共识算法 传统分布式系统领域的 Paxos、Raft 以及密码货币中使用的工作量证明（POW）、权益证明（POS）和委托权益证明（DPOS）。 Paxos 和 Raft 是目前分布式系统领域中两种非常著名的解决一致性问题的共识算法，两者都能解决分布式系统中的一致性问题，但是前者的实现与证明非常难以理解，后者的实现比较简洁并且遵循人的直觉，它的出现就是为了解决 Paxos 难以理解并和难以实现的问题。 ZAB与Paxos算法的联系与区别_大数据_有趣的难受-CSDN博客 共识算法：Raft - 简书 10分钟弄懂Raft算法 - 云+社区 - 腾讯云 Raft算法详解 - 左手编程右手诗 - 博客园 RAFT算法详解_网络_青萍之末的博客-CSDN博客 可视化Raft算法 一个分布式网络节点，每个节点有三种状态：Follower，Candidate，Leader，状态之间是互相转换的。 在 Raft 运行过程中，最主要进行两个活动： 选主 Leader Election 复制日志 Log Replication 选主： 假设现在有5个节点，5个节点一开始的状态都是 Follower。 在一个节点倒计时结束 (Timeout) 后，这个节点的状态变成 Candidate 开始选举，它给其他几个节点发送选举请求 (RequestVote) 其他四个节点都返回成功，这个节点的状态由 Candidate 变成了 Leader，并在每个一小段时间后，就给所有的 Follower 发送一个 Heartbeat 以保持所有节点的状态，Follower 收到 Leader 的 Heartbeat 后重设 Timeout。 这是最简单的选主情况，只要有超过一半的节点投支持票了，Candidate 才会被选举为 Leader，5个节点的情况下，3个节点 (包括 Candidate 本身) 投了支持就行。 "},"docs/Guide/一致性哈希.html":{"url":"docs/Guide/一致性哈希.html","title":"一致性哈希","keywords":"","body":"使用Hash的问题 使用一般Hash算法进行缓存时，会出现一些缺陷，主要体现在服务器数量变动的时候，所有缓存的位置都要发生改变！ 试想一下，如果4台缓存服务器已经不能满足我们的缓存需求，那么我们应该怎么做呢？很简单，多增加几台缓存服务器不就行了！假设：我们增加了一台缓存服务器，那么缓存服务器的数量就由4台变成了5台。那么原本hash(a.png) % 4 = 2 的公式就变成了hash(a.png) % 5 = ？ ， 可想而知这个结果肯定不是2的，这种情况带来的结果就是当服务器数量变动时，所有缓存的位置都要发生改变！换句话说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端数据库请求数据！（缓存雪崩） 同样的，假设4台缓存中突然有一台缓存服务器出现了故障，无法进行缓存，那么我们则需要将故障机器移除，但是如果移除了一台缓存服务器，那么缓存服务器数量从4台变为3台，也是会出现上述的问题！ 所以，我们应该想办法不让这种情况发生，但是由于上述Hash算法本身的缘故，使用取模法进行缓存时，这种情况是无法避免的，为了解决这些问题，Hash一致性算法（一致性Hash算法）诞生了！ 一致性Hash算法 Distributed Hash Table（DHT） 是一种哈希分布方式，其目的是为了克服传统哈希分布在服务器节点数量变化时大量数据迁移的问题。 基本原理 将哈希空间 [0, 2^n^-1] 看成一个哈希环，每个服务器节点都配置到哈希环上。每个数据对象通过哈希取模得到哈希值之后，存放到哈希环中顺时针方向第一个大于等于该哈希值的节点上。 一致性Hash算法也是使用取模的方法，只是，刚才描述的取模法是对服务器的数量进行取模，而一致性Hash算法是对2^32^取模，什么意思呢？简单来说，一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-2^32^-1（即哈希值是一个32位无符号整形），整个哈希环； 整个空间按顺时针方向组织，圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到2^32^-1，也就是说0点左侧的第一个点代表2^32^-1， 0和2^32^-1在零点中方向重合，我们把这个由2^32^个点组成的圆环称为Hash环。 下一步将各个服务器使用Hash进行一个哈希，具体可以选择服务器的IP或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用IP地址哈希后在环空间的位置如下: 一致性Hash算法的容错性和可扩展性 一致性哈希在增加或者删除节点时只会影响到哈希环中相邻的节点，例如下图中新增节点 X，只需要将它前一个节点 C 上的数据重新进行分布即可，对于节点 A、B、D 都没有影响。 综上所述，一致性Hash算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。 Hash环的数据倾斜问题 一致性Hash算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）问题，例如系统中只有两台服务器，其环分布如下： 此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上。为了解决这种数据倾斜问题，一致性Hash算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器IP或主机名的后面增加编号来实现。 例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点： 同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。 疑问 在节点增删时，数据如何迁移，难道就丢失了？ 引用 一致性Hash问题_数据库_CoderTnT的博客-CSDN博客 图解一致性hash算法和实现 - 简书 "},"docs/Guide/限流算法.html":{"url":"docs/Guide/限流算法.html","title":"限流算法","keywords":"","body":"什么是限流 限流是对系统的出入流量进行控制，防止大流量出入，导致资源不足，系统不稳定。 限流系统是对资源访问的控制组件，控制主要的两个功能：限流策略和熔断策略，对于熔断策略，不同的系统有不同的熔断策略诉求，有的系统希望直接拒绝、有的系统希望排队等待、有的系统希望服务降级、有的系统会定制自己的熔断策略，这里只针对限流策略这个功能做详细的设计。 限流算法 核心的算法主要就是四种： A 类：计数器法，滑动窗口法 B 类：令牌桶法，漏桶法 这里的四种算法通常都是在应用级别讨论的，这里不重复介绍这四种算法的实现思路了，只不过我人为的将他们分成了 A，B 两类。 A 类算法，否决式限流。即如果系统设定限流方案是 1 分钟允许 100 次调用，那么真实请求 1 分钟调用 200 次的话，意味着超出的 100 次调用，得到的是空结果或者调用频繁异常。（滑动窗口法是计数器法的升级，因为计数器法会存在临界值问题，不是相对的时间） B 类算法，阻塞式限流。即如果系统设定限流方案是 1 分钟允许 100 次调用，那么真实请求 1 分钟调用 200 次的话，意味着超出的 100 次调用，会均匀安排到下一分钟返回。（当然 B 类算法，也可以立即返回失败，也可以达到否决式限流的效果） 固定窗口和滑动窗口算法了解一下 - 眯眯眼猫头鹰的小树杈 - SegmentFault 思否 1、限制瞬时并发数 Guava RateLimiter 提供了令牌桶算法实现：平滑突发限流(SmoothBursty)和平滑预热限流(SmoothWarmingUp)实现。 2、限制某个接口的时间窗最大请求数 即一个时间窗口内的请求数，如想限制某个接口/服务每秒/每分钟/每天的请求数/调用量。如一些基础服务会被很多其他系统调用，比如商品详情页服务会调用基础商品服务调用，但是怕因为更新量比较大将基础服务打挂，这时我们要对每秒/每分钟的调用量进行限速；一种实现方式如下所示： LoadingCache counter = CacheBuilder.newBuilder() .expireAfterWrite(2, TimeUnit.SECONDS) .build(new CacheLoader() { @Override public AtomicLong load(Long seconds) throws Exception { return new AtomicLong(0); } }); long limit = 1000; while(true) { //得到当前秒 long currentSeconds = System.currentTimeMillis() / 1000; if(counter.get(currentSeconds).incrementAndGet() > limit) { System.out.println(\"限流了:\" + currentSeconds); continue; } //业务处理 } 使用Guava的Cache来存储计数器，过期时间设置为2秒（保证1秒内的计数器是有的），然后我们获取当前时间戳然后取秒数来作为KEY进行计数统计和限流，这种方式也是简单粗暴，刚才说的场景够用了。 3、基于 Redis 实现分布式限流 方案1：存储两个 key，一个用于计时，一个用于计数。请求每调用一次，计数器增加 1，若在计时器时间内计数器未超过阈值，则可以处理任务。（这个方案没太懂怎么计时的key怎么运用） 方案2：用一个key自增请求并设置过期时间 local key = \"rate.limit:\" .. KEYS[1] --限流KEY local limit = tonumber(ARGV[1]) --限流大小 local time = tonumber(ARGV[2]) --限流时间窗口 local current = tonumber(redis.call('get', key) or \"0\") if current + 1 > limit then --如果超出限流大小 return 0 else -- 请求数+1 redis.call(\"INCRBY\", key, \"1\") redis.call(\"expire\", key, time) return current + 1 end 常用的限流算法 常用的限流算法有两种：漏桶算法和令牌桶算法 令牌桶算法 令牌桶算法的原理是系统会以一个恒定的速度往桶里放入令牌，而如果请求需要被处理，则需要先从桶里获取一个令牌，当桶里没有令牌可取时，则拒绝服务。 代表：Guava RateLimiter 最快的方式是使用 RateLimit 类，但是这仅限制在单节点，如果是分布式系统，每个节点的 QPS 是一样的，请求量到服务接口那的话就是 QPS * 节点数 了。所以这种方案在分布式的情况下不适用！ 两种模式： SmoothBursty 平滑突发限流 （可以预消费来保证突发流量） SmoothWarmingUp 平滑预热限流 （冷启动时会以一个比较大的速率慢慢到平均速率；然后趋于平均速率） 漏桶算法 漏桶算法思路很简单，水（请求）先进入到漏桶里，漏桶以一定的速度出水，当水流入速度过大会直接溢出，可以看出漏桶算法能强行限制数据的传输速率。 分布式限流 而分布式限流常用的则有 Hystrix、resilience4j、Sentinel 等框架，但这些框架都需引入第三方的类库，对于国企等一些保守的企业，引入外部类库都需要经过层层审批，较为麻烦。 基于redis的分布式RateLimiter(限流)实现 - 林中小舍 - SegmentFault 思否 Java并发：分布式应用限流 Redis + Lua 实践 - 搜云库技术团队 - SegmentFault 思否 分布式服务限流实战，已经为你排好坑了-InfoQ 接入层限流 接入层通常指请求流量的入口，该层的主要目的有：负载均衡、非法请求过滤、请求聚合、缓存、降级、限流、A/B测试、服务质量监控等等 对于Nginx接入层限流可以使用Nginx自带了两个模块： 连接数限流模块ngx_http_limit_conn_module 漏桶算法实现的请求限流模块ngx_http_limit_req_module。 OpenResty提供的Lua限流模块lua-resty-limit-traffic进行更复杂的限流场景。 limit_conn用来对某个KEY对应的总的网络连接数进行限流，可以按照如IP、域名维度进行限流。 limit_req用来对某个KEY对应的请求的平均速率进行限流，并有两种用法：平滑模式（delay）和允许突发模式(nodelay)。 谈谈高并发系统的限流 - nick hao - 博客园 高并发系统限流设计 - 简书 相关方案 业务端限流： 业务端做限流的话，请求来源于上有系统，流量要求是比较平稳的，峰值不能太高，否则可能一瞬间打挂系统，令牌桶和计数器方式就不太合适了。因为如果流量直接打到业务系统我们是没法进行预估的，大概率会有突发流量，所以选择直接使用流量桶就比较合适了。 consumer 控制对下游流量： 我所处的业务场景是nginx + lua 打入redis 则认为成功，consumer端消费消息，然后持续固定qps对下游发起请求。这种场景下，把消费速率&对下游qps控制放在consumer端来做就比较合适了。首先我们从Redis或者日志文件中读取了数据，并且拼接了请求任务放到任务队列中。然后线程池从任务队列中取任务发起请求，首先我们需要控制加入任务队列的速率，因为加入的任务队列的速度大于任务队列的消费速度，肯定是会导致OOM产生的， 我这里使用的是类似令牌桶的方式，一个线程取n个任务，然后线程内串行，几个线程并行，这样就一定程度上保证了不会出现令牌桶的流量不均问题了，同时减少了锁的争用。实际上使用漏斗算法也是合适的，但是对于这个场景来说，溢出任务实际上是不太好控制，需要让请求的加入速率与消费速度相对保持一致，这一点控制不好很容易oom的，所以直接采用任务队列 + 令牌桶实现是最方便控制也是最容易实现的，采用线程内一次取多个，串行发起请求的方式是可以一定程度上控制住流量的 实践证明效果不错。 高并发下的限流策略 - 云+社区 - 腾讯云 参考 Guava RateLimiter源码解析以及分布式限流总结_maoyeqiu的专栏-CSDN博客 基于Redis的限流系统的设计 - 作业部落 Cmd Markdown 编辑阅读器 RateLimiter配合ConcurrentHashMap对用户进行简单限流 - 沉默王二博客 基于分布式环境下限流系统的设计 | zhisheng的博客 分布式环境下限流方案的实现redis RateLimiter Guava,Token Bucket, Leaky Bucket - 沧海一滴 - 博客园 Guava RateLimiter限流 - 掘金 基于Redis和Lua的分布式限流 - 知乎 "},"docs/Guide/阿里Sentinel.html":{"url":"docs/Guide/阿里Sentinel.html","title":"阿里Sentinel","keywords":"","body":"ProcessorSlotChain Sentinel 的核心骨架，将不同的 Slot 按照顺序串在一起（责任链模式），从而将不同的功能（限流、降级、系统保护）组合在一起。slot chain 其实可以分为两部分：统计数据构建部分（statistic）和判断部分（rule checking）。核心结构： sentinel主要是基于7种不同的Slot形成了一个链表，每个Slot都各司其职，自己做完分内的事之后，会把请求传递给下一个Slot，直到在某一个Slot中命中规则后抛出BlockException而终止。 前三个Slot负责做统计，后面的Slot负责根据统计的结果结合配置的规则进行具体的控制，是Block该请求还是放行。 控制的类型也有很多可选项：根据qps、线程数、冷启动等等。 然后基于这个核心的方法，衍生出了很多其他的功能： 1、dashboard控制台，可以可视化的对每个连接过来的sentinel客户端 (通过发送heartbeat消息)进行控制，dashboard和客户端之间通过http协议进行通讯。 2、规则的持久化，通过实现DataSource接口，可以通过不同的方式对配置的规则进行持久化，默认规则是在内存中的 3、对主流的框架进行适配，包括servlet，dubbo，rRpc等 限流降级神器：哨兵(sentinel)原理分析-云栖社区-阿里云 Sentinel 核心类解析 · alibaba/Sentinel Wiki alibaba sentinel 1.3.0-GA 简述及原理_undergrowth的专栏-CSDN博客_alibaba sentinel 深入剖析阿里sentinel源码，看这篇就够了_Java666999的博客-CSDN博客_sentinel源码分析 阿里Sentinel原理解析 - 知乎 阿里限流中间件Sentinel 原理-如何为系统设置扩展点 - 简书 "},"docs/Guide/秒杀系统的设计.html":{"url":"docs/Guide/秒杀系统的设计.html","title":"秒杀系统设计","keywords":"","body":"秒杀系统 1、前端三板斧【扩容】+【限流】+【静态化】+ 【随机丢请求】 2、后端两条路【内存】+【排队】 后端一切服务的瓶颈都来自于数据库——MySQL 秒杀场景的特点 定时开始，秒杀时大量用户会在同一时间，抢购同一商品，网站瞬时流量激增。 库存有限，秒杀下单数量远远大于库存数量，只有少部分用户能够秒杀成功。 操作可靠，秒杀业务流程比较简单，一般就是下订单减库存。库存就是用户争夺的“资源”，实际被消费的“资源”不能超过计划要售出的“资源”，也就是不能被“超卖”。 系统隔离 业务隔离 + 技术隔离 + 数据库隔离 JS 和 HTML 同时缓存在 CDN 上面，让用户从离自己最近的 CDN 服务器上获取这些信息。 数据库单表500w，估算超过后分库分表。 客户端层 缓存：页面资源静态化 + CDN缓存 防止提前下单: 根据时间戳 Hash值等判断，不要将活动入口提前暴露。 限流：设计一些问答或者滑块的功能，减少此类机器人对服务器的压力。 代理层 缓存，以 Nginx 为例，它可以缓存用户的信息。假设用户信息的修改没有那么频繁，即使有类似的修改也可以通过更新服务来刷新。总比从服务器上获取效率要高得多。 过滤，既然缓存了用户信息，这里就可以过滤掉一些不满足条件的用户。 比如相同IP、相同userId频繁请求。 限流，每个服务器集群能够承受的压力都是有限的。代理层可以根据服务器集群能够承受的最大压力，设置流量的阀值。 通过 Nginx+Lua 合作完成，Lua 从服务注册中心读取服务健康状态，动态调整流量。 应用层 高并发 并发读： 采用分布式缓存甚至LocalCache来抵抗高并发读。即允许读场景下一定的脏数据，这样只会导致少量原本无库存的下单请求被误认为是有库存的，等到真正写数据时再保证最终一致性，由此做到高可用和一致性之间的平衡。 并发写： 写缓存，降低MySQL并发度； 高性能 消息队列（削峰填谷、解耦） 削去秒杀场景下的峰值写流量 通过异步处理简化秒杀请求中的业务流程 解耦，实现秒杀系统模块之间松耦合 当然改成了异步处理，前端在下单的时候，就没法立刻得到下单的结果，所以前端要做一个【抢购中。。。】的状态页面，在此页面中定时轮询下单结果。这样就大大提升了系统的吞吐量，降低了系统压力。 缓存 将存库从MySQL前移到Redis中，Redis的写性能和读性能都远高于MySQL，这就解决了高并发下的性能问题。然后通过队列等异步手段，将变化的数据异步写入到DB中。 超卖问题 引入队列：将所有写DB操作在单队列中排队，完全串行处理。当达到库存阀值的时候就不在消费队列，并关闭购买功能。这就解决了超卖问题。 优点：解决超卖问题，略微提升性能。 缺点：性能受限于队列处理机处理性能和DB的写入性能中最短的那个，另外多商品同时抢购的时候需要准备多条队列。 Redis的分布式锁：为了提高效率，会将这个库存信息放到缓存中。以流行的 Redis 为例，用它存放库存信息，由多个线程来访问就会出现资源争夺的情况。也就是分布式程序争夺唯一资源，为了解决这个问题我们需要实现分布式锁。 未付款的订单通过超时任务判断，恢复库存。 需要保证大并发请求时数据库中的库存字段值不能为负，一般有多种方案： 一是在通过事务来判断，即保证减后库存不能为负，否则就回滚； 二是直接设置数据库字段类型为无符号整数，这样一旦库存为负就会在执行 SQL 时报错； 三是使用 CASE WHEN 判断语句 UPDATE item SET inventory = CASE WHEN inventory >= xxx THEN inventory-xxx ELSE inventory END 业务手段保证商品卖的出去，技术手段保证商品不会超卖，库存问题从来就不是简单的技术难题，解决问题的视角是多种多样的。 减库存的方式 下单减库存。买家下单后，扣减商品库存。下单减库存是最简单的减库存方式，也是控制最为精确的一种 付款减库存。买家下单后，并不立即扣减库存，而是等到付款后才真正扣减库存。但因为付款时才减库存，如果并发比较高，可能出现买家下单后付不了款的情况，因为商品已经被其他人买走了 预扣库存。这种方式相对复杂一些，买家下单后，库存为其保留一定的时间（如 15 分钟），超过这段时间，库存自动释放，释放后其他买家可以购买 秒杀系统设计的 5 个要点：前端三板斧＋后端两条路！ - 云+社区 - 腾讯云 这一次，彻底弄懂“秒杀系统”-Java知音 如何设计一个秒杀系统-云栖社区-阿里云 秒杀业务架构优化之路 - InfoQ 如何设计秒杀系统？ - 知乎 架构师手把手教你如何设计一个秒杀系统？ - 简书 一个秒杀系统的设计思考 - 阿哲的专栏 - SegmentFault 思否 如何设计一个小而美的秒杀系统？ Java生鲜电商平台-秒杀系统最全设计?(小程序/APP) - 巨人大哥 - 博客园 "},"docs/Guide/Thrift-RPC.html":{"url":"docs/Guide/Thrift-RPC.html","title":"Thrift-RPC","keywords":"","body":"Thrift Server端的几种工作模式 TSimpleServer模式（单线程阻塞模式） 采用最简单的阻塞IO，只有一个工作线程，循环监听新请求的到来并完成对请求的处理，一次只能接收和处理一个socket连接。 TNonblockingServer（单线程NIO模式） 单线程工作，采用NIO的方式，所有的socket都被注册到selector中，在一个线程中通过seletor循环监控所有的socket。 优点：IO多路复用，非阻塞IO 缺点：单线程，请求任务一个一个执行。 THsHaServer（半同步半异步模式） TNonblockingServer类的子类，NIO方式， 主线程负责监听，线程池负责处理任务 缺点：新连接请求不能被及时接受。 TThreadPoolServer 阻塞socket方式工作，主线程负责阻塞式监听，业务处理交由一个线程池来处理。 TThreadedSelectorServer 目前Thrift提供的最高级的模式，专门的线程AcceptThread用于处理新连接请求，及时响应大量并发连接请求，负载均衡器分散到多个SelectorThread线程中来完成。 Thrift支持哪些通信协议格式 传输协议上总体上划分为文本(text)和二进制(binary)传输协议 TBinaryProtocol – 二进制编码格式进行数据传输。 TCompactProtocol – 这种协议非常有效的，使用Variable-Length Quantity (VLQ) 编码对数据进行压缩。高效和压缩的二进制格式 TJSONProtocol – 使用JSON的数据编码协议进行数据传输。 TSimpleJSONProtocol – 这种节约只提供JSON只写的协议，适用于通过脚本语言解析 TDebugProtocol – 在开发的过程中帮助开发人员调试用的，以文本的形式展现方便阅读。 Thrift RPC的工作原理 Thrift框架的远程过程调用的工作过程如下： 通过IDL定义一个接口的thrift文件，然后通过thrift的多语言编译功能，将接口定义的thrift文件翻译成对应的语言版本的接口文件； Thrift生成的特定语言的接口文件中包括客户端部分和服务器部分； 客户端通过接口文件中的客户端部分生成一个Client对象，这个客户端对象中包含所有接口函数的存根实现，然后用户代码就可以通过这个Client对象来调用thrift文件中的那些接口函数了，但是，客户端调用接口函数时实际上调用的是接口函数的本地存根实现，如图箭头1所示； 接口函数的存根实现将调用请求发送给thrift服务器端，然后thrift服务器根据调用的函数名和函数参数，调用实际的实现函数来完成具体的操作，如图箭头2所示； Thrift服务器在完成处理之后，将函数的返回值发送给调用的Client对象；如图箭头3所示； Thrift的Client对象将函数的返回值再交付给用户的调用函数，如图箭头4所示； 由浅入深了解Thrift（二）——Thrift工作原理_网络_逍遥子曰：-CSDN博客 "},"docs/Guide/gRPC.html":{"url":"docs/Guide/gRPC.html","title":"gRPC","keywords":"","body":" GRPC是google开源的一个高性能、跨语言的RPC框架，基于HTTP2协议，基于protobuf 3.x，基于Netty 4.x +。GRPC与thrift、avro-rpc等其实在总体原理上并没有太大的区别，简而言之GRPC并没有太多突破性的创新。（如下描述，均基于JAVA语言的实现） 对于开发者而言： 1）需要使用protobuf定义接口，即.proto文件 2）然后使用compile工具生成特定语言的执行代码，比如JAVA、C/C++、Python等。类似于thrift，为了解决跨语言问题。 3）启动一个Server端，server端通过侦听指定的port，来等待Client链接请求，通常使用Netty来构建，GRPC内置了Netty的支持。 4）启动一个或者多个Client端，Client也是基于Netty，Client通过与Server建立TCP长链接，并发送请求；Request与Response均被封装成HTTP2的stream Frame，通过Netty Channel进行交互。 GRPC的缺点: 1）GRPC尚未提供连接池 2）尚未提供“服务发现”、“负载均衡”机制 3）因为基于HTTP2，绝大部多数HTTP Server、Nginx都尚不支持，即Nginx不能将GRPC请求作为HTTP请求来负载均衡，而是作为普通的TCP请求。（nginx将会在1.9版本支持） 4）GRPC尚不成熟，易用性还不是很理想；就本人而言，我还是希望GRPC能够像hessian一样：无IDL文件，无需代码生成，接口通过HTTP表达。 5）Spring容器尚未提供整合。 在实际应用中，GRPC尚未完全提供连接池、服务自动发现、进程内负载均衡等高级特性，需要开发人员额外的封装；最大的问题，就是GRPC生成的接口，调用方式实在是不太便捷（JAVA），最起码与thrift相比还有差距，希望未来能够有所改进。 GRPC原理解析 - it610.com gRPC客户端创建和调用原理解析 - 后端 - 掘金 Tags | 搬砖工·甘罗 GRpc实战及原理分析 | grpc-in-action "},"docs/Guide/算法总结.html":{"url":"docs/Guide/算法总结.html","title":"算法总结","keywords":"","body":"排序算法（sort） 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面； 不稳定：如果a原本在b的前面，而a=b，排序之后a可能会出现在b的后面； 内排序：所有排序操作都在内存中完成； 外排序：由于数据太大，因此把数据放在磁盘中，而排序通过磁盘和内存的数据传输才能进行； 时间复杂度： 一个算法执行所耗费的时间。 空间复杂度：运行完一个程序所需内存的大小 秒懂排序算法 这或许是东半球讲十大排序算法最好的一篇文章 - 掘金 递归算法（recursion） 贪心算法（greed） 是指在每个阶段做选择的时候都做出当前阶段（或状态）最好的选择，并且期望这样做到的结果是全局最优解（但未必是全局最优解） 贪心算法其实是动态规划的一种,由于它的「贪心」，只着眼于当前阶段的最优解，所以每个子问题只会被计算一次，如果由此能得出全局最优解，相对于动态规划要对每个子问题求全局最优解，它的时间复杂度无疑是会下降一个量级的。 动态规划（dynamic programming，简称 dp） 经典题型： 最短路径问题 以下是我综合了动态规划的特点给出的动态规划的定义： 动态规划是一种多阶段决策最优解模型，一般用来求最值问题，多数情况下它可以采用自下而上的递推方式来得出每个子问题的最优解（即最优子结构），进而自然而然地得出依赖子问题的原问题的最优解。 划重点： 多阶段决策，意味着问题可以分解成子问题，子子问题，。。。，也就是说问题可以拆分成多个子问题进行求解 最优子结构，在自下而上的递推过程中，我们求得的每个子问题一定是全局最优解，既然它分解的子问题是全局最优解，那么依赖于它们解的原问题自然也是全局最优解。 自下而上，怎样才能自下而上的求出每个子问题的最优解呢，可以肯定子问题之间是有一定联系的，即迭代递推公式，也叫「状态转移方程」，要定义好这个状态转移方程， 我们就需要定义好每个子问题的状态（DP 状态），那为啥要自下而上地求解呢，因为如果采用像递归这样自顶向下的求解方式，子问题之间可能存在大量的重叠，大量地重叠子问题意味着大量地重复计算，这样时间复杂度很可能呈指数级上升（在下文中我们会看到多个这样重复的计算导致的指数级的时间复杂度），所以自下而上的求解方式可以消除重叠子问题。 最关键的解题思路： 定义子问题dp状态 状态转移方程 求解动态规划基本思路如下（解题四步曲） 判断是否可用递归来解，可以的话进入步骤 2 分析在递归的过程中是否存在大量的重复子问题 采用备忘录(记忆)的方式来存子问题的解以避免大量的重复计算（剪枝） 改用自底向上的方式来递推，即动态规划 一文学会动态规划解题技巧 labuladong的算法小抄 - labuladong的算法小抄 清华学霸总结的动态规划4步曲，仅这篇动归够了 字符串 区分子串和子序列 子串 串中任意个连续的字符组成的子序列称为该串的子串 对于一个字符串变量，例如\"adereegfbw\",它的子串就是像\"ader\"这样可以从中找到的连续的字符串。字符串\"adereegfbw\"本身也属于它本身最长的子串。 ab的子串：a、b、ab和一个空子串共4个即（2+1+1）个，abc的子串：a、 b、 c、 ab、 bc 、abc和一个空子串 共（3+2+1+1）个， 所以若字符串的长度为n,则子串的个数就是[n*(n+1)/2]+1个，\"software\"中非空子串的个数就是8+7+....+1=36个。 子序列 子数列，又称子序列，在数学中，某个序列的子序列是从最初序列通过 去除某些元素但不破坏余下元素的相对位置（在前或在后）而形成的新序列。 “AC”是“ABCDEFG”的子序列，而不是子串。 如字符串： \"pwwkew\" ， 子串是pww, wwk等很多个子串 是连在一起的 子序列是 pwk, pke等很多个子序列 ，但是子序列中的字符在字符串中不一定是连在一起的。 "},"docs/Guide/Elastic-Job原理.html":{"url":"docs/Guide/Elastic-Job原理.html","title":"Elastic-Job原理","keywords":"","body":"elastic-job实现分析 | Muser elastic-job-lite源码分析之选举及分片 - 简书 分布式定时任务调度平台Elastic-Job技术详解 Elastic-Job:动态添加任务,支持动态分片 - 掘金 SpringBoot整合Elastic-Job-lite，实现动态创建定时任务，任务持久化 | Fantasy 介绍 elastic-job的亮点主要如下： 基于quartz 定时任务框架为基础的，因此具备quartz的大部分功能 使用zookeeper做协调，调度中心，更加轻量级 支持任务的分片 支持弹性扩容 ， 可以水平扩展 ， 当任务再次运行时，会检查当前的服务器数量，重新分片，分片结束之后才会继续执行任务 失效转移，容错处理，当一台调度服务器宕机或者跟zookeeper断开连接之后，会立即停止作业，然后再去寻找其他空闲的调度服务器，来运行剩余的任务 提供运维界面，可以管理作业和注册中心 概念 分片概念：任务分布式的执行，需要将一个任务拆分成多个独立的任务项，然后由分布式的服务器分别执行某一个或几个分片项 个性化参数：shardingItemParameter，可以和分片项匹配对应关系。比如：将商品的状态分成上架，下架。那么配置0=上架,1=下架，代码中直接使用上架下架的枚举值即可完成分片项与业务逻辑的对应关系 作用高可用：将分片总数设置成1，多台服务器执行作业将采用1主n从的方式执行 弹性扩容：将任务拆分为n个任务项后，各个服务器分别执行各自分配到的任务项。一旦有新的服器加入集群或有服务器宕机。Elastic-Job将保留本次任务不变，下次任务开始前重新分片。 并行调度：采用任务分片方式实现。将一个任务拆分为n个独立的任务项，由分布式的服务器并行执行各自分配到的分片项。 集中管理：采用基于zookepper的注册中心，集中管理和协调分布式作业的状态，分配和监听。外部系统可直接根据Zookeeper的数据管理和监控elastic-job。 定制化流程任务：作业可分为简单和数据流处理两种模式，数据流又分为高吞吐处理模式和顺序性处理模式，其中高吞吐处理模式可以开启足够多的线程快速的处理数据，而顺序性处理模式将每个分片项分配到一个独立线程，用于保证同一分片的顺序性，这点类似于kafka的分区顺序性。 Elastic-Job的具体模块的底层及如何实现 Elastic-Job采用去中心化设计，主要分为注册中心、数据分片、分布式协调、定时任务处理和定制化流程型任务等模块。 去中心化：指Elastic-Job没有调度中心这一概念。每个运行在集群中的作业服务器都是对等的，节点之间通过注册中心进行分布式协调。但elastic-job有主节点的概念，主节点用于处理一些集中式任务，如分片，清理运行时信息等，并无调度功能，定时调度都是由作业服务器自行触发。 注册中心：注册中心模块目前直接使用zookeeper，用于记录作业的配置，服务器信息以及作业运行状态。Zookeeper虽然很成熟，但原理复杂，使用较难，在海量数据支持的情况下也会有性能和网络问题。 数据分片：数据分片是elastic-job中实现分布式的重要概念，将真实数据和逻辑分片对应，用于解耦作业框架和数据的关系。作业框架只负责将分片合理的分配给相关的作业服务器，而作业服务器需要根据所分配的分片匹配数据进行处理。服务器分片目前都存储在注册中心中，各个服务器根据自己的IP地址拉取分片。 分布式协调：分布式协调模块用于处理作业服务器的动态扩容缩容。一旦集群中有服务器发生变化，分布式协调将自动监测并将变化结果通知仍存活的作业服务器。协调时将会涉及主节点选举，重分片等操作。目前使用的Zookeeper的临时节点和监听器实现主动检查和通知功能。 定时任务处理：定时任务处理根据cron表达式定时触发任务，目前有防止任务同时触发，错过任务重出发等功能。主要还是使用Quartz本身的定时调度功能，为了便于控制，每个任务都使用独立的线程池。 定制化流程型任务：定制化流程型任务将定时任务分为多种流程，有不经任何修饰的简单任务；有用于处理数据的fetchData/processData的数据流任务；以后还将增加消息流任务，文件任务，工作流任务等。用户能以插件的形式扩展并贡献代码。 Elastic-Job - 羽觞醉月 - 博客园 原理 举个典型的job场景，比如余额宝里的昨日收益，系统需要job在每天某个时间点开始，给所有余额宝用户计算收益。如果用户数量不多，我们可以轻易使用quartz来完成，我们让计息job在某个时间点开始执行，循环遍历所有用户计算利息，这没问题。可是，如果用户体量特别大，我们可能会面临着在第二天之前处理不完这么多用户。另外，我们部署job的时候也得注意，我们可能会把job直接放在我们的webapp里，webapp通常是多节点部署的，这样，我们的job也就是多节点，多个job同时执行，很容易造成重复执行，比如用户重复计息，为了避免这种情况，我们可能会对job的执行加锁，保证始终只有一个节点能执行，或者干脆让job从webapp里剥离出来，独自部署一个节点。 elastic-job就可以帮助我们解决上面的问题，elastic底层的任务调度还是使用的quartz，通过zookeeper来动态给job节点分片。 我们来看： 很大体量的用户需要在特定的时间段内计息完成 我们肯定是希望我们的任务可以通过集群达到水平扩展，集群里的每个节点都处理部分用户，不管用户数量有多庞大，我们只要增加机器就可以了，比如单台机器特定时间能处理n个用户，2台机器处理2n个用户，3台3n，4台4n...，再多的用户也不怕了。 使用elastic-job开发的作业都是zookeeper的客户端，比如我希望3台机器跑job，我们将任务分成3片，框架通过zk的协调，最终会让3台机器分别分配到0,1,2的任务片，比如server0-->0，server1-->1，server2-->2，当server0执行时，可以只查询id%3\\==0的用户，server1执行时，只查询id%3\\==1的用户，server2执行时，只查询id%3==2的用户。 任务部署多节点引发重复执行 在上面的基础上，我们再增加server3，此时，server3分不到任务分片，因为只有3片，已经分完了。没有分到任务分片的作业程序将不执行。 如果此时server2挂了，那么server2的分片项会分配给server3，server3有了分片，就会替代server2执行。 如果此时server3也挂了，只剩下server0和server1了，框架也会自动把server3的分片随机分配给server0或者server1，可能会这样，server0-->0，server1-->1,2。 这种特性称之为弹性扩容，即elastic-job名称的由来。 将任务拆分为n个任务项后，各个服务器分别执行各自分配到的任务项。一旦有新的服务器加入集群，或现有服务器下线，elastic-job将在保留本次任务执行不变的情况下，下次任务开始前触发任务重分片。 elastic-job的原理简介和使用 - 我和我的倔强 - 博客园 Elastic-Job-Lite框架的任务处理、执行等都是针对的分片项，也就是说框架面向的是定时任务的分片项，而不是定时任务本身。如果我们不打算对定时任务分片，那么可以把分片数设为1，这样在sharding节点下创建1个分片项0，0分片项将会被分配给一个instance，并启动执行。 Elastic-Job-Lite是提供失效转移功能的，即当正在执行的任务项遇到进程退出或机器宕机等故障，该任务项应该转移到某空闲服务器执行。但是，该功能存在bug： （1）失效转移只能在同一机器上不同实例间完成，跨机器无效； （2）没有判断机器宕机时任务项是否正在执行状态，而是只要遇到宕机，即使任务项没有开始执行，也被转移到其他机器上执行一遍，导致重复执行。 Elastic-Job-Lite是预分片，不是动态分片，即Elastic-Job-Lite是在服务启动时就完成分片项的创建和分配，并保存在zk节点上，而不是定时任务在每次启动时，根据机器的处理能力，重新分配任务项，例如：任务比较繁忙的机器不参与新一轮的任务项分配。这样做的目的是对zk弱依赖，如果进行真正的动态分片，对于秒级的定时任务将会产生很大影响，因为每次任务启动，应用服务器都要跟zookeeper进行通信，并执行重新分片的逻辑，频繁的通信，对于秒级定时任务，会错过很多次执行 分布式定时任务调度平台Elastic-Job技术详解_慕课手记 弹性扩容缩容 通过zk实现各服务的注册、控制及协调，以下是弹性分布的实现： 第一台服务器上线触发主服务器选举。主服务器一旦下线，则重新触发选举，选举过程中阻塞，只有主服务器选举完成，才会执行其他任务。 某作业服务器上线时会自动将服务器信息注册到注册中心，下线时会自动更新服务器状态。 主节点选举，服务器上下线，分片总数变更均更新重新分片标记。 定时任务触发时，如需重新分片，则通过主服务器分片，分片过程中阻塞，分片结束后才可执行任务。如分片过程中主服务器下线，则先选举主服务器，再分片。 通过上一项说明可知，为了维持作业运行时的稳定性，运行过程中只会标记分片状态，不会重新分片。分片仅可能发生在下次任务触发前。 每次分片都会按服务器IP排序，保证分片结果不会产生较大波动。 实现失效转移功能，在某台服务器执行完毕后主动抓取未分配的分片，并且在某台服务器下线后主动寻找可用的服务器执行任务。 失效转移 所谓失效转移，就是在执行任务的过程中遇见异常的情况，这个分片任务可以在其他节点再次执行。这个和上面的HA不同，对于HA，上面如果任务终止，那么不会在其他任务实例上再次重新执行。 Job的失效转移监听来源于FailoverListenerManager中JobCrashedJobListener的dataChanged方法。FailoverListenerManager监听的是zk的instance节点删除事件。如果任务配置了failover等于true，其中某个instance与zk失去联系或被删除，并且失效的节点又不是本身，就会触发失效转移逻辑。 首先，在某个任务实例失效时，elastic-job会在leader节点下面创建failover节点以及items节点。items节点下会有失效任务实例的原本应该做的分片好。比如，失效的任务实例原来负责分片1和2。那么items节点下就会有名字叫1的子节点，就代表分片1需要转移到其他节点上去运行。 然后，由于每个存活着的任务实例都会收到zk节点丢失的事件，哪个分片失效也已经在leader节点的failover子节点下。所以这些或者的任务实例就会争抢这个分片任务来执行。为了保证不重复执行，elastic-job使用了curator的LeaderLatch类来进行选举执行。在获得执行权后，就会在sharding节点的分片上添加failover节点，并写上任务实例，表示这个故障任务迁移到某一个任务实例上去完成。 执行完成后，会把相应的节点和数据删除，避免下一次重复执行。 支持并行调度 采用任务分片方式实现。将一个任务拆分为n个独立的任务项，由分布式的服务器并行执行各自分配到的分片项。 动态分片策略 默认包含三种分片策略： 基于平均分配算法的分片策略、 作业名的哈希值奇偶数决定IP升降序算法的分片策略、根据作业名的哈希值对Job实例列表进行轮转的分片策略，支持自定义分片策略 elastic-job的分片是通过zookeeper来实现的。分片的分片由主节点分配，如下三种情况都会触发主节点上的分片算法执行： a、新的Job实例加入集群 b、现有的Job实例下线（如果下线的是leader节点，那么先选举然后触发分片算法的执行） c、主节点选举 >>elastic-job使用了quartz的调度机制，内部原理一致，增加了性能和可用性。 >>elastic-job使用注册中心(zookeeper)替换了quartz的jdbc数据存储方式，性能有较大提升。 >> elastic-job增加了job的追踪(使用Listener)，便于monitor >>elastic-job使用了分片机制，可以将job分成多个任务项，放到不同的地方执行 >>elastic-job仅支持cronTrigger，quartz支持更多的trigger实现 Quartz框架原理 Quartz定时任务框架存在的问题 Quartz作为开源作业调度中的佼佼者，是作业调度的首选。但是集群环境中Quartz采用API的方式对任务进行管理，从而可以避免上述问题，但是同样存在以下问题： 问题一：调用API的的方式操作任务，不人性化； 问题二：需要持久化业务QuartzJobBean到底层数据表中，系统侵入性相当严重。 问题三：调度逻辑和QuartzJobBean耦合在同一个项目中，这将导致一个问题，在调度任务数量逐渐增多，同时调度任务逻辑逐渐加重的情况加，此时调度系统的性能将大大受限于业务； 问题四：quartz底层以“抢占式”获取DB锁并由抢占成功节点负责运行任务，会导致节点负载悬殊非常大； quartz的分布式调度策略是以数据库为边界资源的一种异步策略。各个调度器都遵守一个基于数据库锁的操作规则从而保证了操作的唯一性。同时多个节点的异步运行保证了服务的可靠。但这种策略有自己的局限性：集群特性对于高CPU使用率的任务效果很好，但是对于大量的短任务，各个节点都会抢占数据库锁，这样就出现大量的线程等待资源。这种情况随着节点的增加会越来越严重。 quartz的分布式只是解决了高可用的问题，并没有解决任务分片的问题，还是会有单机处理的极限。 XXL-Job 和 Elastic-Job的区别 不同点： xxl-job： 大众点评目前已接入XXL-JOB，内部别名《Ferrari》（Ferrari基于XXL-JOB的V1.1版本定制而成，新接入应用推荐升级最新版本）。 据最新统计, 自2016-01-21接入至2017-12-01期间，该系统已调度约100万次，表现优异。新接入应用推荐使用最新版本，因为经过数十个版本的更新，系统的任务模型、UI交互模型以及底层调度通讯模型都有了较大的优化和提升，核心功能更加稳定高效。 文档比较详细,xxl-job分为调度中心(中心式)和执行器(分布式)，调度中心基于集群Quartz实现并支持集群部署，可保证调度中心HA；任务分布式执行，任务\"执行器\"支持集群部署，可保证任务执行HA，其中调度中心集群基于DB，而其他两个框架用zookeeper崃实现分布式锁。 侧重的业务实现的简单和管理的方便，学习成本简单，失败策略和路由策略丰富。推荐使用在“用户基数相对少，服务器数量在一定范围内”的情景下使用，版本更新较快也是其一大亮点，支持子任务，DAG任务和依赖任务已经列入TODOLIST，暂时不支持秒任务，具体支持如下： 支持多种语言作业，语言无关(Java/Go/C++/PHP/Python/Ruby/shell) 分片广播任务：执行器集群部署时，任务路由策略选择\"分片广播\"情况下，一次任务调度将会广播触发集群中所有执行器执行一次任务，可根据分片参数开发分片任务； 动态分片：分片广播任务以执行器为维度进行分片，支持动态扩容执行器集群从而动态增加分片数量，协同进行业务处理；在进行大数据量业务操作时可显著提升任务处理能力和速度。 支持作业高可用 弹性扩容缩容：一旦有新执行器机器上线或者下线，下次调度时将会重新分配任务； 故障转移：任务路由策略选择\"故障转移\"情况下，如果执行器集群中某一台机器故障，将会自动Failover切换到一台正常的执行器发送调度请求。 事件触发：除了\"Cron方式\"和\"任务依赖方式\"触发任务执行之外，支持基于事件的触发任务方式。调度中心提供触发任务单次执行的API服务，可根据业务事件灵活触发。 任务依赖：支持配置子任务依赖，当父任务执行结束且执行成功后将会主动触发一次子任务的执行, 多个子任务用逗号分隔； 容器化：提供官方docker镜像，并实时更新推送dockerhub，进一步实现产品开箱即用； 任务失败重试：支持自定义任务失败重试次数，当任务失败时将会按照预设的失败重试次数主动进行重试；其中分片任务支持分片粒度的失败重试； Elastic-job： 当当开源的分布式调度解决方案，由两个相互独立的子项目Elastic-Job-Lite和Elastic-Job-Cloud组成。Elastic-Job-Lite定位为轻量级无中心化解决方案，使用jar包的形式提供分布式任务的协调服务。一般我们只要使用Elastic-Job-Lite就好。 Elastic-Job-Lite并没有宿主程序，而是基于部署作业框架的程序在到达相应时间点时各自触发调度。它的开发也比较简单，引用Jar包实现一些方法即可，最后编译成Jar包运行。 Elastic-Job-Lite的分布式部署全靠ZooKeeper来同步状态和原数据。实现高可用的任务只需将分片总数设置为1，并把开发的Jar包部署于多个服务器上执行，任务将会以1主N从的方式执行。一旦本次执行任务的服务器崩溃，其他执行任务的服务器将会在下次作业启动时选择一个替补执行。如果开启了失效转移，那么功能效果更好，可以保证在本次作业执行时崩溃，备机之一立即启动替补执行。 Elastic-Job-Lite的任务分片也是通过ZooKeeper来实现，Elastic-Job并不直接提供数据处理的功能，框架只会将分片项分配至各个运行中的作业服务器，开发者需要自行处理分片项与真实数据的对应关系。框架也预置了一些分片策略：平均分配算法策略，作业名哈希值奇偶数算法策略，轮转分片策略。同时也提供了自定义分片策略的接口。 Elastic-Job-Lite还提供了一个任务监控和管理界面：Elastic-Job-Lite-Console。它和Elastic-Job-Lite是两个完全不关联的应用程序，使用ZooKeeper来交换数据，管理人员可以通过这个界面查看、监控和管理Elastic-Job-Lite的任务，必要的时候还能手动触发任务。 关注的是数据，增加了弹性扩容和数据分片的思路，以便于更大限度的利用分布式服务器的资源。但是学习成本相对高些，推荐在“数据量庞大，且部署服务器数量较多”时使用。 "},"docs/DesignPatterns/1-单例模式.html":{"url":"docs/DesignPatterns/1-单例模式.html","title":"单例模式","keywords":"","body":"单例模式 设计模式：单例模式 (关于饿汉式和懒汉式)_Java_薛嘉涛的博客-CSDN博客 饿汉式 public class Singleton { private final static Singleton INSTANCE = new Singleton(); private Singleton(){} public static Singleton getInstance(){ return INSTANCE; } } 普通的懒汉式 (线程不安全，不可用) public class Singleton { private static Singleton instance = null; private Singleton() { } public static Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; } } 这是懒汉式中最简单的一种写法，只有在方法第一次被访问时才会实例化，达到了懒加载的效果。但是这种写法有个致命的问题，就是多线程的安全问题。假设对象还没被实例化，然后有两个线程同时访问，那么就可能出现多次实例化的结果，所以这种写法不可采用。 同步方法的懒汉式 public class Singleton { private static Singleton instance = null; private Singleton() { } public static synchronized Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; } } 这种写法是对getInstance()加了锁的处理，保证了同一时刻只能有一个线程访问并获得实例，但是缺点也很明显，因为synchronized是修饰整个方法，每个线程访问都要进行同步，而其实这个方法只执行一次实例化代码就够了，每次都同步方法显然效率低下，为了改进这种写法，就有了下面的双重检查懒汉式。 双重校验锁懒汉式 public class Singleton { private static volatile Singleton instance; private Singleton() {} public static Singleton getInstance() { if (instance == null) { synchronized (Singleton.class) { if (instance == null) { instance = new Singleton(); } } } return instance; } } 这种写法用了两个if判断，也就是Double-Check，并且同步的不是方法，而是代码块，效率较高，是对第三种写法的改进。为什么要做两次判断呢？这是为了线程安全考虑，还是那个场景，对象还没实例化，两个线程A和B同时访问静态方法并同时运行到第一个if判断语句，这时线程A先进入同步代码块中实例化对象，结束之后线程B也进入同步代码块，如果没有第二个if判断语句，那么线程B也同样会执行实例化对象的操作了。 volatile作用 禁止指令重排序。我们知道new Singleton()是一个非原子操作，编译器可能会重排序【构造函数可能在整个对象初始化完成前执行完毕，即赋值操作（只是在内存中开辟一片存储区域后直接返回内存的引用）在初始化对象前完成】。而线程B在线程A赋值完时判断instance就不为null了，此时B拿到的将是一个没有初始化完成的半成品。 保证可见性。线程A在自己的工作线程内创建了实例，但此时还未同步到主存中；此时线程B在主存中判断instance还是null，那么线程B又将在自己的工作线程中创建一个实例，这样就创建了多个实例。 静态内部类 public class Singleton { private Singleton() {} private static class SingletonInstance { private static final Singleton INSTANCE = new Singleton(); } public static Singleton getInstance() { return SingletonInstance.INSTANCE; } } 枚举 public enum Singleton { INSTANCE; } 单例模式的优缺点 优点单例类只有一个实例，节省了内存资源，对于一些需要频繁创建销毁的对象，使用单例模式可以提高系统性能； 单例模式可以在系统设置全局的访问点，优化和共享数据，例如前面说的Web应用的页面计数器就可以用单例模式实现计数值的保存。 缺点单例模式一般没有接口，扩展的话除了修改代码基本上没有其他途径。 "},"docs/DesignPatterns/2-工厂模式.html":{"url":"docs/DesignPatterns/2-工厂模式.html","title":"工厂模式","keywords":"","body":"一般情况下，工厂模式分为三种更加细分的类型：简单工厂、工厂方法和抽象工厂。不过，在 GoF 的《设计模式》一书中，它将简单工厂模式看作是工厂方法模式的一种特例，所以工厂模式只被分成了工厂方法和抽象工厂两类。 简单工厂（Simple Factory） 首先，简单工厂模式不属于23种GOF四人组（Gang of Four）设计模式，简单工厂一般分为：普通简单工厂、多方法简单工厂、静态方法简单工厂。 普通 就是建立一个工厂类，对实现了同一接口的一些类进行实例的创建。 public interface Sender { public void Send(); } public class MailSender implements Sender { @Override public void Send() { System.out.println(\"this is mailsender!\"); } } public class SmsSender implements Sender { @Override public void Send() { System.out.println(\"this is sms sender!\"); } } public class SendFactory { public Sender produce(String type) { if (\"mail\".equals(type)) { return new MailSender(); } else if (\"sms\".equals(type)) { return new SmsSender(); } else { System.out.println(\"请输入正确的类型!\"); return null; } } } 多个方法 是对普通工厂方法模式的改进，在普通工厂方法模式中，如果传递的字符串出错，则不能正确创建对象，而多个工厂方法模式是提供多个工厂方法，分别创建对象。 //将上面的代码做下修改，改动下SendFactory类就行，如下： public class SendFactory { public Sender produceMail(){ return new MailSender(); } public Sender produceSms(){ return new SmsSender(); } } 多个静态方法 //将上面的多个工厂方法模式里的方法置为静态的，不需要创建实例，直接调用即可。 public class SendFactory { public static Sender produceMail(){ return new MailSender(); } public static Sender produceSms(){ return new SmsSender(); } } 工厂方法（Factory Method） 简单工厂模式有一个问题就是，类的创建依赖工厂类，也就是说，如果想要拓展程序，必须对工厂类进行修改，这违背了闭包原则，所以，从设计角度考虑，有一定的问题，如何解决？就用到工厂方法模式，创建一个工厂接口和创建多个工厂实现类，这样一旦需要增加新的功能，直接增加新的工厂类就可以了，不需要修改之前的代码。 public interface Sender { public void Send(); } public class MailSender implements Sender { @Override public void Send() { System.out.println(\"this is mailsender!\"); } } public class SmsSender implements Sender { @Override public void Send() { System.out.println(\"this is sms sender!\"); } } public class SendMailFactory implements Provider { @Override public Sender produce(){ return new MailSender(); } } public class SendSmsFactory implements Provider{ @Override public Sender produce() { return new SmsSender(); } } public interface Provider { public Sender produce(); } public class Test { public static void main(String[] args) { Provider provider = new SendMailFactory(); Sender sender = provider.produce(); sender.Send(); } } 其实这个模式的好处就是，如果你现在想增加一个功能：发及时信息，则只需做一个实现类，实现Sender接口，同时做一个工厂类，实现Provider接口，就OK了，无需去改动现成的代码。这样做，拓展性较好！ 工厂方法分4个组件： Product：定义工厂方法所创建的对象的接口，也就是实际需要使用的对象的接口。 ConcreteProduct：具体的 Product 接口的实现对象。 Creator：创建器，声明工厂方法，工厂方法通常会返回一个 Product 类型的实例对象，而且多是抽象方法。也可以在Createor里面提供工厂方法的默认实现，让工厂方法返回一个缺省的 Product 类型的实例对象。（对应上面的SendSmsFactory、SendMailFactory） ConcreteCreator：具体的创建器对象，覆盖实现 Creator 定义的工厂方法，返回具体的 Product 实例。 抽象工厂（Abstract Factory） 假设目前你的程序里面有两个对象，苹果(apple)和香蕉(banana),那么你使用工厂模式就已经足够了，因为她们属于同一个品类，都属于水果，如果在添加一个菠萝产品，也只需要把菠萝加入到你的水果工厂里面就够了。 但是如果你程序里面有四个对象，苹果汁，苹果派，香蕉汁，香蕉派，这四个对象正好有明确的层级关系，可以抽象为两个层级，苹果，香蕉，或者果汁，派。这时候工厂模式明显已经不适用了，因为工厂模式是对象都实现了同一个接口，这时候就可以使用抽象工厂模式了。 就是把对象抽象一下，把这四个对象抽象为两个接口，一个果汁接口，一个派的接口。然后再设计一个抽象的工厂（抽象类）abstractFactory，里面生产抽象的对象（也就是接口）Juice，Pie，单看这个结构就是一个工厂模式，但是我们要用生产的是对象而不是接口。所以我们还需要两个具体工厂： 一个AppleFactory继承abstractFactory，实现生成Pie的方法和生成Juice的方法，实际上就是生成对象AppleJuice和ApplePie，一个BananaFactory继承abstractFactory，实现生成Pie的方法和生成Juice的方法，实际上就是生成对象BananaJuice和BananaPie， 这样的话，对于调用者来说，我在开发过程中，只需要知道我操作的对象是Pie或者是Juice就够了，这样降低了耦合。 package abstractFactory; public class Test { public static void main(String args[]){ AbstractFactory factory1 = new AppleFactory(); factory1.createJuice().desc(); factory1.createPie().desc(); //假设我们之前需要的是applePie和appleJuice对象，现在需要换成bananaPie和BananaJuice对象 //我们只需要替换对应的实现工厂（把new AppleFactory换成new BananFactory就可以了，耦合比较低） AbstractFactory factory2 = new BananaFactory(); factory2.createJuice().desc(); factory2.createPie().desc(); } } // 下面是抽象工厂，生产对象的抽象。 package abstractFactory; public abstract class AbstractFactory { abstract Juice createJuice(); abstract Pie createPie(); } // 下面是具体工厂两个 package abstractFactory; public class AppleFactory extends AbstractFactory{ @Override Juice createJuice() { return new AppleJuice(); } @Override Pie createPie() { return new ApplePie(); } } package abstractFactory; public class BananaFactory extends AbstractFactory{ @Override Juice createJuice() { return new BananaJuice(); } @Override Pie createPie() { return new BananaPie(); } } // 下面是对象抽象出来的接口两个 package abstractFactory; public interface Juice { public void desc(); } package abstractFactory; public interface Pie { public void desc(); } // 最后是我们要生产的四个对象。 package abstractFactory; public class AppleJuice implements Juice { @Override public void desc() { System.out.println(\"苹果汁.\"); } } package abstractFactory; public class ApplePie implements Pie { @Override public void desc() { System.out.println(\"苹果派\"); } } package abstractFactory; public class BananaJuice implements Juice { @Override public void desc() { System.out.println(\"香蕉汁.\"); } } package abstractFactory; public class BananaPie implements Pie { @Override public void desc() { System.out.println(\"香蕉派\"); } } 组件 抽象工厂的功能是为一系列相关对象或相互依赖的对象创建一个接口。 实现成接口 使用工厂方法 抽象工厂模式的优缺点 优点： 分离接口和实现 使得切换产品簇变得容易 缺点： 不太容易扩展新的产品 容易造成类层次复杂 参考 23种设计模式详解 - 枫树湾河桥 - 博客园 "},"docs/DesignPatterns/6-模板方法模式.html":{"url":"docs/DesignPatterns/6-模板方法模式.html","title":"模板方法模式","keywords":"","body":"前言 模板设计模式是由抽象类来实现的，主要目的就是将重复的相同的流程放在抽象的父类中，在子类中完成各自不同的业务。 先来介绍一下抽象类。 抽象类 抽象方法指的是只声明而未实现的方法（没有方法体） 抽象方法使用abstract关键字来定义，抽⽅方法所在的类也要使用abstract关键字来定义 抽象类： 不能直接产生实例化对象 如果一个非抽象类继承了抽象类，必须重写抽象类的方法 如果一个抽象类继承了抽象类，作为派生类的抽象类可以不实现最为基类的抽象类方法 private、abstract和final不能同时出现 派生类的访问权限>=基类访问权限（方法重写时） 抽象类中可包含非抽象方法 模板方法模式：在一个方法中定义一个算法的骨架，而将一些步骤延迟到子类中。模板方法使得子类可以在不改变算法结构的情况下，重新定义算法中的某些步骤。模板方法模式是基于继承的代码复用基本技术，模板方法模式的结构和用法也是面向对象设计的核心之一。在模板方法模式中，可以将相同的代码放在父类中，而将不同的方法实现放在不同的子类中。 在模板方法模式中，我们需要准备一个抽象类，将部分逻辑以具体方法以及具体构造函数的形式实现，然后声明一些抽象方法来让子类实现剩余的逻辑。不同的子类可以以不同的方式实现这些抽象方法，从而对剩余的逻辑有不同的实现，这就是模板方法模式的用意。模板方法模式体现了面向对象的诸多重要思想，是一种使用频率较高的模式。 代码 abstract class BankTemplate { public final void takeNumber() { System.out.println(\"取号\"); } public abstract void transact(); public final void evaluate() { System.out.println(\"业务评价：\"); } final void process() { takeNumber(); transact(); evaluate(); } } class Useer1 extends BankTemplate{ @Override public void transact() { System.out.println(\"取钱\"); } } class Useer2 extends BankTemplate{ @Override public void transact() { System.out.println(\"存钱\"); } } public class TestDemo1 { public static void main(String[] args) { Useer1 useer1 = new Useer1(); useer1.process(); Useer2 useer2 = new Useer2(); useer2.process(); } } 使用Lambda表达式 使用你偏爱的Lambda表达式同样也可以解决这些问题（创建算法框架，让具体的实现插入某些部分）。你想要插入的不同算法组件可以通过Lambda表达式或者方法引用的方式实现。 这里我们向 processCustomer 方法引入了第二个参数，它是一个 Consumer 类型的参数，与前文定义的 makeCustomerHappy 的特征保持一致： public void processCustomer(int id, Consumer makeCustomerHappy) { Customer customer = Database.getCustomerWithId(id); makeCustomerHappy.accept(customer); } 现在，你可以很方便地通过传递Lambda表达式，直接插入不同的行为，不再需要继承AbstractOnlineBanking 类了： public static void main(String[] args) { new AbstractOnlineBankingLambda().processCustomer(1337, ( AbstractOnlineBankingLambda.Customer c) -> System.out.println(\"Hello!\")); } 这是又一个例子，佐证了Lamba表达式能帮助你解决设计模式与生俱来的设计僵化问题。 ps: 模板方法是为了差异化的实现，实际上通过参数化传递可以达到。 同样的道理，决策模式为了不同场景的路由，也可以通过 lambda 来实现。 模板设计模式的优缺点 模板方法模式通过把不变的行为搬移到超类，去除了子类中的重复代码。子类实现算法的某些细节，有助于算法的扩展。 通过一个父类调用子类实现的操作，通过子类扩展增加新的行为，符合“开放-封闭原则”。 缺点 每个不同的实现都需要定义一个子类，这会导致类的个数的增加，设计更加抽象。 适用场景 在某些类的算法中，用了相同的方法，造成代码的重复。控制子类扩展，子类必须遵守算法规则。 开闭原则(OCP): 一个软件实体如类、模块和函数应该对扩展开放、对修改关闭。 开闭原则是Java中最基础的设计原则。 "},"docs/DesignPatterns/4-策略模式.html":{"url":"docs/DesignPatterns/4-策略模式.html","title":"策略模式","keywords":"","body":"介绍 代码 使用Lambda表达式 到现在为止，你应该已经意识到 ValidationStrategy 是一个函数接口了（除此之外，它还与 Predicate 具有同样的函数描述）。 这意味着我们不需要声明新的类来实现不同的策略，通过直接传递Lambda表达式就能达到同样的目的，并且还更简洁： Validator v3 = new Validator((String s) -> s.matches(\"\\\\d+\")); System.out.println(v3.validate(\"aaaa\")); Validator v4 = new Validator((String s) -> s.matches(\"[a-z]+\")); System.out.println(v4.validate(\"bbbb\")); 正如你看到的，Lambda 表达式避免了采用策略设计模式时僵化的模板代码。 如果你仔细分析一下个中缘由，可能会发现，Lambda表达式实际已经对部分代码（或策略）进行了封装，而这就是创建策略设计模式的初衷。 因此，我们强烈建议对类似的问题，你应该尽量使用Lambda表达式来解决。 应用 策略模式还是比较常用的一种设计模式，比如java中给我定义好的Comparator 接口就是策略模式的一个实践。 策略模式不仅仅可以优化if else代码，其主要的作用还是解耦策略的定义、创建和使用，控制代码的复杂度，让每个部分都不至于过于复杂、代码量过多。除此之外，对于复杂代码来说，策略模式还能让其满足开闭原则，添加新策略的时候，最小化、集中化代码改动，减少引入 bug 的风险。 减少该死的 if else 嵌套 - 简书 如何无痛降低 if else 面条代码复杂度 - 掘金 用设计模式来代替臃肿的ifelse层层判断 - ghevinn欢迎您光临 - CSDN博客 "},"docs/DesignPatterns/7-观察者模式.html":{"url":"docs/DesignPatterns/7-观察者模式.html","title":"观察者模式","keywords":"","body":""},"docs/DesignPatterns/5-职责链模式.html":{"url":"docs/DesignPatterns/5-职责链模式.html","title":"责任链模式","keywords":"","body":""},"docs/Guide/面试常见问题回答（主观题）.html":{"url":"docs/Guide/面试常见问题回答（主观题）.html","title":"面试常见问题回答（主观题）","keywords":"","body":"面试常见问题回答 自我介绍 我是谁？我来自哪里？ 我为什么参加面试？ 我的以往工作成绩及亮点。 我希望从贵公司得到什么？ 有什么要问我的 目前这个岗位和你们目前部门现在所做的是什么业务、什么事情 你们所用的技术栈都有哪些 我觉得我这次表现的不是太好，你有什么建议或者评价给我吗？ 部门的主要人员分配以及对应的主要工作能简单介绍一下吗？ 未来如果我要加入这个团队，你对我的期望是什么？ 公司对新入职的员工的培养机制是什么样的呢？ 以您来看，这个岗位未来在公司内部的发展如何？ 团队现在面临的最大挑战是什么？ 与同行业的竞争者相比，贵公司的核心竞争优势在什么地方？ 职位空缺的原因（得知上一个人离职的原因） 公司如何保证人才不流失（得知薪资水平、福利待遇如何） 阿里的六脉神剑 新/旧使命 让天下没有难做的生意 新愿景 活102年：我们不追求大，不追求强，我们追求成为一家活102年的好公司； 到2036年，服务20亿消费者，创造1亿就业机会，帮助1000万中小企业盈利。 旧愿景 我们旨在构建未来的商业基础设施。我们的愿景是让客户相会、工作和生活在阿里巴巴，并持续发展最少102年。 干货丨阿里「新六脉神剑」的考核方式 为什么加入/选择我们公司 行业棒：今年XX行业在疫情期间受到的影响是最小的，在未来的发展趋势中，更多的人需要进行财富管理，这个行业很具有发展潜力的。 公司棒：目前贵公司在业界是发展势头迅猛的，尤其是近几年，业务线非常多，可谓是为普通人解决了很多生活问题也带来了很多平台业务。 岗位是我所期待的：贵公司该岗位是我所期待的，与我现在所做的事情非常match，并且我很喜欢贵公司的岗位业务上迎来的挑战。 你能给我们公司带来什么价值 我的专业技能、业务能力、工作经历上与该职位非常匹配，并且有（XX行情）相关的开发经验， 所以我能很快的上手做事情，省去前期因为对整个大的证券业务的不熟悉带来的时间和资源上的浪费； 因为我之前是做相关工作的，所以我有我们自有的一套经验并且能与当前的业务服务架构相结合，摩擦、碰撞出火花来，我认为可以能带来更好的成绩，并且我也喜欢当前这个职位所要面临的挑战，我有信心和我的团队一起攻克难关。 你希望公司能为你带来什么 考查：求职动机 我首先会努力做好自己本职工作，多学习和提升自己，就自身而言我更关注的是我在这个岗位上未来的发展问题，是否有晋升的机会，是否有培训机会，能力的持续是我最重视的。我也会在这个过程中，和团队让公司变的更好更强。 你对新工作有什么计划和期待吗 我希望我自己可以学习更多的知识和技能，这样就可以帮公司负担更多的工作项目，然后在为公司创造更多价值的同时，如果我自己也能快速成长就更好了。 为什么选择离职/换工作 离职原因是面试必问，但我们不能说自己的真实想法，一定要表现出是客观因素导致。如： 现在自己的技术成长有点碰到瓶颈，加上一直对您公司钦慕有加。 因公司提供的平台有限，限制了个人的成长发展，本人希望需求更大的适合发展的平台； 因个人喜欢追求有挑战的事情，贵公司的招聘职位我认为很具有挑战，与我比较相符； 因公司调整了业务，组织架构调整； 你的期望薪资 范围 or 具体的数字 请问公司的薪酬结构是什么样的？（福利、奖金、股票等） 报出自己期望的最高月薪（给HR适当的降价空间） 为什么要这么高的薪资： 前一份工作的薪资水平 期望的薪资涨幅 入职的职位的价值 我与这个职位的匹配程度 你多久可以入职 待业中： 我本打算休息一下，如果公司需要，我7天内可以入职。 正在交接中： 目前正在办理离职交接，大概需要2~3天的时间。 待提离职申请： 目前还在职，需要有人交接工作，需要一个月的时间。 你们工作中是如何团队合作的？ 首先，我们工作中的合作来自于几个部分，分别是产品、研发、测试、设计。 而我是后端开发，主要对接的是组内或者其他部门的后端研发，与客户端。 上线之后，用户投诉说不好用怎么办 先让产品分门别类归类问题 找出重要的的问题 先修复(就是那个重要和紧急的四个象限) 你和别人发生过争执吗？你怎样解决？ 你如何面对压力？ 你有帮助别人的案例么 写博客 帮助别人 互联网的分享精神 如果你没有通过本次面试，会怎么办 首先感谢面试官给我这次机会 其次竞争必然有淘汰，我会坦然面对 我认为我的综合能力和岗位匹配度还不存，可能因为紧张，没有展现出自己的真实水平。 职业规划是什么 行业上有所发展 + 岗位上有所深耕。 希望两年内能成长到能带领小团队，为公司带来更大的价值。 技术路线 --> 架构师 管理路线 --> 团队Leader 未来长远的目标是走管理路线，能够带领属于自己的团队。当然管理路线也并不可能立马就转型，需要有技术的积累经验。 由于软件公司的管理者毕竟不同于一般公司的管理者，专业能力越强，管理起来就越得心应手的，没有专业能力的管理，遇到的问题很多，也很难解决。 因此在短期3-5年我还是会以夯实技术为主、多去参与并积累一些项目的问题解决方案经验，为以后带领团队打下基础。我也会观察我所在的团队的Leader他的一些管理方法，我们内部也会有结构调整，所以在这样的变动下，不同管理者是怎么应对和去带领新的团队、和团队之间成员的磨合是最重要的，这也是我要去未来学习的。 最好要把编程、项目设计、项目管理等基础打打好。这样转型到管理者成功率就会高一些。 你会为你的未来职业规划做什么努力 技术型：架构师 要成为架构师，这条路是很艰难的，至少我是这样认为，我们常会说程序员到了35岁就会走下坡路，我是这样理解的，35岁会成为程序员的一个分水岭，以前听人说程序员是吃青春饭的，这不假，如果你一直都很平庸的到了35岁可能就会很难找工作，所以35岁之前我们一定要向职业规划上努力，我觉得有以下几点。 要了解服务间的调用、整个链路的调用 要清楚我们技术中用的每个点，大到服务架构的规划，小到数据的一致性保证。这都得在我们技术积累到一定程度上，才能有良好的大型系统结构的设计，保证整体架构的高可用、高并发、高性能。 有对技术有广度和深度的研究，先广后深，毕竟很多技术上的原理都想想通的，一个看懂后，其他也就迎刃而解了。 对新技术要有所了解 管理型：团队Leader 要有管理者的思想 要有高技术的能力 要有产品人的先驱 想去什么样的公司（行业/技术） 行业： ​ 我会选择去一个未来有发展趋势的行业，比如： ​ 金融行业，这个是亘古不变的，金融和我们生活息息相关离不开，比如支付宝、美团、饿了么，我们的吃住行现在都可以数字化在手机APP上很方便的帮我们解决了。我相信其他行业也会朝着这个移动互联网的大趋势迈进数字化。 ​ 直播行业， ​ 社交媒体行业， ​ 在线教育、在线办公行业， 行业是一方面，另一方面是公司的业务量、用户数，因为我们是后端开发，系统服务的稳定性、高并发是关键，用户量和业务量在一定程度上可以给我们带来业务和技术上的挑战和难题，在这样环境下成长会更快，更能突破自己。 说出一个你的优点和缺点 3谈3不谈： 避重就轻不谈、致命弱点不谈、生活缺点不谈； 阶段性或群体性的、拔高标准的、已改正或容易改正的； 技术上： 优点：刨根问底、业务完全match经验丰富； 缺点： 性格有点儿急，工作恨不得马上拿结果，所以有些地方考虑的不全，后来我在改善培养自己做事的方式，在细节上考虑周全，精确到每一个内存占用多少字节，现有容量可够多少用户使用等等。 说话太直接了，因为平时都是和机器打交道，说话直来直去的，但是都是对事不对人，后面我也在慢慢的进行换位思考，做到掌握分寸，不会让双方收到伤害。 经验不足、缺乏产品化、整体性思维，还需学习改进； 生活上： 优点：风趣幽默、喜欢看脱口秀，经常说段子，都周围的人，活跃气氛 缺点：没有一开始就能和大家打的火热，需要时间磨合 你对自己这几年的经历满意吗？ ​ 生活岂能用满意与不满意简而概之，过得好不代表满意，过得不好也不代表不满意，人生本就是不断修炼的过程，要向前看，如果生活越来越好，自己越来越优秀即可。在这种程度上来说我对过往是满意的，但这并不代表我过去过的好，如果从来，我会活的不一样，可这个世界上没有如果，于是便只能勇往无前。 ​ 人生所遇见的每一个人每一件事都有其存在的意义，都是在教会我们成长，重要的不是过去怎么样，而是从中吸取经验，让自己越来越好，自己为了更好的生活而一直努力奋斗向前，哪怕风吹雨打。 你遇到过很挫败的事情是什么 当时有一个需求组件起来，需要几个同事合作对接完成，但是我们老大直接选了另一个同事去主导。我就有点失落。 但后来出方案的时候，那个同事的方案有瑕疵，然后我讲出了我的方案，证明了我不比别人差。最后改变了老大的看法，让我主导这个需求项目。 如果你和同事发生争执分歧怎么办 首先我会从自身找问题，不管是设计方案也好，关于出现的问题的原因也好，我会让自己考虑下是否是合理正确的。 如果当事人双方都不是很冷静，谁都不听谁的，可以先冷静下来，让第三人来评判下哪种方案能更适合于当前的业务并且不会给公司带来损失的。 有什么业余爱好 喜欢音乐、脱口秀 平常会写写公众号和博客，搞搞小工具 最近在读的一本书 技术类 我最近在看《深入理解Kafka》 非技术类 我最近在看丰子恺先生的《万般滋味都还是生活》，里面讲到丰子恺先生的在他的人生的各个级段，一些身边发生的小事，有朋友送养的大鹅，还有小猫等等，生活中不只有工作、还有家庭、孩子、欢笑、其他的味道。 你目前还有在谈的其他公司吗？/ 你现在收到几个offer了 有的。 但是XX（当前HR沟通的这家公司）通过的话， 其他公司就不会再继续了。 你对加班这件事怎么看 我可以接受合理的加班，首先我会提升我的工作效率在正常的工作时间完成我的工作。 如果是项目紧急需要加班赶进度，这个大家都能理解，我也会为项目组贡献一份力，争取早日上线； 如果是。。。 收到OFFER后，需要弄清的几个问题 试用期的工资和时间； 转正的考核标准和薪资范围； 加班的工资怎么算； 公司的晋升体系和培训机制； 五险一金按什么比例缴纳； "}}